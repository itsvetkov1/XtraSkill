---
phase: 72-backend-file-generation
plan: 02
type: execute
wave: 2
depends_on: ["72-01"]
files_modified:
  - backend/app/services/llm/claude_cli_adapter.py
  - backend/app/services/ai_service.py
  - backend/app/routes/conversations.py
autonomous: true
requirements: [GEN-01, GEN-02]

must_haves:
  truths:
    - "Sending artifact_generation=true to the chat endpoint triggers the file generation system prompt with a session token"
    - "The CLI subprocess connects to the MCP server via --mcp-config and has built-in tools disabled via --tools ''"
    - "The ARTIFACT_CREATED marker in CLI result output is detected and emitted as an artifact_created SSE event"
    - "BA thread behavior is completely unchanged (same system prompt, no MCP interference)"
  artifacts:
    - path: "backend/app/services/ai_service.py"
      provides: "ASSISTANT_FILE_GEN_PROMPT, artifact_generation parameter threading, session lifecycle"
      contains: "ASSISTANT_FILE_GEN_PROMPT"
    - path: "backend/app/services/llm/claude_cli_adapter.py"
      provides: "--mcp-config and --tools flags in all spawn paths, ARTIFACT_CREATED detection in _translate_event"
      contains: "mcp-config"
    - path: "backend/app/routes/conversations.py"
      provides: "artifact_generation passed to ai_service.stream_chat()"
      contains: "artifact_generation"
  key_links:
    - from: "backend/app/routes/conversations.py"
      to: "backend/app/services/ai_service.py"
      via: "artifact_generation parameter in stream_chat() call"
      pattern: "stream_chat.*artifact_generation"
    - from: "backend/app/services/ai_service.py"
      to: "backend/app/mcp_server.py"
      via: "register_mcp_session / unregister_mcp_session calls"
      pattern: "register_mcp_session"
    - from: "backend/app/services/llm/claude_cli_adapter.py"
      to: "MCP server at /mcp"
      via: "--mcp-config JSON argument in all 3 spawn paths"
      pattern: "mcp-config"
    - from: "backend/app/services/llm/claude_cli_adapter.py"
      to: "backend/app/services/ai_service.py"
      via: "ARTIFACT_CREATED metadata in StreamChunk -> artifact_created SSE event"
      pattern: "ARTIFACT_CREATED"
---

<objective>
Wire the artifact_generation parameter from the chat endpoint through ai_service to the CLI adapter, add the MCP config and tool flags to all CLI spawn paths, add ARTIFACT_CREATED detection in the CLI adapter, and add the conditional file-generation system prompt with session lifecycle management.

Purpose: This is the end-to-end wiring that connects the frontend's `artifact_generation=true` request to the MCP server created in Plan 01, resulting in an `artifact_created` SSE event back to the client.

Output: Modified `claude_cli_adapter.py` (spawn flags + event detection), `ai_service.py` (prompt + parameter threading + session lifecycle), `conversations.py` (parameter pass-through).
</objective>

<execution_context>
@/Users/a1testingmac/.claude/get-shit-done/workflows/execute-plan.md
@/Users/a1testingmac/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/STATE.md
@.planning/phases/72-backend-file-generation/72-RESEARCH.md
@.planning/phases/72-backend-file-generation/72-01-SUMMARY.md

@backend/app/services/llm/claude_cli_adapter.py
@backend/app/services/ai_service.py
@backend/app/routes/conversations.py
@backend/app/mcp_server.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add --mcp-config and --tools flags to all CLI spawn paths and ARTIFACT_CREATED detection</name>
  <files>backend/app/services/llm/claude_cli_adapter.py</files>
  <action>
This task modifies `claude_cli_adapter.py` in three areas:

**A. Add MCP config constant and import (top of file):**

Add `import json` if not already imported (it is). Add a module-level constant:
```python
MCP_SERVER_URL = "http://127.0.0.1:8000/mcp"
MCP_CONFIG_JSON = json.dumps({
    "mcpServers": {
        "assistant": {"type": "http", "url": MCP_SERVER_URL}
    }
})
```

**B. Add --mcp-config and --tools flags to ALL THREE spawn paths:**

The `--mcp-config` and `--tools ""` flags must be added to every place that builds CLI arguments. Per the research anti-pattern guidance: apply to ALL pool processes (BA threads ignore MCP since their prompt doesn't reference it; Assistant threads need it).

1. `_spawn_warm_process()` (line ~180): Add `'--mcp-config', MCP_CONFIG_JSON, '--tools', ''` after `'--model', self._model` in the `create_subprocess_exec` call.

2. `_cold_spawn()` (line ~205): Add the same `'--mcp-config', MCP_CONFIG_JSON, '--tools', ''` after `'--model', self._model`.

3. Direct fallback `cmd` list in `stream_chat()` (line ~607): Add `'--mcp-config', MCP_CONFIG_JSON, '--tools', ''` after `'--model', self.model`.

**C. Add ARTIFACT_CREATED detection in `_translate_event()` (line ~519):**

Replace the `elif event_type == "result":` branch with:
```python
elif event_type == "result":
    result_text = event.get("result", "")
    usage = event.get("usage", {})

    # Detect ARTIFACT_CREATED marker in result text (same protocol as mcp_tools.py)
    artifact_data = None
    if "ARTIFACT_CREATED:" in result_text:
        try:
            marker_start = result_text.index("ARTIFACT_CREATED:") + len("ARTIFACT_CREATED:")
            marker_end = result_text.index("|", marker_start)
            artifact_data = json.loads(result_text[marker_start:marker_end])
        except (ValueError, json.JSONDecodeError):
            logger.warning("Failed to parse ARTIFACT_CREATED marker from CLI result")

    return StreamChunk(
        chunk_type="complete",
        usage={
            "input_tokens": usage.get("input_tokens", 0),
            "output_tokens": usage.get("output_tokens", 0),
        },
        metadata={"artifact_created": artifact_data} if artifact_data else None,
    )
```

This mirrors the existing pattern in `claude_agent_adapter.py` (lines 230-236). The `_stream_agent_chat()` in `ai_service.py` already reads `chunk.metadata["artifact_created"]` at lines 957-967 and emits the SSE event.

Avoid: Do NOT change any ContextVar usage or the existing `set_context()` method. Do NOT modify `_convert_messages_to_prompt()`. Do NOT add `--mcp-config` only for Assistant threads -- apply it universally (BA ignores it).
  </action>
  <verify>
Run: `cd backend && venv/bin/python -c "
from app.services.llm.claude_cli_adapter import MCP_CONFIG_JSON, MCP_SERVER_URL
import json
config = json.loads(MCP_CONFIG_JSON)
assert 'mcpServers' in config
assert config['mcpServers']['assistant']['url'] == 'http://127.0.0.1:8000/mcp'
print('MCP config OK')
"`

Verify spawn paths contain new flags by reading the file and confirming `--mcp-config` appears 3 times (once per spawn path).
  </verify>
  <done>All 3 CLI spawn paths include --mcp-config and --tools "" flags; _translate_event() detects ARTIFACT_CREATED markers in result events and populates StreamChunk metadata.</done>
</task>

<task type="auto">
  <name>Task 2: Thread artifact_generation through ai_service and conversations, add file-gen system prompt</name>
  <files>backend/app/services/ai_service.py, backend/app/routes/conversations.py</files>
  <action>
This task modifies two files to wire the `artifact_generation` parameter end-to-end.

**A. In `backend/app/services/ai_service.py`:**

1. Add the file generation system prompt constant near the top of the file (after existing imports/constants):
```python
ASSISTANT_FILE_GEN_PROMPT = (
    "You are a file generation assistant. When asked to generate a file:\n"
    "1. Call the save_artifact tool with title, content_markdown, and session_token='{session_token}'\n"
    "2. Call save_artifact ONCE and stop immediately after\n"
    "3. Do not produce any conversational text before or after calling the tool"
)
```

2. Add `import uuid as _uuid` at the top of the file (near other stdlib imports).

3. Update `stream_chat()` signature (line ~1039) to accept `artifact_generation`:
```python
async def stream_chat(
    self,
    messages: List[Dict[str, Any]],
    project_id: str,
    thread_id: str,
    db,
    artifact_generation: bool = False
) -> AsyncGenerator[Dict[str, Any], None]:
```

4. Update the agent provider routing call inside `stream_chat()` (line ~1061) to pass the parameter:
```python
if getattr(self, 'is_agent_provider', False):
    async for event in self._stream_agent_chat(messages, project_id, thread_id, db, artifact_generation=artifact_generation):
        yield event
    return
```

5. Update `_stream_agent_chat()` signature (line ~864) to accept `artifact_generation`:
```python
async def _stream_agent_chat(
    self,
    messages: List[Dict[str, Any]],
    project_id: str,
    thread_id: str,
    db,
    artifact_generation: bool = False
) -> AsyncGenerator[Dict[str, Any], None]:
```

6. Replace LOGIC-01 (line ~931) with session-aware prompt selection:
```python
# LOGIC-01: System prompt selection
# BA threads: full BA system prompt (unchanged)
# Assistant threads with artifact_generation: file-gen prompt with session token
# Assistant threads without artifact_generation: empty prompt (regular chat)
session_token = ""
if self.thread_type == "ba_assistant":
    system_prompt = SYSTEM_PROMPT
elif self.thread_type == "assistant" and artifact_generation:
    session_token = str(_uuid.uuid4())
    from app.mcp_server import register_mcp_session
    register_mcp_session(session_token, db, thread_id)
    system_prompt = ASSISTANT_FILE_GEN_PROMPT.format(session_token=session_token)
else:
    system_prompt = ""
```

7. Add session cleanup after the stream completes. In the `finally` block of `_stream_agent_chat()` (or after the stream loop), add:
```python
# Clean up MCP session if registered
if session_token:
    from app.mcp_server import unregister_mcp_session
    unregister_mcp_session(session_token)
```
If there is no existing `finally` block around the stream loop, wrap the `try/except` that contains the `async for chunk in self.adapter.stream_chat(...)` loop with a `try/finally` that calls `unregister_mcp_session` in the `finally`.

**B. In `backend/app/routes/conversations.py`:**

Update the `raw_stream` creation (line ~160) to pass `artifact_generation`:
```python
raw_stream = ai_service.stream_chat(
    conversation,
    thread.project_id,
    thread_id,
    db,
    artifact_generation=body.artifact_generation
)
```

CRITICAL: Do NOT change any other logic in conversations.py. The existing `artifact_generation` handling (skip user message save on line 127, ephemeral instruction append on line 139) must remain unchanged.

CRITICAL: Do NOT alter the BA branch of LOGIC-01. The `if self.thread_type == "ba_assistant": system_prompt = SYSTEM_PROMPT` must remain identical to prevent regression (Pitfall 6 from research).
  </action>
  <verify>
Run: `cd backend && venv/bin/python -c "
from app.services.ai_service import ASSISTANT_FILE_GEN_PROMPT, AIService
# Verify prompt has session_token placeholder
assert '{session_token}' in ASSISTANT_FILE_GEN_PROMPT
# Verify it can be formatted
formatted = ASSISTANT_FILE_GEN_PROMPT.format(session_token='test-token-123')
assert 'test-token-123' in formatted
print('System prompt OK')
"`

Run: `cd backend && venv/bin/python -c "
import inspect
from app.services.ai_service import AIService
sig = inspect.signature(AIService.stream_chat)
assert 'artifact_generation' in sig.parameters
sig2 = inspect.signature(AIService._stream_agent_chat)
assert 'artifact_generation' in sig2.parameters
print('Signatures OK')
"`

Run: `cd backend && venv/bin/python -c "
import ast, inspect
source = open('backend/app/routes/conversations.py').read()
assert 'artifact_generation=body.artifact_generation' in source or 'artifact_generation' in source
print('Conversations wiring OK')
"`
  </verify>
  <done>artifact_generation flows from conversations.py -> stream_chat() -> _stream_agent_chat(); file-gen system prompt with session token is used when artifact_generation=True on assistant threads; session cleanup occurs after stream; BA threads are unchanged.</done>
</task>

</tasks>

<verification>
1. `cd backend && venv/bin/python -c "from app.models import ArtifactType; assert ArtifactType.GENERATED_FILE.value == 'generated_file'"` -- enum exists (from Plan 01)
2. `cd backend && venv/bin/python -c "from app.mcp_server import mcp_app, register_mcp_session"` -- MCP server importable (from Plan 01)
3. `cd backend && venv/bin/python -c "from app.services.llm.claude_cli_adapter import MCP_CONFIG_JSON"` -- CLI adapter has MCP config
4. `cd backend && venv/bin/python -c "from app.services.ai_service import ASSISTANT_FILE_GEN_PROMPT"` -- system prompt exists
5. `cd backend && venv/bin/python -c "import inspect; from app.services.ai_service import AIService; assert 'artifact_generation' in inspect.signature(AIService.stream_chat).parameters"` -- parameter threaded
6. Grep: `grep -c 'mcp-config' backend/app/services/llm/claude_cli_adapter.py` returns 3 (one per spawn path) plus the constant definition
7. Grep: `grep 'ARTIFACT_CREATED' backend/app/services/llm/claude_cli_adapter.py` shows detection code in _translate_event
</verification>

<success_criteria>
- artifact_generation=true in POST body triggers the ASSISTANT_FILE_GEN_PROMPT with a UUID session token
- All 3 CLI spawn paths include --mcp-config pointing to http://127.0.0.1:8000/mcp and --tools "" to disable built-in tools
- _translate_event() scans result events for ARTIFACT_CREATED marker and populates chunk metadata
- BA thread system prompt selection is unchanged (no regression)
- Session token is registered before stream and cleaned up after stream completes
- conversations.py passes body.artifact_generation to ai_service.stream_chat()
</success_criteria>

<output>
After completion, create `.planning/phases/72-backend-file-generation/72-02-SUMMARY.md`
</output>
