---
phase: 29-backend-service-tests
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/tests/unit/__init__.py
  - backend/tests/unit/services/__init__.py
  - backend/tests/unit/services/test_conversation_service.py
  - backend/tests/unit/services/test_token_tracking.py
autonomous: true

must_haves:
  truths:
    - "Pure functions can be tested without database or mocks"
    - "Token estimation returns reasonable approximations"
    - "Cost calculation uses correct pricing for known models"
    - "Conversation truncation preserves recent messages"
  artifacts:
    - path: "backend/tests/unit/services/test_conversation_service.py"
      provides: "Pure function tests for conversation_service"
      min_lines: 60
    - path: "backend/tests/unit/services/test_token_tracking.py"
      provides: "Cost calculation tests for token_tracking"
      min_lines: 40
  key_links:
    - from: "test_conversation_service.py"
      to: "app/services/conversation_service.py"
      via: "direct import"
      pattern: "from app.services.conversation_service import"
    - from: "test_token_tracking.py"
      to: "app/services/token_tracking.py"
      via: "direct import"
      pattern: "from app.services.token_tracking import"
---

<objective>
Create unit tests for pure functions in conversation_service and token_tracking modules.

Purpose: Establish test infrastructure for backend services and cover the easiest-to-test pure functions first. This creates the tests/unit/services directory structure and demonstrates testing patterns.

Output: Test files for conversation_service (estimate_tokens, truncate_conversation) and token_tracking (calculate_cost) pure functions with 90%+ coverage on those functions.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/29-backend-service-tests/29-RESEARCH.md
@backend/app/services/conversation_service.py
@backend/app/services/token_tracking.py
@backend/tests/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create unit test directory structure</name>
  <files>
    backend/tests/unit/__init__.py
    backend/tests/unit/services/__init__.py
    backend/tests/unit/services/conftest.py
  </files>
  <action>
Create the directory structure for unit tests:

1. Create `backend/tests/unit/__init__.py` (empty file)
2. Create `backend/tests/unit/services/__init__.py` (empty file)
3. Create `backend/tests/unit/services/conftest.py` with:
   - Import pytest
   - Comment explaining this file can contain service-specific fixtures
   - Note that shared fixtures come from tests/fixtures/ via pytest_plugins

This establishes the test directory structure for all Phase 29 plans.
  </action>
  <verify>
Directory structure exists:
```bash
ls backend/tests/unit/services/
```
Should show __init__.py and conftest.py
  </verify>
  <done>tests/unit/services/ directory exists with proper __init__.py and conftest.py files</done>
</task>

<task type="auto">
  <name>Task 2: Create conversation_service pure function tests</name>
  <files>backend/tests/unit/services/test_conversation_service.py</files>
  <action>
Create test file for conversation_service pure functions:

```python
"""Unit tests for conversation_service pure functions."""

import pytest
from app.services.conversation_service import (
    estimate_tokens,
    estimate_messages_tokens,
    truncate_conversation,
    CHARS_PER_TOKEN,
    MAX_CONTEXT_TOKENS,
)


class TestEstimateTokens:
    """Tests for estimate_tokens function."""

    def test_empty_string_returns_zero(self):
        assert estimate_tokens("") == 0

    def test_short_text_estimation(self):
        # CHARS_PER_TOKEN = 4, so 11 chars = 2 tokens
        text = "Hello world"  # 11 characters
        assert estimate_tokens(text) == 2

    def test_exact_multiple_of_chars_per_token(self):
        text = "a" * 400  # 400 chars = 100 tokens exactly
        assert estimate_tokens(text) == 100

    def test_rounds_down(self):
        text = "a" * 7  # 7 chars = 1 token (rounds down from 1.75)
        assert estimate_tokens(text) == 1


class TestEstimateMessagesTokens:
    """Tests for estimate_messages_tokens function."""

    def test_empty_list_returns_zero(self):
        assert estimate_messages_tokens([]) == 0

    def test_single_message(self):
        messages = [{"role": "user", "content": "a" * 400}]
        assert estimate_messages_tokens(messages) == 100

    def test_multiple_messages(self):
        messages = [
            {"role": "user", "content": "a" * 400},
            {"role": "assistant", "content": "b" * 200},
        ]
        # 400/4 + 200/4 = 100 + 50 = 150
        assert estimate_messages_tokens(messages) == 150

    def test_message_with_missing_content(self):
        messages = [{"role": "user"}]  # No content key
        assert estimate_messages_tokens(messages) == 0

    def test_message_with_list_content(self):
        # Multi-part content (tool results)
        messages = [{
            "role": "user",
            "content": [
                {"type": "text", "content": "a" * 40},
                {"type": "tool_result", "content": "b" * 40},
            ]
        }]
        # 40/4 + 40/4 = 10 + 10 = 20
        assert estimate_messages_tokens(messages) == 20


class TestTruncateConversation:
    """Tests for truncate_conversation function."""

    def test_no_truncation_when_under_limit(self):
        messages = [
            {"role": "user", "content": "Hi"},
            {"role": "assistant", "content": "Hello!"},
        ]
        result = truncate_conversation(messages, 10000)
        assert result == messages

    def test_truncation_keeps_recent_messages(self):
        # Create messages that exceed budget
        messages = [
            {"role": "user", "content": "a" * 4000},      # 1000 tokens
            {"role": "assistant", "content": "b" * 4000}, # 1000 tokens
            {"role": "user", "content": "recent"},        # ~1 token
        ]
        # Budget = 100 * 0.8 = 80 tokens
        result = truncate_conversation(messages, 100)

        # Should have summary + recent message
        assert len(result) == 2
        assert "[System note:" in result[0]["content"]
        assert result[1]["content"] == "recent"

    def test_truncation_adds_summary_with_correct_count(self):
        messages = [
            {"role": "user", "content": "a" * 4000},
            {"role": "assistant", "content": "b" * 4000},
            {"role": "user", "content": "c" * 4000},
            {"role": "user", "content": "recent"},
        ]
        result = truncate_conversation(messages, 100)

        # Summary should mention truncated count
        assert "3 earlier messages" in result[0]["content"]

    def test_no_summary_when_nothing_truncated(self):
        messages = [{"role": "user", "content": "short"}]
        result = truncate_conversation(messages, 10000)

        # No summary message added
        assert len(result) == 1
        assert "[System note:" not in result[0]["content"]

    def test_truncation_uses_80_percent_budget(self):
        # 80% of 1000 = 800 tokens budget
        # Message with 700 tokens should fit, 900 should not
        messages = [
            {"role": "user", "content": "a" * 3600},  # 900 tokens
            {"role": "user", "content": "b" * 2800},  # 700 tokens
        ]
        result = truncate_conversation(messages, 1000)

        # Only the 700-token message should remain (fits in 800 budget)
        # Plus summary
        assert len(result) == 2
        assert result[1]["content"] == "b" * 2800
```

Test structure:
- TestEstimateTokens: 4 tests for basic token estimation
- TestEstimateMessagesTokens: 5 tests including edge cases
- TestTruncateConversation: 5 tests for truncation logic
  </action>
  <verify>
```bash
cd backend && python -m pytest tests/unit/services/test_conversation_service.py -v
```
All tests pass.
  </verify>
  <done>14 tests pass for conversation_service pure functions</done>
</task>

<task type="auto">
  <name>Task 3: Create token_tracking cost calculation tests</name>
  <files>backend/tests/unit/services/test_token_tracking.py</files>
  <action>
Create test file for token_tracking pure functions:

```python
"""Unit tests for token_tracking pure functions."""

import pytest
from decimal import Decimal
from app.services.token_tracking import (
    calculate_cost,
    PRICING,
    DEFAULT_MONTHLY_BUDGET,
)


class TestCalculateCost:
    """Tests for calculate_cost function."""

    def test_known_model_input_cost(self):
        # Claude Sonnet: $3/1M input tokens
        cost = calculate_cost("claude-sonnet-4-5-20250929", 1_000_000, 0)
        assert cost == Decimal("3.00")

    def test_known_model_output_cost(self):
        # Claude Sonnet: $15/1M output tokens
        cost = calculate_cost("claude-sonnet-4-5-20250929", 0, 1_000_000)
        assert cost == Decimal("15.00")

    def test_known_model_combined_cost(self):
        # 1000 input + 500 output
        # Input: 1000/1M * $3 = $0.003
        # Output: 500/1M * $15 = $0.0075
        cost = calculate_cost("claude-sonnet-4-5-20250929", 1000, 500)
        expected = Decimal("0.003") + Decimal("0.0075")
        assert cost == expected

    def test_unknown_model_uses_default_pricing(self):
        # Unknown model should use default pricing
        cost = calculate_cost("unknown-model-xyz", 1_000_000, 0)
        # Default input price is $3/1M
        assert cost == Decimal("3.00")

    def test_zero_tokens_returns_zero(self):
        cost = calculate_cost("claude-sonnet-4-5-20250929", 0, 0)
        assert cost == Decimal("0")

    def test_decimal_precision_maintained(self):
        # Small token counts should maintain precision
        cost = calculate_cost("claude-sonnet-4-5-20250929", 1, 1)
        # 1/1M * $3 = 0.000003
        # 1/1M * $15 = 0.000015
        # Total = 0.000018
        assert cost > Decimal("0")
        assert cost < Decimal("0.001")

    def test_large_token_count(self):
        # 10M tokens
        cost = calculate_cost("claude-sonnet-4-5-20250929", 10_000_000, 10_000_000)
        # 10 * $3 + 10 * $15 = $30 + $150 = $180
        assert cost == Decimal("180.00")


class TestPricingConfig:
    """Tests for pricing configuration constants."""

    def test_default_pricing_exists(self):
        assert "default" in PRICING
        assert "input" in PRICING["default"]
        assert "output" in PRICING["default"]

    def test_claude_sonnet_pricing_exists(self):
        assert "claude-sonnet-4-5-20250929" in PRICING

    def test_default_monthly_budget_is_reasonable(self):
        # Should be a positive Decimal
        assert DEFAULT_MONTHLY_BUDGET > Decimal("0")
        assert DEFAULT_MONTHLY_BUDGET == Decimal("50.00")
```

Test structure:
- TestCalculateCost: 7 tests for cost calculation
- TestPricingConfig: 3 tests for configuration constants
  </action>
  <verify>
```bash
cd backend && python -m pytest tests/unit/services/test_token_tracking.py -v
```
All tests pass.
  </verify>
  <done>10 tests pass for token_tracking pure functions</done>
</task>

</tasks>

<verification>
Run all new unit tests:
```bash
cd backend && python -m pytest tests/unit/services/ -v
```
All 24 tests should pass.

Check coverage on the pure function portions:
```bash
cd backend && python -m pytest tests/unit/services/ --cov=app.services.conversation_service --cov=app.services.token_tracking --cov-report=term-missing
```
Pure functions (estimate_tokens, truncate_conversation, calculate_cost) should have 90%+ coverage.
</verification>

<success_criteria>
1. tests/unit/services/ directory structure created
2. test_conversation_service.py has 14 passing tests
3. test_token_tracking.py has 10 passing tests
4. All tests run without database or external dependencies
5. `pytest tests/unit/services/ -v` completes successfully
</success_criteria>

<output>
After completion, create `.planning/phases/29-backend-service-tests/29-01-SUMMARY.md`
</output>
