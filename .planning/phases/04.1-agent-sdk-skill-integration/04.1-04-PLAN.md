---
phase: 04.1-agent-sdk-skill-integration
plan: 04
type: execute
wave: 3
depends_on: ["04.1-02", "04.1-03"]
files_modified:
  - backend/tests/test_skill_integration.py
  - backend/tests/conftest.py
autonomous: true

must_haves:
  truths:
    - "Tests verify mode question is asked at session start"
    - "Tests verify one-question-at-a-time protocol"
    - "Tests verify ambiguous term clarification"
    - "Tests verify technical redirect behavior"
    - "Tests verify BRD generation follows template"
    - "All skill behavior tests pass"
  artifacts:
    - path: "backend/tests/test_skill_integration.py"
      provides: "Integration tests for skill behaviors"
      exports: ["test_mode_detection", "test_one_question_protocol", "test_ambiguity_clarification", "test_technical_redirect", "test_brd_generation"]
  key_links:
    - from: "backend/tests/test_skill_integration.py"
      to: "backend/app/services/agent_service.py"
      via: "AgentService import"
      pattern: "from app.services.agent_service import AgentService"
    - from: "backend/tests/test_skill_integration.py"
      to: "backend/app/services/skill_loader.py"
      via: "load_skill_prompt import"
      pattern: "from app.services.skill_loader import load_skill_prompt"
---

<objective>
Create integration tests validating skill behaviors.

Purpose: Verify that the business-analyst skill integration works correctly by testing the 6 success criteria: mode detection, one-question protocol, ambiguity clarification, technical redirect, BRD template compliance, and overall skill behavior.

Output: Passing test suite that validates all skill behaviors work as specified.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04.1-agent-sdk-skill-integration/04.1-RESEARCH.md

# Previous plans for context
@.planning/phases/04.1-agent-sdk-skill-integration/04.1-02-PLAN.md
@.planning/phases/04.1-agent-sdk-skill-integration/04.1-03-PLAN.md

# Skill files for expected behaviors
@.claude/business-analyst/SKILL.md

# Existing test patterns
@backend/tests/conftest.py
@backend/tests/test_thread_integration.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create skill integration test fixtures</name>
  <files>
    backend/tests/conftest.py
  </files>
  <action>
Update backend/tests/conftest.py to add fixtures for skill integration testing.

Add the following fixtures at the end of the file:

```python
# ============================================
# Skill Integration Test Fixtures
# ============================================

@pytest.fixture
def skill_prompt():
    """Load the business-analyst skill prompt for testing."""
    from app.services.skill_loader import load_skill_prompt
    return load_skill_prompt()


@pytest.fixture
def mock_agent_response():
    """Factory for creating mock agent responses."""
    def _create_response(text: str, tool_calls: list = None):
        return {
            "text": text,
            "tool_calls": tool_calls or [],
            "usage": {"input_tokens": 100, "output_tokens": 50}
        }
    return _create_response


@pytest.fixture
def sample_discovery_context():
    """Sample context from a discovery conversation."""
    return {
        "business_objective": "Reduce customer onboarding time from 14 days to 2 days",
        "personas": [
            {
                "name": "Account Manager",
                "role": "Manages customer relationships",
                "pain_points": ["Manual data entry", "Slow approval process"],
                "goals": ["Faster customer activation", "More time for strategic work"]
            }
        ],
        "user_flows": [
            "Customer signs contract -> Account Manager creates account -> Customer activates"
        ],
        "success_metrics": [
            "Onboarding time < 2 days",
            "Customer satisfaction > 90%"
        ]
    }


@pytest.fixture
def sample_brd_content():
    """Sample valid BRD content for testing."""
    return """# Business Requirements Document: Customer Onboarding Portal

## Executive Summary
The Customer Onboarding Portal aims to reduce customer onboarding time from 14 days to 2 days.

## Business Context
Current manual process requires 5 different systems and multiple handoffs.

## Business Objectives

### Primary Objective
Reduce onboarding time from 14 days to 2 days by automating data entry and approvals.

### Secondary Objectives
Improve customer satisfaction scores from 75% to 90%.

## User Personas

### Persona 1: Account Manager
- **Demographics:** 30-45 years old, 5+ years experience
- **Role and Responsibilities:** Manages customer relationships, handles onboarding
- **Pain Points:** Manual data entry, slow approval process
- **Goals:** Faster customer activation, more time for strategic work
- **Technical Proficiency:** Intermediate

## User Flows and Journeys

### Flow 1: New Customer Onboarding
**User Persona:** Account Manager
**Business Goal:** Complete customer setup in under 2 days
**User Goal:** Activate customer account quickly

**Steps:**
1. Account Manager receives signed contract
2. System auto-populates customer data from contract
3. Account Manager reviews and confirms
4. Customer receives activation email

## Functional Requirements

### Must-Have Requirements (Priority 0)
1. **Automated Data Entry**
   - Description: System extracts customer data from signed contracts
   - Business rationale: Eliminates manual data entry errors and delays
   - User story: As an Account Manager, I need automated data extraction so that I can onboard customers faster
   - Success criteria: 95% of fields auto-populated correctly

## Business Processes

### Process 1: Customer Onboarding
- **Current State:** 14 days, 5 systems, multiple manual handoffs
- **Future State:** 2 days, single system, automated workflows
- **Process Flow:** Contract -> Auto-extract -> Review -> Activate
- **Stakeholders:** Sales, Account Management, Customer Success

## Stakeholder Analysis

| Stakeholder Group | Role | Key Requirements | Success Criteria | Concerns |
|-------------------|------|------------------|------------------|----------|
| Account Managers | Primary users | Fast onboarding | < 2 days | Learning curve |
| Customers | End beneficiaries | Quick activation | < 48 hours | Data security |

## Success Metrics and KPIs

| KPI | Current State | Target State | Measurement Method | Timeline |
|-----|---------------|--------------|-------------------|----------|
| Onboarding time | 14 days | 2 days | Time tracking | 3 months |
| Customer satisfaction | 75% | 90% | NPS survey | 6 months |

## Regulatory and Compliance Requirements
Data must be stored in accordance with GDPR requirements. Customer consent required for data processing.

## Assumptions and Constraints
**Assumptions:**
- Contracts are digitally signed and machine-readable
- Account Managers have stable internet connectivity

**Constraints:**
- Must integrate with existing CRM system
- Must launch before Q4 2026

## Risks and Mitigation Strategies

| Risk | Impact | Likelihood | Mitigation Strategy |
|------|--------|------------|-------------------|
| Integration delays | High | Medium | Early prototyping with CRM team |
| User adoption resistance | Medium | Low | Training program and champions |

## Next Steps
- Technical team review and architecture design
- Proposal development with cost and timeline estimates
- Customer review and sign-off on requirements
"""
```

These fixtures provide:
- skill_prompt: Loaded skill content for verification
- mock_agent_response: Factory for creating test responses
- sample_discovery_context: Realistic discovery session data
- sample_brd_content: Valid BRD for validation testing
  </action>
  <verify>
Run:
```bash
cd backend && python -c "
import pytest
from tests.conftest import *

# Verify fixtures are defined
print('Fixtures added to conftest.py')
"
```
  </verify>
  <done>
Test fixtures added for skill_prompt, mock_agent_response, sample_discovery_context, and sample_brd_content.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create skill behavior integration tests</name>
  <files>
    backend/tests/test_skill_integration.py
  </files>
  <action>
Create backend/tests/test_skill_integration.py:

```python
"""
Skill Integration Tests.

Tests verify that the business-analyst skill behaviors work correctly:
1. Mode detection at session start
2. One-question-at-a-time protocol
3. Ambiguous term clarification
4. Technical discussion redirect
5. BRD generation follows template
6. Overall skill behavior validation

These are integration tests that verify skill loading and prompt behavior.
For actual AI response testing, use manual verification with real API calls.
"""
import pytest
import re
from app.services.skill_loader import load_skill_prompt, get_skill_references
from app.services.brd_generator import (
    validate_brd_content,
    format_preflight_checklist,
    BRD_REQUIRED_SECTIONS,
    BRD_SECTIONS
)
from app.services.agent_service import AgentService


class TestSkillLoading:
    """Test skill content loading and structure."""

    def test_skill_prompt_loads_successfully(self):
        """Verify skill prompt loads without errors."""
        prompt = load_skill_prompt()
        assert prompt is not None
        assert len(prompt) > 1000  # Skill + references should be substantial

    def test_skill_prompt_contains_core_identity(self):
        """Verify skill prompt contains business-analyst identity."""
        prompt = load_skill_prompt()
        assert "business analyst" in prompt.lower()
        assert "brd" in prompt.lower() or "business requirements document" in prompt.lower()

    def test_skill_references_loaded(self):
        """Verify all reference files are loaded."""
        refs = get_skill_references()
        expected_refs = [
            "discovery-framework",
            "brd-template",
            "tone-guidelines",
            "error-protocols"
        ]
        for ref in expected_refs:
            assert ref in refs, f"Missing reference: {ref}"

    def test_skill_prompt_contains_references(self):
        """Verify combined prompt includes reference content."""
        prompt = load_skill_prompt()
        # Check for content from each reference file
        assert "discovery priority sequence" in prompt.lower()  # discovery-framework
        assert "executive summary" in prompt.lower()  # brd-template
        assert "consultative" in prompt.lower()  # tone-guidelines


class TestModeDetection:
    """Test mode detection behavior (Success Criteria #1)."""

    def test_skill_includes_mode_question(self):
        """Verify skill prompt contains mode detection question."""
        prompt = load_skill_prompt()
        # Should contain the mode question or its key elements
        assert "meeting mode" in prompt.lower() or "document refinement mode" in prompt.lower()

    def test_skill_includes_mode_options(self):
        """Verify skill prompt includes both mode options."""
        prompt = load_skill_prompt()
        prompt_lower = prompt.lower()
        # Check for both modes
        has_meeting = "meeting mode" in prompt_lower or "live discovery" in prompt_lower
        has_refinement = "document refinement" in prompt_lower or "modifying an existing" in prompt_lower
        assert has_meeting, "Missing Meeting Mode reference"
        assert has_refinement, "Missing Document Refinement Mode reference"


class TestOneQuestionProtocol:
    """Test one-question-at-a-time protocol (Success Criteria #2)."""

    def test_skill_mandates_one_question(self):
        """Verify skill prompt enforces one question at a time."""
        prompt = load_skill_prompt()
        prompt_lower = prompt.lower()
        # Should contain explicit mandate
        assert "one question at a time" in prompt_lower
        assert "never batch" in prompt_lower or "never combine" in prompt_lower or "never batch multiple" in prompt_lower

    def test_skill_includes_question_format(self):
        """Verify skill prompt includes question format requirements."""
        prompt = load_skill_prompt()
        prompt_lower = prompt.lower()
        # Should require rationale and options
        assert "rationale" in prompt_lower
        assert "suggested answer options" in prompt_lower or "three suggested" in prompt_lower or "3 answer options" in prompt_lower.replace("three", "3")

    def test_skill_includes_question_example(self):
        """Verify skill prompt includes example questions."""
        prompt = load_skill_prompt()
        # Should have example questions with options (A), (B), (C)
        has_options = "(a)" in prompt.lower() and "(b)" in prompt.lower() and "(c)" in prompt.lower()
        assert has_options, "Missing example question with A/B/C options"


class TestAmbiguityClarification:
    """Test ambiguous term clarification (Success Criteria #3)."""

    def test_skill_includes_zero_assumption_protocol(self):
        """Verify skill prompt includes zero-assumption protocol."""
        prompt = load_skill_prompt()
        prompt_lower = prompt.lower()
        assert "zero-assumption" in prompt_lower or "never assume" in prompt_lower

    def test_skill_lists_ambiguous_terms(self):
        """Verify skill prompt lists common ambiguous terms."""
        prompt = load_skill_prompt()
        prompt_lower = prompt.lower()
        ambiguous_terms = ["seamless", "intuitive", "scalable", "user-friendly"]
        found_terms = sum(1 for term in ambiguous_terms if term in prompt_lower)
        assert found_terms >= 2, f"Should list at least 2 ambiguous terms, found {found_terms}"

    def test_skill_includes_clarification_format(self):
        """Verify skill prompt includes clarification format."""
        prompt = load_skill_prompt()
        # Should include the clarification template
        assert "when you say" in prompt.lower()
        assert "do you mean" in prompt.lower()


class TestTechnicalRedirect:
    """Test technical discussion redirect (Success Criteria #4)."""

    def test_skill_includes_technical_boundary(self):
        """Verify skill prompt includes technical boundary enforcement."""
        prompt = load_skill_prompt()
        prompt_lower = prompt.lower()
        assert "technical" in prompt_lower
        assert "redirect" in prompt_lower or "boundary" in prompt_lower

    def test_skill_lists_technical_topics_to_avoid(self):
        """Verify skill prompt lists technical topics to avoid."""
        prompt = load_skill_prompt()
        prompt_lower = prompt.lower()
        technical_topics = ["react", "vue", "python", "java", "database", "api", "microservices", "architecture"]
        found_topics = sum(1 for topic in technical_topics if topic in prompt_lower)
        assert found_topics >= 3, f"Should list at least 3 technical topics to avoid, found {found_topics}"

    def test_skill_includes_redirect_template(self):
        """Verify skill prompt includes redirect response template."""
        prompt = load_skill_prompt()
        # Should include redirect phrase
        assert "technical team" in prompt.lower()
        assert "business perspective" in prompt.lower()


class TestBRDGeneration:
    """Test BRD generation follows template (Success Criteria #5)."""

    def test_brd_required_sections_defined(self):
        """Verify required BRD sections are defined."""
        assert len(BRD_REQUIRED_SECTIONS) >= 5
        assert "Executive Summary" in BRD_REQUIRED_SECTIONS
        assert "Business Objectives" in BRD_REQUIRED_SECTIONS
        assert "User Personas" in BRD_REQUIRED_SECTIONS

    def test_brd_validation_catches_missing_sections(self):
        """Verify BRD validation catches missing required sections."""
        incomplete_brd = """# Business Requirements Document
## Executive Summary
This is a summary.
"""
        result = validate_brd_content(incomplete_brd)
        assert not result.is_valid
        assert len(result.missing_sections) > 0
        assert "Business Objectives" in result.missing_sections or "User Personas" in result.missing_sections

    def test_brd_validation_accepts_complete_brd(self, sample_brd_content):
        """Verify BRD validation accepts complete BRD."""
        result = validate_brd_content(sample_brd_content)
        assert result.is_valid, f"Valid BRD rejected. Missing: {result.missing_sections}"
        assert result.section_count >= 5

    def test_brd_validation_warns_on_placeholders(self):
        """Verify BRD validation warns on placeholder text."""
        brd_with_placeholder = """# Business Requirements Document
## Executive Summary
[TBD]
## Business Objectives
### Primary Objective
[To be determined]
## User Personas
### Persona 1
[Insert persona details]
## Functional Requirements
[Add requirements]
## Success Metrics
[TODO]
"""
        result = validate_brd_content(brd_with_placeholder)
        assert len(result.warnings) > 0

    def test_brd_validation_warns_on_technical_terms(self):
        """Verify BRD validation warns on technical implementation terms."""
        brd_with_tech = """# Business Requirements Document
## Executive Summary
We will build this using React and PostgreSQL.
## Business Objectives
### Primary Objective
Deploy on AWS using Kubernetes.
## User Personas
### Persona 1
Developer using Python.
## Functional Requirements
Build a microservices API.
## Success Metrics
API response time under 200ms.
"""
        result = validate_brd_content(brd_with_tech)
        assert len(result.warnings) > 0
        # Should flag multiple technical terms
        warning_text = " ".join(result.warnings).lower()
        assert "react" in warning_text or "python" in warning_text or "aws" in warning_text

    def test_preflight_checklist_formatting(self):
        """Verify preflight checklist formats correctly."""
        # All passed
        passed, checklist = format_preflight_checklist(
            has_objective=True,
            has_personas=True,
            has_flows=True,
            has_metrics=True,
            has_requirements=True,
            has_stakeholders=True
        )
        assert passed
        assert "[x]" in checklist
        assert "[ ]" not in checklist

        # Some missing
        passed, checklist = format_preflight_checklist(
            has_objective=True,
            has_personas=False,
            has_flows=True,
            has_metrics=False,
            has_requirements=True,
            has_stakeholders=True
        )
        assert not passed
        assert "[ ]" in checklist


class TestAgentServiceIntegration:
    """Test AgentService integration with skill."""

    def test_agent_service_loads_skill(self):
        """Verify AgentService loads skill prompt."""
        service = AgentService()
        assert service.skill_prompt is not None
        assert len(service.skill_prompt) > 1000

    def test_agent_service_has_tools(self):
        """Verify AgentService has required tools."""
        service = AgentService()
        assert service.tools_server is not None
        # Tools should be registered

    def test_agent_service_skill_contains_protocols(self):
        """Verify AgentService skill prompt contains key protocols."""
        service = AgentService()
        prompt = service.skill_prompt.lower()
        assert "one question at a time" in prompt
        assert "zero-assumption" in prompt or "never assume" in prompt
        assert "technical" in prompt and "redirect" in prompt or "boundary" in prompt


class TestSkillBehaviorValidation:
    """End-to-end skill behavior validation (Success Criteria #6)."""

    def test_skill_has_session_management(self):
        """Verify skill includes session management guidance."""
        prompt = load_skill_prompt()
        prompt_lower = prompt.lower()
        assert "session" in prompt_lower
        assert "context" in prompt_lower or "cumulative" in prompt_lower

    def test_skill_has_contradiction_detection(self):
        """Verify skill includes contradiction detection."""
        prompt = load_skill_prompt()
        assert "contradiction" in prompt.lower()

    def test_skill_has_understanding_verification(self):
        """Verify skill includes understanding verification."""
        prompt = load_skill_prompt()
        prompt_lower = prompt.lower()
        assert "verify" in prompt_lower or "verification" in prompt_lower
        assert "understanding" in prompt_lower

    def test_skill_has_professional_tone(self):
        """Verify skill includes professional tone guidance."""
        prompt = load_skill_prompt()
        prompt_lower = prompt.lower()
        assert "professional" in prompt_lower or "consultative" in prompt_lower
        assert "tone" in prompt_lower

    def test_skill_has_error_handling(self):
        """Verify skill includes error handling protocols."""
        prompt = load_skill_prompt()
        prompt_lower = prompt.lower()
        assert "error" in prompt_lower
        # Should have guidance for common issues
        has_customer_guidance = "cannot articulate" in prompt_lower or "unfamiliar" in prompt_lower
        has_incomplete_guidance = "incomplete" in prompt_lower
        assert has_customer_guidance or has_incomplete_guidance
```

This test file:
- Tests skill loading (prompt and references)
- Tests mode detection (SC #1)
- Tests one-question protocol (SC #2)
- Tests ambiguity clarification (SC #3)
- Tests technical redirect (SC #4)
- Tests BRD template compliance (SC #5)
- Tests overall skill behavior (SC #6)
  </action>
  <verify>
Run:
```bash
cd backend && python -m pytest tests/test_skill_integration.py -v --tb=short
```

All tests should pass. If any fail, fix the underlying issue before proceeding.
  </verify>
  <done>
Integration test suite validates all 6 success criteria for skill behavior. Tests verify skill loading, mode detection, one-question protocol, ambiguity clarification, technical redirect, and BRD generation.
  </done>
</task>

</tasks>

<verification>
Run the full test suite:
```bash
cd backend && python -m pytest tests/test_skill_integration.py -v
```

Expected results:
1. TestSkillLoading: All 4 tests pass (skill loads, contains identity, references loaded, references in prompt)
2. TestModeDetection: All 2 tests pass (mode question present, both modes included)
3. TestOneQuestionProtocol: All 3 tests pass (mandate, format, examples)
4. TestAmbiguityClarification: All 3 tests pass (protocol, terms, format)
5. TestTechnicalRedirect: All 3 tests pass (boundary, topics, template)
6. TestBRDGeneration: All 6 tests pass (sections, validation, placeholders, tech terms, checklist)
7. TestAgentServiceIntegration: All 3 tests pass (loads skill, has tools, contains protocols)
8. TestSkillBehaviorValidation: All 5 tests pass (session, contradiction, verification, tone, errors)
</verification>

<success_criteria>
- Test fixtures defined in conftest.py
- test_skill_integration.py covers all 6 success criteria
- TestSkillLoading verifies skill files load correctly
- TestModeDetection verifies mode question behavior
- TestOneQuestionProtocol verifies question format enforcement
- TestAmbiguityClarification verifies zero-assumption protocol
- TestTechnicalRedirect verifies boundary enforcement
- TestBRDGeneration verifies template compliance
- TestAgentServiceIntegration verifies service configuration
- TestSkillBehaviorValidation verifies overall behavior
- All tests pass: `pytest tests/test_skill_integration.py`
</success_criteria>

<output>
After completion, create `.planning/phases/04.1-agent-sdk-skill-integration/04.1-04-SUMMARY.md`
</output>
