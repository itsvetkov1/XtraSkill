---
phase: 54-backend-foundation-document-parsing-security
plan: 03
type: execute
wave: 3
depends_on: ["54-01", "54-02"]
files_modified:
  - backend/app/routes/documents.py
  - backend/app/services/document_search.py
autonomous: true

must_haves:
  truths:
    - "User can upload Excel (.xlsx) files and see extracted text content via GET /documents/{id}"
    - "User can upload CSV files with international characters and see extracted text"
    - "User can upload PDF files and see extracted text with page information"
    - "User can upload Word (.docx) files and see extracted text with paragraph structure"
    - "Malicious files (wrong magic numbers, zip bombs, oversized files) are rejected with clear error messages"
    - "Uploaded rich documents appear in FTS5 full-text search results"
    - "Existing text document upload still works (backward compatible)"
    - "Document list endpoint returns content_type and metadata for each document"
    - "GET document endpoint returns content_text for rich documents (not decrypted binary)"
    - "GET document endpoint returns original binary for download via new download endpoint"
  artifacts:
    - path: "backend/app/routes/documents.py"
      provides: "Updated upload endpoint accepting rich formats, download endpoint for binary files, content_type in responses"
      contains: "ParserFactory"
    - path: "backend/app/services/document_search.py"
      provides: "Updated index_document function (unchanged API, consumed by updated route)"
      contains: "index_document"
  key_links:
    - from: "backend/app/routes/documents.py"
      to: "backend/app/services/document_parser/__init__.py"
      via: "ParserFactory.get_parser() called in upload endpoint"
      pattern: "ParserFactory\\.get_parser"
    - from: "backend/app/routes/documents.py"
      to: "backend/app/services/file_validator.py"
      via: "validate_file_security() called before parsing"
      pattern: "validate_file_security"
    - from: "backend/app/routes/documents.py"
      to: "backend/app/services/encryption.py"
      via: "encrypt_binary() for rich docs, encrypt_document() for text (backward compat)"
      pattern: "encrypt_binary|encrypt_document"
    - from: "backend/app/routes/documents.py"
      to: "backend/app/services/document_search.py"
      via: "index_document() called with extracted text for FTS5"
      pattern: "index_document"
---

<objective>
Integrate parsers, validators, and new schema into the document upload/read/search routes.

Purpose: Wire everything together: the upload endpoint accepts rich formats (via ParserFactory), validates security (via file_validator), extracts text, encrypts binary content, stores dual-column data, and indexes for search. The get/list endpoints return content_type and metadata. A new download endpoint serves original binary files.

Output: Fully functional document upload supporting all 6 content types, security validation, dual-column storage, FTS5 indexing, and a binary download endpoint.
</objective>

<execution_context>
@/Users/a1testingmac/.claude/get-shit-done/workflows/execute-plan.md
@/Users/a1testingmac/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/54-backend-foundation-document-parsing-security/54-RESEARCH.md
@.planning/phases/54-backend-foundation-document-parsing-security/54-01-SUMMARY.md
@.planning/phases/54-backend-foundation-document-parsing-security/54-02-SUMMARY.md

# Files being modified
@backend/app/routes/documents.py
@backend/app/services/document_search.py

# Files being consumed (read for integration)
@backend/app/services/document_parser/__init__.py
@backend/app/services/file_validator.py
@backend/app/services/encryption.py
@backend/app/models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update upload endpoint for rich document formats with security validation and dual-column storage</name>
  <files>
    backend/app/routes/documents.py
  </files>
  <action>
Rewrite the upload endpoint in `backend/app/routes/documents.py` to support all 6 content types with security validation, format-specific parsing, and dual-column storage.

**1. Update imports:**

Replace the existing imports section with:
```python
import json
from typing import List, Optional
from fastapi import APIRouter, Depends, File, HTTPException, Response, status, UploadFile
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.database import get_db
from app.models import Document, Project, User
from app.routes.auth import get_current_user
from app.services.encryption import get_encryption_service
from app.services.document_search import index_document, search_documents
from app.services.document_parser import ParserFactory
from app.services.file_validator import validate_file_security
```

**2. Update constants:**

Replace `MAX_FILE_SIZE = 1024 * 1024` and `ALLOWED_CONTENT_TYPES` with:
```python
# File upload constraints — rich documents up to 10MB
ALLOWED_CONTENT_TYPES = ParserFactory.ALL_CONTENT_TYPES
```

(MAX_FILE_SIZE is now handled by file_validator.py centrally)

**3. Rewrite `upload_document` endpoint:**

The new flow:
1. Verify project ownership (unchanged)
2. Validate content type against `ALLOWED_CONTENT_TYPES`
3. Read file bytes
4. Call `validate_file_security(content_bytes, file.content_type)` — handles size, magic numbers, zip bombs
5. Get parser: `parser = ParserFactory.get_parser(file.content_type)`
6. Call `parser.validate_security(content_bytes)` — format-specific security check
7. Parse: `parsed = parser.parse(content_bytes)` — returns {text, summary, metadata}
8. Encrypt for storage:
   - For rich formats (`ParserFactory.is_rich_format(file.content_type)`): use `encrypt_binary(content_bytes)` — store original binary
   - For text formats: use `encrypt_document(parsed["text"])` — store plaintext (backward compatible)
9. Create Document with all columns:
   ```python
   doc = Document(
       project_id=project_id,
       filename=file.filename or "untitled",
       content_type=file.content_type,
       content_encrypted=encrypted,
       content_text=parsed["text"],
       metadata_json=json.dumps(parsed["metadata"]) if parsed["metadata"] else None,
   )
   ```
10. Index for FTS5: `await index_document(db, doc.id, doc.filename, parsed["text"])`
11. Commit and return response including content_type and metadata

Return format:
```python
return {
    "id": doc.id,
    "filename": doc.filename,
    "content_type": doc.content_type,
    "metadata": parsed["metadata"],
    "created_at": doc.created_at.isoformat()
}
```

**4. Update `list_documents` endpoint:**

Add content_type and metadata to the response:
```python
return [
    {
        "id": doc.id,
        "filename": doc.filename,
        "content_type": doc.content_type or "text/plain",
        "metadata": json.loads(doc.metadata_json) if doc.metadata_json else None,
        "created_at": doc.created_at.isoformat()
    }
    for doc in documents
]
```

**5. Update `get_document` endpoint:**

Return content_text for ALL documents (rich and text). For rich documents, the "content" field should be the extracted text (not the decrypted binary, which is meaningless as text). For legacy text documents where content_text is NULL, fall back to decrypting content_encrypted.

```python
# Get content text
if doc.content_text is not None:
    content = doc.content_text
else:
    # Legacy document — decrypt from content_encrypted (text format)
    content = get_encryption_service().decrypt_document(doc.content_encrypted)

return {
    "id": doc.id,
    "filename": doc.filename,
    "content_type": doc.content_type or "text/plain",
    "content": content,
    "metadata": json.loads(doc.metadata_json) if doc.metadata_json else None,
    "created_at": doc.created_at.isoformat()
}
```

**6. Add new `download_document` endpoint** for binary file download:

```python
@router.get("/documents/{document_id}/download")
async def download_document(
    document_id: str,
    current_user: dict = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Download original document file.

    For rich documents (XLSX, CSV, PDF, DOCX): returns original binary.
    For text documents: returns plain text content.
    """
    # Get document with project join to verify ownership
    stmt = select(Document).join(Project).where(
        Document.id == document_id,
        Project.user_id == current_user["user_id"]
    )
    doc = (await db.execute(stmt)).scalar_one_or_none()
    if not doc:
        raise HTTPException(status_code=404, detail="Document not found")

    content_type = doc.content_type or "text/plain"

    if ParserFactory.is_rich_format(content_type):
        # Rich document — decrypt as binary
        file_bytes = get_encryption_service().decrypt_binary(doc.content_encrypted)
        return Response(
            content=file_bytes,
            media_type=content_type,
            headers={
                "Content-Disposition": f'attachment; filename="{doc.filename}"'
            }
        )
    else:
        # Text document — decrypt as text
        plaintext = get_encryption_service().decrypt_document(doc.content_encrypted)
        return Response(
            content=plaintext.encode('utf-8'),
            media_type=content_type,
            headers={
                "Content-Disposition": f'attachment; filename="{doc.filename}"'
            }
        )
```

IMPORTANT considerations:
- The error message for unsupported content type should list supported formats: "Unsupported file type. Supported: .txt, .md, .xlsx, .csv, .pdf, .docx"
- The upload endpoint should handle parser exceptions gracefully — wrap `parser.parse()` in try/except and return a 400 with "Failed to parse document: {error}" detail
- The security validation (file_validator + parser.validate_security) MUST run BEFORE the parse() call
  </action>
  <verify>
```bash
cd backend && source venv/bin/activate
# Start the backend server temporarily to test endpoints
python -c "
# Verify imports work correctly
from app.routes.documents import router, ALLOWED_CONTENT_TYPES
print(f'Allowed types: {ALLOWED_CONTENT_TYPES}')
print(f'Route count: {len(router.routes)}')

# Verify new routes exist
route_paths = [r.path for r in router.routes]
print(f'Routes: {route_paths}')

# Check that download endpoint exists
assert any('download' in str(p) for p in route_paths), 'Download route missing'
print('Download route present')

# Check ParserFactory import works in routes
from app.services.document_parser import ParserFactory
from app.services.file_validator import validate_file_security
print('All imports successful')
"
```
  </verify>
  <done>
    - Upload endpoint accepts all 6 content types (xlsx, csv, pdf, docx, txt, md)
    - Security validation runs before parsing (file_validator + parser.validate_security)
    - Rich documents encrypted as binary, text documents encrypted as text (backward compat)
    - Dual-column storage: content_encrypted (original) + content_text (extracted)
    - FTS5 indexing of extracted text for all formats
    - Upload response includes content_type and metadata
    - List endpoint includes content_type and metadata per document
    - Get endpoint returns extracted text (content_text) for all documents
    - New download endpoint serves original binary for rich documents
    - Error messages are clear and user-friendly
  </done>
</task>

<task type="auto">
  <name>Task 2: Verify full integration with end-to-end test via API</name>
  <files>
    backend/app/services/document_search.py
  </files>
  <action>
**1. Verify document_search.py needs no changes:**

The existing `index_document(db, doc_id, filename, content)` function works unchanged — it receives plaintext content and inserts into FTS5. The caller (upload route) now passes extracted text from parsers instead of raw file content. The `search_documents()` function also works unchanged since the FTS5 schema columns haven't changed (document_id, filename, content).

Review `document_search.py` and confirm no changes needed. If the function signatures match what Plan 54-03 Task 1 expects, leave the file as-is.

**2. Run end-to-end integration verification:**

Create a test script that exercises the full flow (create in /tmp, run, then delete):

```python
"""End-to-end integration test for rich document support."""
import asyncio
import io
import json
import sys

# Add backend to path
sys.path.insert(0, '.')

async def test_integration():
    from app.database import init_db, engine, AsyncSessionLocal
    from app.services.document_parser import ParserFactory
    from app.services.file_validator import validate_file_security

    print("1. Database initialization...")
    await init_db()
    print("   OK")

    print("2. Testing text parser (backward compat)...")
    parser = ParserFactory.get_parser("text/plain")
    result = parser.parse(b"Hello world\nLine 2")
    assert result["text"] == "Hello world\nLine 2"
    assert result["metadata"]["format"] == "Text"
    print(f"   OK: text={result['text'][:30]}")

    print("3. Testing CSV parser with UTF-8...")
    csv_bytes = "Name,City\nJosé,São Paulo\nMüller,München".encode('utf-8')
    parser = ParserFactory.get_parser("text/csv")
    result = parser.parse(csv_bytes)
    assert "José" in result["text"]
    assert "São Paulo" in result["text"]
    print(f"   OK: text={result['text'][:50]}")

    print("4. Testing file size validation...")
    try:
        validate_file_security(b"x" * (11 * 1024 * 1024), "text/plain")
        print("   FAIL: Should reject oversized file")
    except Exception as e:
        print(f"   OK: Rejected with: {e.detail}")

    print("5. Testing ParserFactory routing...")
    for ct in ParserFactory.ALL_CONTENT_TYPES:
        p = ParserFactory.get_parser(ct)
        print(f"   {ct} -> {type(p).__name__}")

    print("6. Testing unsupported type rejection...")
    try:
        ParserFactory.get_parser("application/octet-stream")
        print("   FAIL: Should reject unsupported type")
    except ValueError as e:
        print(f"   OK: Rejected with: {e}")

    print("\nAll integration tests passed!")

asyncio.run(test_integration())
```

Run this test script to verify the full integration works. Then delete the test script.

**3. If document_search.py does need any updates** (unlikely based on analysis):

The only potential change: if the search query needs to handle metadata or content_type filtering, that would be a Phase 55 concern (frontend/AI integration), not Phase 54. Leave document_search.py unchanged for now.
  </action>
  <verify>
```bash
cd backend && source venv/bin/activate
# Quick smoke test of the full integration
python -c "
import asyncio
from app.database import init_db
from app.services.document_parser import ParserFactory
from app.services.file_validator import validate_file_security

async def smoke():
    await init_db()

    # CSV with encoding
    csv = 'a,b\n1,2'.encode('utf-8')
    p = ParserFactory.get_parser('text/csv')
    r = p.parse(csv)
    assert r['text'] and r['metadata']['row_count'] == 2

    # Text backward compat
    p = ParserFactory.get_parser('text/plain')
    r = p.parse(b'hello')
    assert r['text'] == 'hello'

    # Size rejection
    try:
        validate_file_security(b'x' * (11*1024*1024), 'text/plain')
        assert False, 'Should reject'
    except:
        pass

    print('Integration smoke test: ALL PASSED')

asyncio.run(smoke())
"
```
  </verify>
  <done>
    - document_search.py confirmed unchanged (index_document API compatible with new upload flow)
    - End-to-end integration verified: parser -> validator -> model -> FTS5 indexing
    - CSV encoding detection works with international characters
    - File size validation rejects oversized files
    - ParserFactory routes all content types correctly
    - Backward compatibility maintained for plain text documents
  </done>
</task>

</tasks>

<verification>
**Phase-level integration checks (run after all plans complete):**

1. **Upload Excel:** Create a simple .xlsx file, upload via `POST /projects/{id}/documents`, verify response includes content_type and metadata with sheet_names
2. **Upload CSV:** Upload a CSV with international characters, verify text extraction preserves encoding
3. **Upload PDF:** Upload a PDF, verify text extraction includes [Page N] markers
4. **Upload Word:** Upload a .docx, verify paragraph text extracted
5. **Upload text (backward compat):** Upload a .txt file, verify existing behavior unchanged
6. **Search integration:** After uploading rich documents, verify `GET /projects/{id}/documents/search?q=term` returns results from rich document content
7. **Security - oversized:** Upload an 11MB file, verify 413 response
8. **Security - wrong type:** Send a renamed .txt as .xlsx content type, verify rejection
9. **Download:** GET `/documents/{id}/download` returns original binary for rich documents
10. **List with metadata:** GET `/projects/{id}/documents` includes content_type and metadata for all documents
</verification>

<success_criteria>
- Upload endpoint accepts .xlsx, .csv, .pdf, .docx, .txt, .md files
- Security validation rejects oversized, spoofed, and malicious files before parsing
- Rich documents stored as encrypted binary with extracted text in content_text
- FTS5 search finds content from all document formats
- Document list includes content_type and metadata
- Document get returns extracted text (not binary) for rich formats
- Document download returns original file for rich formats
- Existing text upload flow unchanged (backward compatible)
</success_criteria>

<output>
After completion, create `.planning/phases/54-backend-foundation-document-parsing-security/54-03-SUMMARY.md`
</output>
