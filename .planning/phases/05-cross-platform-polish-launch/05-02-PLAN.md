---
phase: 05-cross-platform-polish-launch
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - frontend/test/widget/project_list_screen_test.dart
  - frontend/test/widget/document_list_screen_test.dart
  - frontend/test/widget/conversation_screen_test.dart
  - frontend/test/widget/login_screen_test.dart
  - backend/tests/test_backend_integration.py
autonomous: true

must_haves:
  truths:
    - Core UI components (project list, document list, conversation) render without crashes
    - Tests catch regressions in project/document list rendering
    - Backend integration tests cover all critical API flows
  artifacts:
    - path: "frontend/test/widget/project_list_screen_test.dart"
      provides: "Widget tests for project list UI"
      min_lines: 40
    - path: "frontend/test/widget/conversation_screen_test.dart"
      provides: "Widget tests for chat UI"
      min_lines: 40
    - path: "backend/tests/test_backend_integration.py"
      provides: "Comprehensive backend integration tests"
      min_lines: 200
  key_links:
    - from: "frontend/test/widget/project_list_screen_test.dart"
      to: "ProjectListScreen widget"
      via: "testWidgets with pumpWidget"
      pattern: "testWidgets.*ProjectListScreen"
    - from: "backend/tests/test_backend_integration.py"
      to: "API endpoints"
      via: "AsyncClient test fixture"
      pattern: "@pytest\\.mark\\.asyncio"
---

<objective>
Expand test coverage with widget tests for core Flutter screens and consolidate backend integration tests to catch regressions before production deployment.

Purpose: Establish automated quality gates that verify UI rendering, state management, and API contracts, reducing manual testing burden and catching breaking changes early.

Output: Widget tests for 4 critical screens (login, projects, documents, conversation) and comprehensive backend integration test suite covering all API endpoints.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-cross-platform-polish-launch/05-RESEARCH.md

# Existing test patterns
@frontend/test/integration/auth_flow_test.dart
@backend/tests/test_auth_integration.py
@backend/tests/conftest.py

# Screens to test
@frontend/lib/screens/auth/login_screen.dart
@frontend/lib/screens/projects/project_list_screen.dart
@frontend/lib/screens/documents/document_list_screen.dart
@frontend/lib/screens/conversation/conversation_screen.dart
</context>

<tasks>

<task type="auto">
  <name>Create widget tests for core Flutter screens</name>
  <files>
    frontend/test/widget/login_screen_test.dart
    frontend/test/widget/project_list_screen_test.dart
    frontend/test/widget/document_list_screen_test.dart
    frontend/test/widget/conversation_screen_test.dart
  </files>
  <action>
    Create frontend/test/widget/ directory if it doesn't exist.

    Write widget tests following Flutter testing pyramid (20% widget tests - focus on critical UI).

    **login_screen_test.dart:**
    ```dart
    import 'package:flutter/material.dart';
    import 'package:flutter_test/flutter_test.dart';
    import 'package:frontend/screens/auth/login_screen.dart';

    void main() {
      testWidgets('Login screen displays OAuth buttons', (tester) async {
        await tester.pumpWidget(const MaterialApp(home: LoginScreen()));

        expect(find.text('Sign in with Google'), findsOneWidget);
        expect(find.text('Sign in with Microsoft'), findsOneWidget);
        expect(find.byIcon(Icons.login), findsNWidgets(2));
      });

      testWidgets('Login screen shows app branding', (tester) async {
        await tester.pumpWidget(const MaterialApp(home: LoginScreen()));

        expect(find.text('Business Analyst Assistant'), findsOneWidget);
      });
    }
    ```

    **project_list_screen_test.dart:**
    Mock ProjectProvider using mockito (already in dev_dependencies). Test:
    - Empty state displays when no projects
    - Loading state shows skeleton (if Skeletonizer added in 05-01)
    - Project cards render with name and description
    - FloatingActionButton present for create action
    - Error state shows when provider has error

    Pattern:
    ```dart
    testWidgets('Shows empty state when no projects', (tester) async {
      final mockProvider = MockProjectProvider();
      when(mockProvider.projects).thenReturn([]);
      when(mockProvider.isLoading).thenReturn(false);

      await tester.pumpWidget(
        MaterialApp(
          home: ChangeNotifierProvider<ProjectProvider>.value(
            value: mockProvider,
            child: ProjectListScreen(),
          ),
        ),
      );

      expect(find.text('No projects yet'), findsOneWidget);
    });
    ```

    **document_list_screen_test.dart:**
    Mock DocumentProvider. Test:
    - Empty state when no documents
    - Document list renders with file names
    - Upload button present
    - Loading and error states

    **conversation_screen_test.dart:**
    Mock ConversationProvider. Test:
    - Message bubbles render for user and assistant roles
    - Chat input field present
    - Send button present
    - Loading state shows during streaming
    - Empty state shows "Start conversation"

    Use mockito for all provider mocks. Generate mocks with:
    ```dart
    // At top of test file
    @GenerateNiceMocks([MockSpec<ProjectProvider>()])
    import 'project_list_screen_test.mocks.dart';
    ```

    Run `flutter pub run build_runner build` after creating test files to generate mocks.

    Keep tests focused on UI rendering, NOT business logic (that's for unit tests). Follow test pyramid: many simple widget tests, not complex integration scenarios.
  </action>
  <verify>
    1. flutter pub run build_runner build (generates mocks)
    2. flutter test test/widget/ (all widget tests pass)
    3. flutter analyze (no issues)
    4. Count test files: `ls frontend/test/widget/*.dart | wc -l` (should be 4)
  </verify>
  <done>
    - login_screen_test.dart validates OAuth buttons and branding display
    - project_list_screen_test.dart validates empty/loading/loaded/error states
    - document_list_screen_test.dart validates document list rendering and states
    - conversation_screen_test.dart validates message display and input UI
    - All widget tests use mockito for provider mocking
    - All tests pass with `flutter test test/widget/`
  </done>
</task>

<task type="auto">
  <name>Consolidate and expand backend integration tests</name>
  <files>
    backend/tests/test_backend_integration.py
  </files>
  <action>
    Create comprehensive backend integration test file consolidating all critical API flows. Current test_auth_integration.py has 14 tests (good start). Expand coverage to all endpoints.

    Structure tests by API resource:

    **Authentication (14 tests - already exist, consolidate here):**
    - JWT creation and verification
    - Google/Microsoft OAuth initiate endpoints
    - State parameter uniqueness (CSRF protection)
    - Protected endpoints require authentication (403 without token)
    - Protected endpoints work with valid token (200)
    - Invalid tokens rejected (401)
    - User creation on first login
    - User update on subsequent login
    - Logout endpoint

    **Projects (8 tests):**
    - Create project with valid data returns 201
    - Create project with empty name returns 422
    - List projects returns user's projects only (not other users')
    - Get project by ID returns project details
    - Get project by ID with wrong user returns 404
    - Update project name and description returns 200
    - Update non-existent project returns 404
    - Projects ordered by updated_at DESC

    **Documents (7 tests):**
    - Upload document to project returns 201
    - Upload document with invalid project_id returns 404
    - List documents returns project's documents
    - Get document content returns decrypted text
    - Document ownership validated (can't access other user's documents)
    - Upload oversized document returns 422
    - Upload non-text file returns 422

    **Threads (6 tests):**
    - Create thread in project returns 201
    - List threads returns project's threads
    - Get thread with messages returns conversation history
    - Thread ownership validated (can't access other user's threads)
    - Threads ordered by created_at DESC
    - Messages within thread ordered by created_at ASC

    **AI Chat (5 tests):**
    - Send message to thread with ANTHROPIC_API_KEY mocked returns SSE stream
    - Send message without API key returns 500
    - Send message to non-existent thread returns 404
    - Budget exceeded returns 429
    - Token usage recorded after message

    **Artifacts (4 tests):**
    - List artifacts for thread returns artifacts
    - Get artifact by ID returns markdown content
    - Export artifact as PDF returns binary with correct Content-Type
    - Export artifact as Word returns binary with correct Content-Type

    Use pytest fixtures from conftest.py (client, db_session). Add new fixtures if needed:
    - test_user fixture (creates user, returns user object)
    - test_project fixture (creates project for test_user)
    - test_thread fixture (creates thread in test_project)

    Mock Anthropic API calls using pytest-mock or unittest.mock to avoid real API usage during tests. Pattern:
    ```python
    @pytest.mark.asyncio
    async def test_chat_message(client, test_thread, monkeypatch):
        # Mock Anthropic Messages.stream call
        mock_stream = AsyncMock(return_value=[...])
        monkeypatch.setattr('app.services.ai_service.anthropic.messages.stream', mock_stream)

        response = await client.post(f'/api/threads/{test_thread.id}/chat', json={'content': 'test'})
        assert response.status_code == 200
    ```

    Total target: 44+ tests (14 auth + 30 new). Aim for 80%+ endpoint coverage.
  </action>
  <verify>
    1. cd backend && pytest tests/test_backend_integration.py -v (all tests pass)
    2. pytest --cov=app tests/test_backend_integration.py --cov-report=term (coverage report)
    3. Count tests: `pytest --collect-only tests/test_backend_integration.py | grep "test_" | wc -l`
  </verify>
  <done>
    - test_backend_integration.py consolidates all 14 auth tests from test_auth_integration.py
    - Added 8 project CRUD tests covering create, list, get, update, ownership
    - Added 7 document tests covering upload, list, get, validation, ownership
    - Added 6 thread tests covering create, list, get, ordering, ownership
    - Added 5 AI chat tests with mocked Anthropic API calls
    - Added 4 artifact tests covering list, get, PDF export, Word export
    - Test fixtures created for test_user, test_project, test_thread
    - All tests pass with pytest tests/test_backend_integration.py
    - 44+ tests total providing comprehensive API coverage
  </done>
</task>

</tasks>

<verification>
**Widget Tests:**
1. Run `flutter test test/widget/` - all tests pass
2. Check coverage: Tests render key UI elements (buttons, text, empty states)
3. Modify a screen (e.g., remove OAuth button) - widget test should fail

**Backend Tests:**
1. Run `pytest tests/test_backend_integration.py -v` - all tests pass
2. Total test count: `pytest --collect-only tests/test_backend_integration.py` shows 44+ tests
3. Break an endpoint (e.g., remove auth check) - integration test should fail
4. Run with coverage: `pytest --cov=app tests/test_backend_integration.py --cov-report=html`
5. Open htmlcov/index.html - should see >70% coverage for routes and services

**Test Pyramid Validation:**
- Widget tests are fast (<5 seconds total)
- Integration tests complete in <30 seconds
- No flaky tests (run 3 times, all pass each time)
</verification>

<success_criteria>
- 4 widget test files created covering login, projects, documents, conversation screens
- All widget tests pass and render critical UI elements (buttons, lists, empty states)
- Backend integration tests expanded to 44+ tests covering all major endpoints
- Test fixtures (test_user, test_project, test_thread) enable efficient test setup
- All tests pass in CI/CD environment (no environment-specific failures)
- Anthropic API calls mocked in tests (no real API usage during test runs)
</success_criteria>

<output>
After completion, create `.planning/phases/05-cross-platform-polish-launch/05-02-SUMMARY.md`
</output>
