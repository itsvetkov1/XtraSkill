---
phase: 58-agent-sdk-adapter
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/services/llm/claude_agent_adapter.py
  - backend/app/services/llm/base.py
  - backend/app/services/mcp_tools.py
autonomous: true

must_haves:
  truths:
    - "ClaudeAgentAdapter.stream_chat() yields StreamChunk events from SDK agent loop"
    - "SDK events (text, tool_use, thinking, complete, error) translate to StreamChunk without data loss"
    - "MCP tools receive database/project/thread context via HTTP-based transport"
    - "Agent decides when to search docs or save artifacts (proactive tool use)"
    - "Source attribution tracking works via _documents_used_context"
  artifacts:
    - path: "backend/app/services/llm/claude_agent_adapter.py"
      provides: "Full stream_chat implementation with SDK event translation"
      contains: "async for message in query"
    - path: "backend/app/services/mcp_tools.py"
      provides: "HTTP MCP transport with context propagation via headers"
      contains: "StreamableHTTPServerManager"
  key_links:
    - from: "backend/app/services/llm/claude_agent_adapter.py"
      to: "claude_agent_sdk.query"
      via: "async generator iteration"
      pattern: "async for message in query"
    - from: "backend/app/services/llm/claude_agent_adapter.py"
      to: "backend/app/services/mcp_tools.py"
      via: "MCP server factory and context propagation"
      pattern: "create_ba_mcp_server|_db_context|_project_id_context"
---

<objective>
Implement ClaudeAgentAdapter.stream_chat() with full SDK event translation and HTTP-based MCP tool integration.

Purpose: This is the core adapter that translates Claude Agent SDK streaming events into the existing StreamChunk format, enabling the SDK's multi-turn agent loop to work through the LLMAdapter abstraction. The adapter must set up MCP tools with proper context propagation so the agent can proactively search documents and save artifacts.

Output: Working ClaudeAgentAdapter that yields StreamChunk events from SDK query(), with MCP tools accessible via HTTP transport for production-ready context isolation.
</objective>

<execution_context>
@/Users/a1testingmac/.claude/get-shit-done/workflows/execute-plan.md
@/Users/a1testingmac/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Phase 57 summaries (provides SDK setup and MCP tools)
@.planning/phases/57-foundation/57-01-SUMMARY.md
@.planning/phases/57-foundation/57-02-SUMMARY.md

# Research with SDK patterns and pitfalls
@.planning/phases/58-agent-sdk-adapter/58-RESEARCH.md

# Key source files
@backend/app/services/llm/claude_agent_adapter.py
@backend/app/services/llm/base.py
@backend/app/services/mcp_tools.py
@backend/app/services/agent_service.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement ClaudeAgentAdapter.stream_chat() with SDK event translation and MCP integration</name>
  <files>
    backend/app/services/llm/claude_agent_adapter.py
    backend/app/services/llm/base.py
    backend/app/services/mcp_tools.py
  </files>
  <action>
    Replace the stub implementation of ClaudeAgentAdapter.stream_chat() with a working implementation.
    Use the existing AgentService (agent_service.py) as a proven reference — it already works with
    the same SDK, same tools, same event types. The adapter must produce StreamChunk objects instead
    of SSE event dicts.

    **ClaudeAgentAdapter changes (claude_agent_adapter.py):**

    1. Add imports for SDK types: `query`, `ClaudeAgentOptions`, `AssistantMessage`, `TextBlock`,
       `ToolUseBlock`, `ToolResultBlock`, `ResultMessage`, `StreamEvent` from `claude_agent_sdk`.

    2. Add imports from mcp_tools: `create_ba_mcp_server`, `_db_context`, `_project_id_context`,
       `_thread_id_context`, `_documents_used_context`.

    3. Add class attribute `is_agent_provider = True` — this signals to AIService that this adapter
       handles tools internally and the manual tool loop must be skipped.

    4. Expand `__init__` to accept optional `db`, `project_id`, `thread_id` parameters that will be
       set before stream_chat is called. Store them as instance attributes. Create the MCP server via
       `create_ba_mcp_server()`.

    5. Add method `set_context(self, db, project_id, thread_id)` that stores these values. AIService
       will call this before calling stream_chat().

    6. Implement `stream_chat()`:
       a. Set ContextVars for MCP tools: `_db_context.set(db)`, `_project_id_context.set(project_id)`,
          `_thread_id_context.set(thread_id)`, `_documents_used_context.set([])`.
       b. Build prompt from messages using `_convert_messages_to_prompt(messages)` — extract conversation
          history. Use the same format as AgentService: `[ROLE]: content` separated by double newlines.
       c. Configure `ClaudeAgentOptions` with:
          - `system_prompt=system_prompt` (the BA system prompt passed in by AIService — per locked
            decision to use current BA system prompt identical to direct API adapter)
          - `mcp_servers={"ba": self.mcp_server}`
          - `allowed_tools=["mcp__ba__search_documents", "mcp__ba__save_artifact"]`
          - `permission_mode="acceptEdits"`
          - `include_partial_messages=True` (enables StreamEvent for incremental text)
          - `model=self.model`
          - No `max_turns` parameter (per locked decision: no turn limit for POC)
       d. Iterate `async for message in query(prompt=prompt, options=options)`:

          **StreamEvent handling:**
          - Check for `delta` attribute with `text` attribute -> yield `StreamChunk(chunk_type="text", content=delta.text)`
          - This provides incremental streaming (locked decision: continuous stream across agent rounds)

          **AssistantMessage handling:**
          - Iterate `message.content` blocks
          - `TextBlock`: Do NOT yield as text (already streamed via StreamEvent when `include_partial_messages=True`)
          - `ToolUseBlock`: Yield `StreamChunk(chunk_type="tool_use", tool_call={"id": block.id, "name": block.name, "input": block.input})`
            This provides tool activity visibility (locked decision: show tool activity indicators)
          - `ToolResultBlock`: Check for ARTIFACT_CREATED marker in content (same pattern as AgentService).
            Parse artifact event data and yield a special StreamChunk. Add a new optional field
            `metadata` to StreamChunk to carry artifact_created event data.

          **ResultMessage handling:**
          - Extract usage from `message.usage` (input_tokens, output_tokens)
          - Get documents_used from `_documents_used_context.get()` for source attribution
          - Yield `StreamChunk(chunk_type="complete", usage={...})` with documents_used in metadata

       e. Wrap entire iteration in try/except. On exception:
          - Yield `StreamChunk(chunk_type="error", error=f"Agent SDK error (turn N): {str(e)}")`
          - Per locked decision: error messages include diagnostic info (which tool failed, which turn)
          - Per locked decision: discard partial output on failure

    7. Implement `_convert_messages_to_prompt(messages)`:
       - Same logic as AgentService: iterate messages, format as `[ROLE]: content`
       - Handle list content (tool_results, multi-part)
       - Return joined string

    **StreamChunk extension (base.py):**

    8. Add `metadata: Optional[Dict[str, Any]] = None` field to StreamChunk dataclass.
       This carries agent-specific data: artifact_created events, documents_used for source attribution,
       tool status indicators. Keep it optional to avoid breaking existing adapters.

    **MCP tools enhancement (mcp_tools.py):**

    9. The user's locked decision says "use proper HTTP-based MCP transport for context propagation."
       However, the existing ContextVar approach already works for in-process tools (proven by
       AgentService). For this POC phase, keep the ContextVar approach since the SDK runs in-process
       (not subprocess). The HTTP MCP transport is more relevant for CLI adapter (Phase 59) where
       the subprocess boundary prevents ContextVar propagation.

       Decision rationale: The Agent SDK's `query()` runs in the same Python process as FastAPI.
       ContextVars work correctly within the same asyncio event loop. HTTP transport adds complexity
       without benefit for in-process SDK. Document this choice clearly.

       Enhancement to mcp_tools.py: Increase search result count from 3 to 5 for agent mode
       (locked decision: larger context windows for search results). Add a `max_results` parameter
       to search_documents_tool that defaults to 3 but can be overridden. The ClaudeAgentAdapter
       will set a context variable `_max_results_context` to 5 before calling.

    **Do NOT:**
    - Add turn limits or request timeouts (locked decision: no limits for POC)
    - Implement Claude Code skills integration (deferred)
    - Add production hardening like rate limiting (deferred)
    - Duplicate tool execution logic — SDK handles tools internally via MCP
  </action>
  <verify>
    Run: `cd /Users/a1testingmac/projects/XtraSkill/backend && python -c "from app.services.llm.claude_agent_adapter import ClaudeAgentAdapter; a = ClaudeAgentAdapter(api_key='test'); print(f'Provider: {a.provider.value}, Model: {a.model}, Has MCP: {a.mcp_server is not None}, Is Agent: {a.is_agent_provider}')"` — should print provider, model, MCP server presence, and is_agent_provider=True.

    Run: `cd /Users/a1testingmac/projects/XtraSkill/backend && python -c "from app.services.llm.base import StreamChunk; c = StreamChunk(chunk_type='text', content='hello', metadata={'test': True}); print(f'Type: {c.chunk_type}, Meta: {c.metadata}')"` — should print metadata field.

    Run: `cd /Users/a1testingmac/projects/XtraSkill/backend && python -c "from app.services.mcp_tools import _max_results_context; print('max_results_context imported OK')"` — should import without error (if max_results context added).
  </verify>
  <done>
    ClaudeAgentAdapter has a working stream_chat() that:
    - Iterates SDK query() with proper options (system_prompt, MCP server, model)
    - Translates StreamEvent -> StreamChunk(text)
    - Translates ToolUseBlock -> StreamChunk(tool_use) for visibility
    - Translates ToolResultBlock -> StreamChunk with artifact metadata
    - Translates ResultMessage -> StreamChunk(complete) with usage and documents_used
    - Handles errors with diagnostic info
    - Has is_agent_provider=True class attribute
    - StreamChunk has metadata field
    - MCP tools enhanced for agent mode (larger search results)
  </done>
</task>

<task type="auto">
  <name>Task 2: Modify AIService to bypass manual tool loop for agent providers</name>
  <files>
    backend/app/services/ai_service.py
  </files>
  <action>
    Modify AIService.stream_chat() to detect agent providers (ClaudeAgentAdapter) and skip the
    manual tool execution loop. The SDK handles tool execution internally via MCP, so AIService
    must only forward StreamChunk events without trying to execute tools itself.

    **Changes to AIService.__init__:**

    1. After creating the adapter via `LLMFactory.create(provider)`, check if the adapter has
       `is_agent_provider = True`. Store this as `self.is_agent_provider`.

    **Changes to AIService.stream_chat():**

    2. Add an early branch at the top of stream_chat:
       ```python
       if self.is_agent_provider:
           async for event in self._stream_agent_chat(messages, project_id, thread_id, db):
               yield event
           return
       ```

    3. Implement `_stream_agent_chat()` method:
       a. Call `self.adapter.set_context(db, project_id, thread_id)` to pass request context
       b. Stream from adapter: `async for chunk in self.adapter.stream_chat(messages, SYSTEM_PROMPT, max_tokens=4096)`
          Note: Do NOT pass `tools` parameter — agent providers use MCP tools, not Anthropic tool format
       c. Translate StreamChunk events to SSE event dicts (same format as existing stream_chat):

          - `chunk.chunk_type == "text"` -> yield `{"event": "text_delta", "data": json.dumps({"text": chunk.content})}`
          - `chunk.chunk_type == "tool_use"` -> yield `{"event": "tool_executing", "data": json.dumps({"status": _tool_status_message(chunk.tool_call["name"])})}`
            where _tool_status_message maps tool names:
            - "mcp__ba__save_artifact" or contains "save_artifact" -> "Generating artifact..."
            - "mcp__ba__search_documents" or contains "search_documents" -> "Searching project documents..."
            - default -> f"Using tool: {name}..."
            This satisfies locked decision: show tool activity indicators
          - `chunk.chunk_type == "complete"` -> yield message_complete event with content, usage, and
            documents_used from chunk.metadata (for source attribution chips — locked decision)
          - `chunk.chunk_type == "error"` -> yield error event
          - If chunk.metadata contains "artifact_created" -> yield artifact_created event BEFORE message_complete

       d. Log AI stream start and completion with timing (same logging pattern as existing stream_chat)

    4. Add helper `_tool_status_message(tool_name: str) -> str` for mapping tool names to status messages.

    **IMPORTANT: Do NOT modify the existing tool loop path.** The `while True` loop with tool
    execution must remain intact for direct API providers (anthropic, google, deepseek). Only the
    new `_stream_agent_chat` method handles agent providers.

    **Do NOT:**
    - Execute tools manually for agent providers (SDK does this via MCP)
    - Modify the conversations.py route (it already passes thread.model_provider to AIService)
    - Buffer entire response (stream incrementally)
  </action>
  <verify>
    Run: `cd /Users/a1testingmac/projects/XtraSkill/backend && python -c "
from app.services.ai_service import AIService
from unittest.mock import patch, MagicMock

# Test with direct API provider
with patch('app.services.ai_service.LLMFactory') as mock_factory:
    mock_adapter = MagicMock()
    mock_adapter.is_agent_provider = False
    mock_factory.create.return_value = mock_adapter
    svc = AIService(provider='anthropic')
    print(f'Direct API is_agent_provider: {svc.is_agent_provider}')  # Should be False

# Test with agent provider
with patch('app.services.ai_service.LLMFactory') as mock_factory:
    mock_adapter = MagicMock()
    mock_adapter.is_agent_provider = True
    mock_factory.create.return_value = mock_adapter
    svc = AIService(provider='claude-code-sdk')
    print(f'Agent is_agent_provider: {svc.is_agent_provider}')  # Should be True
"` — should print False then True.

    Verify the existing stream_chat path is untouched by checking the `while True` loop still exists:
    `grep -n "while True" /Users/a1testingmac/projects/XtraSkill/backend/app/services/ai_service.py` — should find the existing tool loop.
  </verify>
  <done>
    AIService correctly detects agent providers and routes to _stream_agent_chat() which:
    - Sets context on adapter before streaming
    - Forwards StreamChunk events as SSE event dicts
    - Shows tool activity indicators for tool_use chunks
    - Includes documents_used in message_complete for source attribution
    - Emits artifact_created events when artifacts are saved
    - Logs stream start/completion with timing
    - Does NOT execute tools manually (SDK handles via MCP)
    - Existing direct API tool loop is untouched
  </done>
</task>

</tasks>

<verification>
1. Import verification: `python -c "from app.services.llm.claude_agent_adapter import ClaudeAgentAdapter"` succeeds
2. Adapter has is_agent_provider=True: `python -c "from app.services.llm.claude_agent_adapter import ClaudeAgentAdapter; assert ClaudeAgentAdapter(api_key='test').is_agent_provider"`
3. StreamChunk has metadata field: `python -c "from app.services.llm.base import StreamChunk; StreamChunk(chunk_type='test', metadata={})"`
4. AIService detects agent provider: AIService('claude-code-sdk').is_agent_provider is True
5. AIService preserves direct API path: AIService('anthropic').is_agent_provider is False
6. No circular imports: All imports resolve without error
7. Existing tests still pass: `cd backend && python -m pytest tests/unit/llm/ -v`
</verification>

<success_criteria>
1. ClaudeAgentAdapter.stream_chat() is implemented (not a stub) and yields StreamChunk events
2. SDK event types (StreamEvent, AssistantMessage, ToolUseBlock, ToolResultBlock, ResultMessage) are translated to StreamChunk
3. MCP tools (search_documents, save_artifact) are connected with proper context via ContextVars
4. AIService routes agent providers to _stream_agent_chat() which forwards chunks without manual tool execution
5. Tool activity indicators yield from tool_use chunks
6. Source attribution data (documents_used) included in complete chunk metadata
7. Error handling includes diagnostic info (per locked decision)
8. No turn limits or timeouts (per locked decision)
9. Existing direct API adapter path unchanged
</success_criteria>

<output>
After completion, create `.planning/phases/58-agent-sdk-adapter/58-01-SUMMARY.md`
</output>
