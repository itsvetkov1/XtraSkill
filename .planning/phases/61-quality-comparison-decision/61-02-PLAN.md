---
phase: 61-quality-comparison-decision
plan: 02
type: execute
wave: 2
depends_on: ["61-01"]
files_modified:
  - backend/evaluation/analyze_results.py
  - backend/evaluation/generate_report.py
  - backend/evaluation/report_template.md
autonomous: true

must_haves:
  truths:
    - "Analysis script computes per-dimension means, improvement percentages, and Mann-Whitney U significance tests for each provider vs baseline"
    - "Report generator produces a markdown comparison report with statistical results, cost-quality tradeoff, and clear go/no-go recommendation"
    - "Decision logic applies the >20% improvement threshold from STATE.md decision criteria"
  artifacts:
    - path: "backend/evaluation/analyze_results.py"
      provides: "Statistical analysis: means, std dev, p-values, improvement percentages per provider per dimension"
      contains: "mannwhitneyu"
    - path: "backend/evaluation/generate_report.py"
      provides: "Report generation with Jinja2 template rendering and decision matrix"
      contains: "make_recommendation"
    - path: "backend/evaluation/report_template.md"
      provides: "Jinja2 markdown template for comparison report"
      contains: "{{ stats"
  key_links:
    - from: "backend/evaluation/analyze_results.py"
      to: "backend/evaluation_data/blind_review/scoring_template.csv"
      via: "Reads completed scores CSV"
      pattern: "read_csv"
    - from: "backend/evaluation/generate_report.py"
      to: "backend/evaluation/analyze_results.py"
      via: "Imports analysis functions"
      pattern: "from.*analyze_results import"
    - from: "backend/evaluation/generate_report.py"
      to: "backend/evaluation/report_template.md"
      via: "Loads Jinja2 template"
      pattern: "Template.*report_template"
---

<objective>
Create the analysis pipeline and report generator for the quality comparison.

Purpose: Build the scripts that take human-scored quality ratings and produce statistical analysis with a clear go/no-go recommendation. These scripts are independent of BRD generation and can be written before scoring happens.

Output: Statistical analysis script, report generator, and Jinja2 report template.
</objective>

<execution_context>
@C:/Users/ibcve/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ibcve/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/61-quality-comparison-decision/61-RESEARCH.md
@.planning/phases/61-quality-comparison-decision/61-01-SUMMARY.md
@backend/evaluation/quality_rubric.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create statistical analysis script</name>
  <files>
    backend/evaluation/analyze_results.py
  </files>
  <action>
    Create the analysis script that processes human quality scores and produces statistical comparisons.

    **backend/evaluation/analyze_results.py** — Module with these functions:

    1. **`load_scores(scores_path, mapping_path)` function**:
       - Reads completed scoring CSV from `scores_path` (default: `evaluation_data/blind_review/scoring_template.csv`)
       - Reads review_id -> {provider, scenario_id} mapping from `mapping_path` (default: `evaluation_data/metadata/review_id_mapping.json`)
       - Merges provider info into scores DataFrame
       - Validates: all review_ids have scores (no empty cells), all review_ids have mapping entries
       - Returns pandas DataFrame with columns: review_id, provider, scenario_id, completeness, ac_quality, consistency, error_coverage, notes

    2. **`analyze_provider_comparison(scores_df)` function**:
       - Groups scores by provider
       - For each of the 4 quality dimensions (completeness, ac_quality, consistency, error_coverage):
         - Computes mean and std dev per provider
         - Computes improvement percentage: ((provider_mean - baseline_mean) / baseline_mean) * 100
         - Runs Mann-Whitney U test (from scipy.stats) comparing each agent provider vs baseline (anthropic)
           - Use `alternative='two-sided'` (not one-sided — we want to detect degradation too)
         - Records p-value and statistical significance at alpha=0.05
       - Computes aggregate scores: mean across all 4 dimensions per provider
       - Returns dict structured per research example: `{dimension: {baseline_mean, sdk_mean, cli_mean, sdk_improvement_pct, cli_improvement_pct, sdk_vs_baseline_pvalue, cli_vs_baseline_pvalue, sdk_significant, cli_significant}}`

    3. **`calculate_cost_summary()` function**:
       - Reads metadata files from `evaluation_data/metadata/{provider}_{scenario_id}.json`
       - Aggregates per provider: total_input_tokens, total_output_tokens, total_cost_usd, avg_cost_per_brd, avg_generation_time_s
       - Computes cost increase percentages: ((agent_cost - baseline_cost) / baseline_cost) * 100
       - Returns dict: `{provider: {total_cost, avg_cost_per_brd, avg_time_s, cost_increase_pct}}`

    4. **`make_recommendation(stats, cost_summary)` function**:
       - Applies decision criteria from STATE.md:
         - Need >20% average quality improvement to justify adoption
         - Cost overhead threshold: 30-50% increase considered acceptable IF quality justifies it
       - Computes average improvement across all 4 dimensions for each agent provider
       - Counts dimensions with statistically significant improvement
       - Decision matrix (per research):
         - If avg_improvement >= 20% AND significant_dims >= 3: "ADOPT [provider]" with HIGH/MEDIUM confidence
         - If avg_improvement 10-20% (marginal): "LARGER STUDY RECOMMENDED" with LOW confidence
         - If avg_improvement < 10%: "STAY WITH DIRECT API" with MEDIUM confidence
         - If BOTH SDK and CLI qualify, recommend the one with higher improvement AND lower cost
       - Returns dict: `{recommendation, confidence, rationale, sdk_summary, cli_summary}`

    5. **`run_full_analysis()` function**:
       - Orchestrates: load_scores -> analyze_provider_comparison -> calculate_cost_summary -> make_recommendation
       - Saves results to `evaluation_results/statistics.json`
       - Creates `evaluation_results/` directory if needed
       - Returns all results as dict

    6. **`if __name__ == "__main__":`** — Runs full analysis and prints summary table to stdout.

    **Dependencies**: pandas, scipy (both in standard scientific Python stack). Add `pandas>=2.0.0` and `scipy>=1.10.0` to requirements.txt if not present.

    **Important**: Use `try/except ImportError` for pandas/scipy with clear error message telling user to install them. These are evaluation-only dependencies, not production.
  </action>
  <verify>
    - `cd backend && python -c "from evaluation.analyze_results import analyze_provider_comparison, make_recommendation, calculate_cost_summary; print('Import OK')"` succeeds
    - make_recommendation function applies >20% threshold from STATE.md
    - analyze_provider_comparison uses scipy.stats.mannwhitneyu for statistical testing
    - load_scores validates no empty score cells
  </verify>
  <done>
    Analysis script importable, computes per-dimension statistical comparisons with Mann-Whitney U tests, applies >20% improvement decision threshold, and produces structured results dict.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create report generator and template</name>
  <files>
    backend/evaluation/generate_report.py
    backend/evaluation/report_template.md
  </files>
  <action>
    Create the report generator and Jinja2 template for the final comparison report.

    1. **backend/evaluation/report_template.md** — Jinja2 markdown template with these sections:

       ```
       # Quality Comparison Report: Claude Code as AI Backend

       **Generated:** {{ generated_date }}
       **Samples per provider:** {{ samples_per_provider }}
       **Providers compared:** {{ providers | join(', ') }}

       ## Executive Summary

       {{ recommendation.rationale }}

       **Recommendation:** {{ recommendation.recommendation }}
       **Confidence:** {{ recommendation.confidence }}

       ## Methodology

       - {{ samples_per_provider }} standardized test scenarios per provider ({{ total_samples }} total BRDs)
       - 4-dimension quality rubric (1-4 scale): Completeness, AC Quality, Consistency, Error Coverage
       - Blind review protocol (reviewer did not know which provider generated each BRD)
       - Mann-Whitney U test for statistical significance (alpha = 0.05)
       - Decision threshold: >20% average quality improvement required to justify adoption

       ## Quality Scores by Dimension

       | Dimension | Baseline (anthropic) | SDK (claude-code-sdk) | CLI (claude-code-cli) | SDK Improvement | CLI Improvement |
       |-----------|---------------------|-----------------------|-----------------------|-----------------|-----------------|
       {% for dim in dimensions %}
       | {{ dim.name }} | {{ "%.2f"|format(dim.baseline_mean) }} | {{ "%.2f"|format(dim.sdk_mean) }} | {{ "%.2f"|format(dim.cli_mean) }} | {{ "%+.1f%%"|format(dim.sdk_improvement_pct) }}{% if dim.sdk_significant %} * {% endif %} | {{ "%+.1f%%"|format(dim.cli_improvement_pct) }}{% if dim.cli_significant %} * {% endif %} |
       {% endfor %}
       | **Average** | {{ "%.2f"|format(baseline_avg) }} | {{ "%.2f"|format(sdk_avg) }} | {{ "%.2f"|format(cli_avg) }} | {{ "%+.1f%%"|format(sdk_avg_improvement) }} | {{ "%+.1f%%"|format(cli_avg_improvement) }} |

       \* Statistically significant at p < 0.05

       ## Cost Analysis

       | Metric | Baseline | SDK | CLI |
       |--------|----------|-----|-----|
       | Avg cost per BRD | ${{ "%.4f"|format(cost.anthropic.avg_cost_per_brd) }} | ${{ "%.4f"|format(cost.sdk.avg_cost_per_brd) }} | ${{ "%.4f"|format(cost.cli.avg_cost_per_brd) }} |
       | Cost increase | — | {{ "%+.1f%%"|format(cost.sdk.cost_increase_pct) }} | {{ "%+.1f%%"|format(cost.cli.cost_increase_pct) }} |
       | Avg generation time | {{ "%.1f"|format(cost.anthropic.avg_time_s) }}s | {{ "%.1f"|format(cost.sdk.avg_time_s) }}s | {{ "%.1f"|format(cost.cli.avg_time_s) }}s |

       ## Cost-Quality Tradeoff

       | Provider | Quality Improvement | Cost Increase | Improvement per Cost % |
       |----------|--------------------|--------------|-----------------------|
       | claude-code-sdk | {{ "%+.1f%%"|format(sdk_avg_improvement) }} | {{ "%+.1f%%"|format(cost.sdk.cost_increase_pct) }} | {{ "%.2f"|format(sdk_quality_per_cost) }} |
       | claude-code-cli | {{ "%+.1f%%"|format(cli_avg_improvement) }} | {{ "%+.1f%%"|format(cost.cli.cost_increase_pct) }} | {{ "%.2f"|format(cli_quality_per_cost) }} |

       ## Decision Matrix

       | Factor | claude-code-sdk | claude-code-cli | Threshold |
       |--------|----------------|-----------------|-----------|
       | Avg quality improvement | {{ "%+.1f%%"|format(sdk_avg_improvement) }} | {{ "%+.1f%%"|format(cli_avg_improvement) }} | >20% |
       | Significant dimensions | {{ sdk_significant_dims }}/4 | {{ cli_significant_dims }}/4 | >=3/4 |
       | Cost increase | {{ "%+.1f%%"|format(cost.sdk.cost_increase_pct) }} | {{ "%+.1f%%"|format(cost.cli.cost_increase_pct) }} | <50% |
       | Implementation complexity | Medium (in-process MCP) | High (subprocess management) | — |

       ## Limitations

       - Small sample size (n={{ samples_per_provider }} per condition) limits statistical power
       - Results are directional insights, not definitive proof
       - Single reviewer scoring (no inter-rater reliability metric)
       - Test scenarios may not cover all production use cases
       - Provider prompts may differ slightly (combined prompt approach for CLI vs native system prompt for SDK)

       ## Recommendation

       **{{ recommendation.recommendation }}**

       {{ recommendation.rationale }}

       {% if recommendation.recommendation == "LARGER STUDY RECOMMENDED" %}
       ### Suggested Next Steps
       - Increase sample size to n=30 per provider for 80% statistical power
       - Add second independent reviewer for inter-rater reliability
       - Include production conversation logs as test scenarios
       {% elif "ADOPT" in recommendation.recommendation %}
       ### Suggested Next Steps
       - Merge feature/claude-code-backend to master
       - Production hardening: HTTP MCP transport, system prompt separation
       - Monitor quality metrics in production for 2 weeks
       {% else %}
       ### Suggested Next Steps
       - Consider enhancing direct API with multi-pass refinement
       - Archive experiment branch for future reference
       - Focus on prompt engineering improvements for direct API
       {% endif %}
       ```

    2. **backend/evaluation/generate_report.py** — Report generator module:

       - **`generate_comparison_report(output_path=None)` function**:
         - Imports and calls `run_full_analysis()` from `analyze_results`
         - Loads `report_template.md` Jinja2 template
         - Prepares template context:
           - `generated_date`: today's ISO date
           - `samples_per_provider`: 5
           - `providers`: ["anthropic", "claude-code-sdk", "claude-code-cli"]
           - `total_samples`: 15
           - `recommendation`: from analysis
           - `dimensions`: list of per-dimension stats dicts
           - `baseline_avg`, `sdk_avg`, `cli_avg`: aggregate means
           - `sdk_avg_improvement`, `cli_avg_improvement`: aggregate improvement percentages
           - `sdk_quality_per_cost`, `cli_quality_per_cost`: quality improvement % / cost increase %
           - `sdk_significant_dims`, `cli_significant_dims`: count of statistically significant dimensions
           - `cost`: cost summary dict with provider keys
         - Renders template
         - Writes to `output_path` (default: `evaluation_results/comparison_report.md`)
         - Also saves `evaluation_results/statistics.json` with raw analysis data
         - Returns rendered report string

       - **`if __name__ == "__main__":`** — Runs report generation, prints path to generated report

    **Dependency**: jinja2 (already in requirements.txt). pandas, scipy from Task 1.
  </action>
  <verify>
    - `cd backend && python -c "from evaluation.generate_report import generate_comparison_report; print('Import OK')"` succeeds
    - Report template contains all required sections (Executive Summary, Methodology, Quality Scores, Cost Analysis, Decision Matrix, Limitations, Recommendation)
    - Template uses Jinja2 syntax correctly ({{ }} and {% %} blocks)
    - generate_comparison_report imports from analyze_results correctly
  </verify>
  <done>
    Report generator and Jinja2 template produce a complete comparison report with quality scores table, cost analysis, cost-quality tradeoff, decision matrix, limitations, and actionable recommendation with next steps.
  </done>
</task>

</tasks>

<verification>
1. All 3 evaluation scripts (analyze_results.py, generate_report.py) are importable
2. Report template contains all required comparison sections
3. Analysis applies Mann-Whitney U test and >20% improvement threshold
4. Report template renders decision matrix with quality vs cost tradeoff
5. Scripts follow existing codebase conventions (docstrings, type hints)
</verification>

<success_criteria>
- Statistical analysis script computes per-dimension comparisons with significance testing
- Report generator produces comprehensive markdown report from Jinja2 template
- Decision logic implements the >20% improvement threshold from project decisions
- Cost-quality tradeoff analysis included in report
- All scripts importable without runtime errors
</success_criteria>

<output>
After completion, create `.planning/phases/61-quality-comparison-decision/61-02-SUMMARY.md`
</output>
