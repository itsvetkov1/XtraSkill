---
phase: 69-token-optimization
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/services/ai_service.py
  - backend/tests/unit/services/test_conversation_service.py
  - backend/tests/unit/llm/test_claude_cli_adapter.py
autonomous: true
requirements: [TOKEN-01, TOKEN-02, TOKEN-03, TOKEN-04]

must_haves:
  truths:
    - "Conversations exceeding 180K estimated tokens yield an SSE error event instead of sending to CLI"
    - "The error message includes the estimated token count and tells user to start a new conversation"
    - "MAX_CONTEXT_TOKENS remains 150000 (regression-protected by test)"
    - "Token growth across 20+ turns with document search annotations is linear, not quadratic"
    - "Tool_use annotation ([searched documents]) is far smaller than raw document content in token count"
  artifacts:
    - path: "backend/app/services/ai_service.py"
      provides: "EMERGENCY_TOKEN_LIMIT constant and 180K check in _stream_agent_chat()"
      contains: "EMERGENCY_TOKEN_LIMIT"
    - path: "backend/tests/unit/services/test_conversation_service.py"
      provides: "TOKEN-02 regression test + TOKEN-04 linear growth tests"
      contains: "TestLinearTokenGrowth"
    - path: "backend/tests/unit/llm/test_claude_cli_adapter.py"
      provides: "TOKEN-01 annotation size verification test"
      contains: "test_tool_use_annotation_far_smaller_than_document_content"
  key_links:
    - from: "backend/app/services/ai_service.py"
      to: "app.services.conversation_service.estimate_messages_tokens"
      via: "import and call before adapter.stream_chat()"
      pattern: "estimate_messages_tokens\\(messages\\)"
---

<objective>
Add 180K emergency token limit in _stream_agent_chat() and comprehensive token optimization tests covering all four TOKEN requirements.

Purpose: Prevent silent CLI failures on long conversations and verify token growth characteristics are linear.
Output: 1 production code change (180K emergency check) + 6-8 new tests across 3 test files.
</objective>

<execution_context>
@/Users/a1testingmac/.claude/get-shit-done/workflows/execute-plan.md
@/Users/a1testingmac/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/69-token-optimization/69-RESEARCH.md
@.planning/phases/68-core-conversation-memory-fix/68-01-SUMMARY.md
@backend/app/services/ai_service.py
@backend/app/services/conversation_service.py
@backend/tests/unit/services/test_conversation_service.py
@backend/tests/unit/llm/test_claude_cli_adapter.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add 180K emergency token limit in _stream_agent_chat()</name>
  <files>backend/app/services/ai_service.py</files>
  <action>
Add the 180K emergency token check to `_stream_agent_chat()` in `ai_service.py`. This is a CLI-adapter-specific safety net above the existing 150K truncation limit.

**Step 1: Add module-level constant** near the top of ai_service.py (after existing imports/constants):
```python
# Emergency token ceiling for agent providers (above the 150K soft limit in conversation_service)
EMERGENCY_TOKEN_LIMIT = 180000
```

**Step 2: Add import** — add `estimate_messages_tokens` to the imports from conversation_service:
```python
from app.services.conversation_service import estimate_messages_tokens
```

**Step 3: Insert the emergency check** in `_stream_agent_chat()`, AFTER `self.adapter.set_context(db, project_id, thread_id)` and BEFORE `accumulated_text = ""`. The exact insertion point is between line ~902 (set_context call) and line ~904 (accumulated_text = ""):
```python
            # TOKEN-03: Emergency token limit for agent providers
            # The 150K soft limit in build_conversation_context() should have already truncated,
            # but this catches edge cases (single huge message, estimation arithmetic drift).
            # Formatting overhead (~75 tokens for 20-turn conversation) is negligible at this scale.
            estimated_tokens = estimate_messages_tokens(messages)
            if estimated_tokens > EMERGENCY_TOKEN_LIMIT:
                yield {
                    "event": "error",
                    "data": json.dumps({
                        "message": (
                            f"This conversation has grown too long to continue "
                            f"({estimated_tokens:,} estimated tokens). "
                            "Please start a new conversation."
                        )
                    })
                }
                return
```

**What NOT to do:**
- Do NOT add this check in `conversations.py` route handler (it's CLI-adapter-specific)
- Do NOT change MAX_CONTEXT_TOKENS from 150000 (TOKEN-02 requirement)
- Do NOT raise a Python exception (SSE streaming context — yield the error event instead)
- Do NOT import tiktoken or any new dependency
  </action>
  <verify>
1. `cd /Users/a1testingmac/projects/XtraSkill/backend && grep -n "EMERGENCY_TOKEN_LIMIT" app/services/ai_service.py` — should show constant definition and usage
2. `cd /Users/a1testingmac/projects/XtraSkill/backend && ./venv/bin/python -c "from app.services.ai_service import EMERGENCY_TOKEN_LIMIT; print(EMERGENCY_TOKEN_LIMIT)"` — should print 180000
3. No syntax errors: `cd /Users/a1testingmac/projects/XtraSkill/backend && ./venv/bin/python -c "import app.services.ai_service"` — should exit cleanly
  </verify>
  <done>
EMERGENCY_TOKEN_LIMIT = 180000 defined as module constant. Emergency check in _stream_agent_chat() yields SSE error event with token count and actionable message when estimated tokens > 180K. Check is placed after set_context() and before adapter.stream_chat(). No new dependencies introduced.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add token optimization tests for all TOKEN requirements</name>
  <files>
    backend/tests/unit/services/test_conversation_service.py
    backend/tests/unit/llm/test_claude_cli_adapter.py
  </files>
  <action>
Add tests covering TOKEN-01, TOKEN-02, TOKEN-03, and TOKEN-04.

**Part A: TOKEN-02 regression test + TOKEN-04 linear growth tests** in `backend/tests/unit/services/test_conversation_service.py`

Append the following test classes at the END of the file:

```python
class TestMaxContextTokensRegression:
    """TOKEN-02: Regression test ensuring 150K truncation limit is preserved."""

    def test_max_context_tokens_is_150000(self):
        """MAX_CONTEXT_TOKENS must remain 150000 — changing it breaks TOKEN-02."""
        assert MAX_CONTEXT_TOKENS == 150000, (
            f"MAX_CONTEXT_TOKENS changed to {MAX_CONTEXT_TOKENS}. "
            "This breaks TOKEN-02 requirement. If intentional, update this test."
        )

    def test_chars_per_token_is_4(self):
        """CHARS_PER_TOKEN must remain 4 — consistent estimation across codebase."""
        assert CHARS_PER_TOKEN == 4


class TestLinearTokenGrowth:
    """TOKEN-04: Verify token growth is linear across 20+ turn conversations with doc searches."""

    def _make_conversation(self, turns: int):
        """Build a realistic multi-turn conversation with document search annotations."""
        messages = []
        for i in range(turns):
            messages.append({
                "role": "user",
                "content": f"Tell me about feature {i} in the project documents."
            })
            # Realistic assistant response: some text + doc search annotation
            messages.append({
                "role": "assistant",
                "content": (
                    f"Based on the project documentation, feature {i} works as follows. "
                    f"[searched documents]\n"
                    f"This feature handles the {i}-th use case with standard behavior."
                )
            })
        return messages

    def test_token_growth_is_linear_over_20_turns(self):
        """Token counts grow linearly (not quadratically) across 20+ turns."""
        token_counts = []
        for num_turns in range(1, 22):  # 1 to 21 turns
            messages = self._make_conversation(num_turns)
            tokens = estimate_messages_tokens(messages)
            token_counts.append(tokens)

        # Linear growth: each additional turn adds approximately the same token count
        # Compute first differences (turn N+1 - turn N)
        diffs = [token_counts[i+1] - token_counts[i] for i in range(len(token_counts)-1)]

        # All first differences should be approximately equal (linear = constant first diff)
        # Allow 20% variance to handle integer truncation effects
        avg_diff = sum(diffs) / len(diffs)
        for d in diffs:
            assert abs(d - avg_diff) / avg_diff < 0.20, (
                f"Non-linear growth detected: diff={d} deviates >20% from avg={avg_diff:.1f}. "
                f"Token counts: {token_counts}"
            )

    def test_token_growth_not_quadratic(self):
        """Confirm token count at turn N is NOT proportional to N^2."""
        messages_5 = self._make_conversation(5)
        messages_10 = self._make_conversation(10)
        messages_20 = self._make_conversation(20)

        tokens_5 = estimate_messages_tokens(messages_5)
        tokens_10 = estimate_messages_tokens(messages_10)
        tokens_20 = estimate_messages_tokens(messages_20)

        # For linear growth: tokens_20 / tokens_5 should be ~4 (20/5 = 4x)
        # For quadratic growth: tokens_20 / tokens_5 would be ~16 (20^2/5^2 = 16x)
        ratio_10_vs_5 = tokens_10 / tokens_5  # Should be ~2 (linear)
        ratio_20_vs_5 = tokens_20 / tokens_5  # Should be ~4 (linear)

        assert ratio_10_vs_5 < 3.0, f"Turn 10/5 ratio {ratio_10_vs_5:.2f} suggests non-linear growth"
        assert ratio_20_vs_5 < 6.0, f"Turn 20/5 ratio {ratio_20_vs_5:.2f} suggests non-linear growth"

    def test_21_turns_token_count_reasonable(self):
        """A 21-turn conversation with doc annotations stays well under emergency limit."""
        messages = self._make_conversation(21)
        tokens = estimate_messages_tokens(messages)
        # 21 turns × ~2 messages × ~30 words ≈ ~1260 words ≈ ~1680 tokens
        # Should be well under 180K emergency limit
        assert tokens < 10000, f"21-turn conversation uses {tokens} tokens — unexpectedly high"
        assert tokens > 0, "Token count should be positive"
```

**Part B: TOKEN-01 annotation size test** in `backend/tests/unit/llm/test_claude_cli_adapter.py`

Add this test method inside the existing `TestClaudeCLIAdapterMessageConversion` class (which already has `mock_which` fixture):

```python
    def test_tool_use_annotation_far_smaller_than_document_content(self, mock_which):
        """TOKEN-01: [searched documents] annotation uses far fewer tokens than full document content."""
        adapter = ClaudeCLIAdapter(api_key="test-key")

        # Simulate assistant message with document search tool_use + text
        large_doc_content = "a" * 2000  # ~500 tokens of document content
        messages_with_doc_in_tool_use = [
            {"role": "user", "content": "What does the spec say?"},
            {
                "role": "assistant",
                "content": [
                    {"type": "text", "text": "Based on the spec:"},
                    {"type": "tool_use", "id": "t1", "name": "search_documents",
                     "input": {"query": "spec", "result": large_doc_content}}
                ]
            }
        ]

        prompt = adapter._convert_messages_to_prompt(messages_with_doc_in_tool_use)

        # The tool_use block's large content is NOT in the output
        assert large_doc_content not in prompt
        # The annotation is tiny (2 words vs 500 tokens)
        assert "[searched documents]" in prompt
        # Overall prompt is much shorter than if tool_use content were included
        assert len(prompt) < len(large_doc_content)
```

**Part C: TOKEN-03 emergency limit test** — also add in `test_claude_cli_adapter.py` inside `TestClaudeCLIAdapterMessageConversion`:

```python
    def test_emergency_token_limit_constant_exists(self, mock_which):
        """TOKEN-03: EMERGENCY_TOKEN_LIMIT constant is importable and set to 180000."""
        from app.services.ai_service import EMERGENCY_TOKEN_LIMIT
        assert EMERGENCY_TOKEN_LIMIT == 180000
```

**What NOT to do:**
- Do NOT use tiktoken or external tokenizer — use existing estimate_messages_tokens
- Do NOT create a new test file — add to existing test files
- Do NOT fix the pre-existing test_stream_chat_passes_api_key_in_env failure
- Do NOT create message builders growing per turn (messages must be fixed-size per turn for honest linear growth testing)
  </action>
  <verify>
1. Run conversation_service tests:
   `cd /Users/a1testingmac/projects/XtraSkill/backend && ./venv/bin/python -m pytest tests/unit/services/test_conversation_service.py -v --tb=short`
   Expected: All existing tests pass + 5 new tests pass (2 regression + 3 linear growth)

2. Run CLI adapter tests:
   `cd /Users/a1testingmac/projects/XtraSkill/backend && ./venv/bin/python -m pytest tests/unit/llm/test_claude_cli_adapter.py -v --tb=short`
   Expected: 47 pass + 2 new pass = 49 pass, 1 pre-existing fail (test_stream_chat_passes_api_key_in_env)

3. Run all unit tests to confirm no regressions:
   `cd /Users/a1testingmac/projects/XtraSkill/backend && ./venv/bin/python -m pytest tests/unit/ -q --tb=short`
   Expected: All pass except 1 pre-existing failure
  </verify>
  <done>
7 new tests added across 2 test files:
- test_conversation_service.py: TestMaxContextTokensRegression (2 tests for TOKEN-02) + TestLinearTokenGrowth (3 tests for TOKEN-04)
- test_claude_cli_adapter.py: test_tool_use_annotation_far_smaller_than_document_content (TOKEN-01) + test_emergency_token_limit_constant_exists (TOKEN-03)
All new tests pass. Pre-existing failure count unchanged (1).
  </done>
</task>

</tasks>

<verification>
1. **TOKEN-01 verified:** test_tool_use_annotation_far_smaller_than_document_content proves annotations are smaller than raw tool_use content
2. **TOKEN-02 verified:** test_max_context_tokens_is_150000 and test_chars_per_token_is_4 guard against accidental constant changes
3. **TOKEN-03 verified:** EMERGENCY_TOKEN_LIMIT = 180000 constant exists; emergency check in _stream_agent_chat() yields SSE error event; test_emergency_token_limit_constant_exists confirms importability
4. **TOKEN-04 verified:** TestLinearTokenGrowth with 3 tests confirms linear growth across 21 turns and refutes quadratic growth hypothesis

**Full test suite verification:**
```bash
cd /Users/a1testingmac/projects/XtraSkill/backend && ./venv/bin/python -m pytest tests/unit/ -q --tb=short
```
Expected: All tests pass (except 1 pre-existing failure in test_stream_chat_passes_api_key_in_env).
</verification>

<success_criteria>
- EMERGENCY_TOKEN_LIMIT = 180000 defined in ai_service.py
- Emergency check in _stream_agent_chat() yields error event for >180K tokens
- MAX_CONTEXT_TOKENS = 150000 unchanged and regression-protected
- 20+ turn linear growth assertion passes
- All new tests pass; no regressions introduced
- Pre-existing failure count: exactly 1 (unchanged)
</success_criteria>

<output>
After completion, create `.planning/phases/69-token-optimization/69-01-SUMMARY.md`
</output>
