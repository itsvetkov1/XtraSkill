---
phase: 61-quality-comparison-decision
plan: 03
type: execute
wave: 3
depends_on: ["61-01", "61-02"]
files_modified:
  - backend/evaluation_data/anthropic/TC-01.md
  - backend/evaluation_data/anthropic/TC-02.md
  - backend/evaluation_data/anthropic/TC-03.md
  - backend/evaluation_data/anthropic/TC-04.md
  - backend/evaluation_data/anthropic/TC-05.md
  - backend/evaluation_data/claude-code-sdk/TC-01.md
  - backend/evaluation_data/claude-code-sdk/TC-02.md
  - backend/evaluation_data/claude-code-sdk/TC-03.md
  - backend/evaluation_data/claude-code-sdk/TC-04.md
  - backend/evaluation_data/claude-code-sdk/TC-05.md
  - backend/evaluation_data/claude-code-cli/TC-01.md
  - backend/evaluation_data/claude-code-cli/TC-02.md
  - backend/evaluation_data/claude-code-cli/TC-03.md
  - backend/evaluation_data/claude-code-cli/TC-04.md
  - backend/evaluation_data/claude-code-cli/TC-05.md
  - backend/evaluation_data/metadata/
  - backend/evaluation_data/blind_review/scoring_template.csv
  - backend/evaluation_data/blind_review/
autonomous: false

must_haves:
  truths:
    - "15 BRDs generated (5 per provider) and saved to evaluation_data/{provider}/ directories"
    - "Metadata captured for all 15 samples (token usage, generation time, cost estimate)"
    - "Blind review materials created: anonymized BRDs and scoring CSV template"
    - "Scoring template CSV contains 15 rows with empty score columns ready for human review"
  artifacts:
    - path: "backend/evaluation_data/anthropic/"
      provides: "5 baseline BRDs from direct API provider"
    - path: "backend/evaluation_data/claude-code-sdk/"
      provides: "5 BRDs from Agent SDK provider"
    - path: "backend/evaluation_data/claude-code-cli/"
      provides: "5 BRDs from CLI subprocess provider"
    - path: "backend/evaluation_data/metadata/"
      provides: "Token usage, timing, and cost metadata for all 15 samples"
    - path: "backend/evaluation_data/blind_review/scoring_template.csv"
      provides: "Empty scoring template for blind human review"
    - path: "backend/evaluation_data/metadata/review_id_mapping.json"
      provides: "Mapping from anonymized review IDs to provider/scenario"
  key_links:
    - from: "backend/evaluation/generate_samples.py"
      to: "backend/evaluation_data/"
      via: "Script generates BRDs and writes to data directories"
      pattern: "evaluation_data"
---

<objective>
Generate all 15 BRDs across 3 providers and prepare blind review materials for human scoring.

Purpose: Execute the controlled experiment by running each test scenario through all 3 LLM providers, capturing BRD outputs and metadata, then anonymizing for unbiased human quality review.

Output: 15 generated BRDs, metadata files, anonymized blind review package with scoring template.
</objective>

<execution_context>
@C:/Users/ibcve/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ibcve/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/61-quality-comparison-decision/61-RESEARCH.md
@.planning/phases/61-quality-comparison-decision/61-01-SUMMARY.md
@.planning/phases/61-quality-comparison-decision/61-02-SUMMARY.md
@backend/evaluation/generate_samples.py
@backend/evaluation/test_scenarios.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Generate 15 BRDs across all providers and create blind review materials</name>
  <files>
    backend/evaluation_data/anthropic/TC-01.md
    backend/evaluation_data/anthropic/TC-02.md
    backend/evaluation_data/anthropic/TC-03.md
    backend/evaluation_data/anthropic/TC-04.md
    backend/evaluation_data/anthropic/TC-05.md
    backend/evaluation_data/claude-code-sdk/TC-01.md
    backend/evaluation_data/claude-code-sdk/TC-02.md
    backend/evaluation_data/claude-code-sdk/TC-03.md
    backend/evaluation_data/claude-code-sdk/TC-04.md
    backend/evaluation_data/claude-code-sdk/TC-05.md
    backend/evaluation_data/claude-code-cli/TC-01.md
    backend/evaluation_data/claude-code-cli/TC-02.md
    backend/evaluation_data/claude-code-cli/TC-03.md
    backend/evaluation_data/claude-code-cli/TC-04.md
    backend/evaluation_data/claude-code-cli/TC-05.md
    backend/evaluation_data/metadata/
    backend/evaluation_data/blind_review/scoring_template.csv
    backend/evaluation_data/blind_review/
  </files>
  <action>
    Run the generation script to produce all 15 BRDs and prepare blind review materials.

    **Pre-requisites check:**
    1. Verify ANTHROPIC_API_KEY is set (required for all 3 providers)
    2. Verify backend database is accessible (needed for tool execution context)
    3. Verify `claude` CLI is available on PATH (needed for claude-code-cli provider)

    **Generation execution:**
    ```bash
    cd backend
    python -m evaluation.generate_samples all
    ```

    This runs:
    - `generate_all_samples()` — generates 15 BRDs (3 providers x 5 scenarios)
    - `anonymize_for_blind_review()` — creates blind review package

    **Expected output:**
    - 5 BRDs in `evaluation_data/anthropic/`
    - 5 BRDs in `evaluation_data/claude-code-sdk/`
    - 5 BRDs in `evaluation_data/claude-code-cli/`
    - 15 metadata files in `evaluation_data/metadata/`
    - 15 anonymized BRDs in `evaluation_data/blind_review/`
    - `evaluation_data/blind_review/scoring_template.csv` with 15 rows
    - `evaluation_data/metadata/review_id_mapping.json`

    **Error handling:**
    - If a single provider fails (e.g., CLI subprocess timeout), document the failure and continue with remaining providers
    - If generation partially completes, re-run for missing samples only (script should skip existing files)
    - Log any errors during generation for debugging

    **Post-generation validation:**
    - Verify all 15 BRDs exist and are non-empty
    - Verify each BRD contains expected sections (executive summary, objectives, personas, etc.)
    - Verify metadata files contain token_usage and generation_time
    - Verify scoring template has 15 rows with correct review_id column
    - Print cost summary across all providers

    **IMPORTANT**: This task makes real API calls and costs real money (estimated $0.50-$2.00 total for 15 BRDs with Sonnet). The ANTHROPIC_API_KEY must have sufficient credits.

    **IMPORTANT**: This task may take 15-45 minutes depending on provider response times. Agent providers (SDK/CLI) may take longer due to multi-turn agent loops.

    If any provider consistently fails, document the failure mode and proceed with available providers. A minimum of 5 baseline (anthropic) BRDs is required for the comparison to proceed.
  </action>
  <verify>
    - `ls backend/evaluation_data/anthropic/*.md | wc -l` shows 5 files
    - `ls backend/evaluation_data/claude-code-sdk/*.md | wc -l` shows 5 files
    - `ls backend/evaluation_data/claude-code-cli/*.md | wc -l` shows 5 files
    - `ls backend/evaluation_data/metadata/*.json | wc -l` shows at least 15 files (metadata)
    - `ls backend/evaluation_data/blind_review/*.md | wc -l` shows 15 files
    - `cat backend/evaluation_data/blind_review/scoring_template.csv | wc -l` shows 16 lines (header + 15 data rows)
    - Each BRD file is non-empty and contains markdown content
  </verify>
  <done>
    15 BRDs generated across all 3 providers (5 each), metadata captured, and blind review materials prepared with anonymized BRDs and empty scoring template.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Human scores 15 BRDs using quality rubric</name>
  <what-built>
    15 BRDs have been generated across 3 providers and anonymized for blind review.
    The scoring template and quality rubric are ready for human evaluation.
  </what-built>
  <how-to-verify>
    **Scoring instructions:**

    1. Open the quality rubric reference:
       ```
       cd backend && python -c "from evaluation.quality_rubric import RUBRIC_DESCRIPTION; print(RUBRIC_DESCRIPTION)"
       ```
       This prints the scoring criteria for all 4 dimensions.

    2. Open the scoring template:
       `backend/evaluation_data/blind_review/scoring_template.csv`
       Edit this file (Excel, Google Sheets, or text editor).

    3. For each review_id row:
       - Read the corresponding BRD: `backend/evaluation_data/blind_review/{review_id}.md`
       - Score each dimension 1-4 based on the rubric criteria:
         - **completeness** (1-4): Are all BRD sections present with sufficient detail?
         - **ac_quality** (1-4): Are acceptance criteria specific, measurable, testable?
         - **consistency** (1-4): Is terminology unified, no contradictions?
         - **error_coverage** (1-4): Are error cases and edge cases addressed?
       - Add optional notes in the `notes` column

    4. Save the completed CSV. All 15 rows must have scores in all 4 dimensions.

    **Tips for consistent scoring:**
    - Score the first 2-3 BRDs, then review your scores for calibration before continuing
    - Take breaks between batches (5 BRDs per session recommended)
    - The BRDs are anonymized — you should not know which provider generated each one
    - Estimated time: 3-5 hours total (15 BRDs x 12-20 minutes each)

    5. After scoring, verify completeness:
       ```
       cd backend && python -c "
       import csv
       with open('evaluation_data/blind_review/scoring_template.csv') as f:
           reader = csv.DictReader(f)
           rows = list(reader)
           scored = sum(1 for r in rows if r['completeness'] and r['ac_quality'] and r['consistency'] and r['error_coverage'])
           print(f'Scored: {scored}/15')
       "
       ```

    When all 15 BRDs are scored, type "scored" to proceed to analysis.
  </how-to-verify>
  <resume-signal>Type "scored" when all 15 BRDs have been scored in the CSV, or describe any issues</resume-signal>
</task>

</tasks>

<verification>
1. All 15 BRDs exist in evaluation_data/{provider}/ directories
2. Metadata files contain token_usage and generation_time for all samples
3. Blind review materials are complete (anonymized BRDs + scoring template)
4. Human scoring complete (all 15 rows have scores in all 4 dimensions)
5. Scoring CSV is valid and parseable
</verification>

<success_criteria>
- 5+ BRDs generated per provider (QUAL-01, QUAL-02, QUAL-03)
- Quality metrics scored across all samples (QUAL-04 partial — scoring done)
- Blind review protocol followed (reviewer did not know provider identity)
- Scoring template completed and saved
</success_criteria>

<output>
After completion, create `.planning/phases/61-quality-comparison-decision/61-03-SUMMARY.md`
</output>
