---
phase: 19-backend-abstraction
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/services/llm/__init__.py
  - backend/app/services/llm/base.py
  - backend/app/services/llm/factory.py
  - backend/app/services/llm/anthropic_adapter.py
autonomous: true

must_haves:
  truths:
    - "LLMAdapter abstract base class defines stream_chat() contract"
    - "StreamChunk dataclass can represent text, thinking, tool_use, complete, and error chunks"
    - "LLMProvider enum includes ANTHROPIC value"
    - "LLMFactory.create('anthropic') returns working AnthropicAdapter instance"
    - "AnthropicAdapter.stream_chat() yields StreamChunk objects from Anthropic API"
  artifacts:
    - path: "backend/app/services/llm/__init__.py"
      provides: "Package exports"
      exports: ["LLMAdapter", "LLMProvider", "StreamChunk", "LLMFactory", "AnthropicAdapter"]
    - path: "backend/app/services/llm/base.py"
      provides: "Abstract base class, StreamChunk dataclass, LLMProvider enum"
      min_lines: 50
    - path: "backend/app/services/llm/factory.py"
      provides: "LLMFactory class with create() method"
      min_lines: 25
    - path: "backend/app/services/llm/anthropic_adapter.py"
      provides: "AnthropicAdapter implementation"
      min_lines: 80
  key_links:
    - from: "backend/app/services/llm/factory.py"
      to: "backend/app/services/llm/anthropic_adapter.py"
      via: "imports AnthropicAdapter"
      pattern: "from .anthropic_adapter import AnthropicAdapter"
    - from: "backend/app/services/llm/anthropic_adapter.py"
      to: "backend/app/services/llm/base.py"
      via: "inherits LLMAdapter"
      pattern: "class AnthropicAdapter\\(LLMAdapter\\)"
---

<objective>
Create the LLM provider abstraction layer with abstract base class, StreamChunk dataclass, factory, and extract Anthropic-specific streaming logic into AnthropicAdapter.

Purpose: Establish provider-agnostic foundation that enables future Gemini and DeepSeek adapters (Phase 21) without changing the interface AIService uses.

Output: New `backend/app/services/llm/` package with working AnthropicAdapter that mirrors current streaming behavior.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/19-backend-abstraction/19-RESEARCH.md

# Current implementation to extract from:
@backend/app/services/ai_service.py
@backend/app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LLM base module with abstractions</name>
  <files>
    backend/app/services/llm/__init__.py
    backend/app/services/llm/base.py
  </files>
  <action>
Create `backend/app/services/llm/` directory and two files:

**base.py:**
- Import: `abc.ABC`, `abc.abstractmethod`, `dataclasses.dataclass`, `enum.Enum`, `typing` (AsyncGenerator, Dict, Any, List, Optional)
- Create `LLMProvider(str, Enum)` with value `ANTHROPIC = "anthropic"`
- Create `@dataclass StreamChunk` with fields:
  - `chunk_type: str` - One of: "text", "thinking", "tool_use", "tool_result", "complete", "error"
  - `content: str = ""` - Text content for text chunks, error message for error chunks
  - `thinking_content: Optional[str] = None` - Reserved for future providers (Gemini, DeepSeek)
  - `tool_call: Optional[Dict[str, Any]] = None` - For tool_use chunks: {"id": str, "name": str, "input": dict}
  - `usage: Optional[Dict[str, int]] = None` - For complete chunks: {"input_tokens": int, "output_tokens": int}
  - `error: Optional[str] = None` - For error chunks
- Create `class LLMAdapter(ABC)` with:
  - `@property @abstractmethod def provider(self) -> LLMProvider`
  - `@abstractmethod async def stream_chat(self, messages: List[Dict[str, Any]], system_prompt: str, tools: Optional[List[Dict[str, Any]]] = None, max_tokens: int = 4096) -> AsyncGenerator[StreamChunk, None]`

**__init__.py:**
- Export: `LLMAdapter`, `LLMProvider`, `StreamChunk` from `.base`
- (LLMFactory and AnthropicAdapter added in later tasks)

Use docstrings explaining each class's purpose. Match the patterns from research document.
  </action>
  <verify>
```bash
cd backend && python -c "from app.services.llm import LLMAdapter, LLMProvider, StreamChunk; print('Imports OK'); print(f'Provider: {LLMProvider.ANTHROPIC.value}')"
```
  </verify>
  <done>LLMAdapter ABC, StreamChunk dataclass, and LLMProvider enum are importable and functional</done>
</task>

<task type="auto">
  <name>Task 2: Create AnthropicAdapter implementation</name>
  <files>backend/app/services/llm/anthropic_adapter.py</files>
  <action>
Create `anthropic_adapter.py` that extracts streaming logic from current `ai_service.py`:

**AnthropicAdapter class:**
- `__init__(self, api_key: str, model: Optional[str] = None)`:
  - Store `api_key` and `model` (default to "claude-sonnet-4-5-20250929")
  - Create `self.client = anthropic.AsyncAnthropic(api_key=api_key)`
- `@property def provider(self) -> LLMProvider: return LLMProvider.ANTHROPIC`
- `async def stream_chat(...)` implementation:

**stream_chat behavior (single API call, no tool loop):**
The adapter handles ONE message stream call. Tool loop orchestration stays in AIService.

```python
async def stream_chat(
    self,
    messages: List[Dict[str, Any]],
    system_prompt: str,
    tools: Optional[List[Dict[str, Any]]] = None,
    max_tokens: int = 4096,
) -> AsyncGenerator[StreamChunk, None]:
    try:
        async with self.client.messages.stream(
            model=self.model,
            max_tokens=max_tokens,
            messages=messages,
            tools=tools or [],
            system=system_prompt
        ) as stream:
            async for text in stream.text_stream:
                yield StreamChunk(chunk_type="text", content=text)

            final = await stream.get_final_message()

            # Yield tool_use chunks if present
            for block in final.content:
                if block.type == "tool_use":
                    yield StreamChunk(
                        chunk_type="tool_use",
                        tool_call={"id": block.id, "name": block.name, "input": block.input}
                    )

            # Yield complete chunk with usage
            yield StreamChunk(
                chunk_type="complete",
                content="",  # Content already streamed
                usage={
                    "input_tokens": final.usage.input_tokens,
                    "output_tokens": final.usage.output_tokens
                }
            )

    except anthropic.APIError as e:
        yield StreamChunk(chunk_type="error", error=f"Anthropic API error: {str(e)}")
    except Exception as e:
        yield StreamChunk(chunk_type="error", error=f"Unexpected error: {str(e)}")
```

**Key differences from current ai_service.py:**
- NO while True loop - single API call only
- NO tool execution - just yields tool_use chunks
- NO SSE event formatting - yields StreamChunk objects
- Errors yield StreamChunk(chunk_type="error") instead of SSE events

**Update __init__.py:** Add `from .anthropic_adapter import AnthropicAdapter` and export it.
  </action>
  <verify>
```bash
cd backend && python -c "
from app.services.llm import AnthropicAdapter, LLMProvider
adapter = AnthropicAdapter(api_key='test-key')
print(f'Provider: {adapter.provider.value}')
print(f'Model: {adapter.model}')
print('AnthropicAdapter instantiation OK')
"
```
  </verify>
  <done>AnthropicAdapter can be instantiated and has correct provider property and model default</done>
</task>

<task type="auto">
  <name>Task 3: Create LLMFactory</name>
  <files>
    backend/app/services/llm/factory.py
    backend/app/services/llm/__init__.py
  </files>
  <action>
Create `factory.py` with LLMFactory class:

```python
from typing import Optional
from .base import LLMAdapter, LLMProvider
from .anthropic_adapter import AnthropicAdapter
from app.config import settings


class LLMFactory:
    """Factory for creating LLM provider adapters."""

    _adapters = {
        LLMProvider.ANTHROPIC: AnthropicAdapter,
    }

    @classmethod
    def create(
        cls,
        provider: str,
        model: Optional[str] = None,
    ) -> LLMAdapter:
        """
        Create adapter for specified provider.

        Args:
            provider: Provider name string (e.g., "anthropic")
            model: Optional model override

        Returns:
            LLMAdapter instance

        Raises:
            ValueError: If provider is not supported or no API key configured
        """
        try:
            provider_enum = LLMProvider(provider)
        except ValueError:
            raise ValueError(f"Unsupported provider: {provider}")

        adapter_class = cls._adapters.get(provider_enum)
        if not adapter_class:
            raise ValueError(f"No adapter registered for provider: {provider}")

        api_key = cls._get_api_key(provider_enum)
        return adapter_class(api_key=api_key, model=model)

    @classmethod
    def _get_api_key(cls, provider: LLMProvider) -> str:
        """Get API key from settings for provider."""
        if provider == LLMProvider.ANTHROPIC:
            key = settings.anthropic_api_key
            if not key:
                raise ValueError("ANTHROPIC_API_KEY not configured")
            return key
        raise ValueError(f"No API key configuration for {provider}")

    @classmethod
    def list_providers(cls) -> list[str]:
        """Return list of supported provider names."""
        return [p.value for p in cls._adapters.keys()]
```

**Update __init__.py:**
- Add `from .factory import LLMFactory`
- Final exports: `LLMAdapter`, `LLMProvider`, `StreamChunk`, `LLMFactory`, `AnthropicAdapter`
  </action>
  <verify>
```bash
cd backend && python -c "
from app.services.llm import LLMFactory, LLMProvider
print(f'Providers: {LLMFactory.list_providers()}')
# Note: Will fail if ANTHROPIC_API_KEY not set, which is expected in test
try:
    adapter = LLMFactory.create('anthropic')
    print(f'Created adapter: {adapter.provider.value}')
except ValueError as e:
    if 'API_KEY' in str(e):
        print('Factory works (API key not set in test env, expected)')
    else:
        raise
"
```
  </verify>
  <done>LLMFactory.create('anthropic') returns AnthropicAdapter when API key is configured</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Import test:**
```bash
cd backend && python -c "
from app.services.llm import (
    LLMAdapter, LLMProvider, StreamChunk,
    LLMFactory, AnthropicAdapter
)
print('All exports available')
print(f'Providers: {LLMFactory.list_providers()}')
"
```

2. **Type check (if mypy available):**
```bash
cd backend && python -m mypy app/services/llm/ --ignore-missing-imports 2>/dev/null || echo "mypy not installed, skipping"
```

3. **StreamChunk field test:**
```bash
cd backend && python -c "
from app.services.llm import StreamChunk
chunk = StreamChunk(chunk_type='text', content='hello')
print(f'chunk_type: {chunk.chunk_type}')
print(f'content: {chunk.content}')
print(f'thinking_content: {chunk.thinking_content}')
print(f'tool_call: {chunk.tool_call}')
print(f'usage: {chunk.usage}')
print(f'error: {chunk.error}')
"
```
</verification>

<success_criteria>
- LLM package exists at `backend/app/services/llm/`
- LLMAdapter ABC defines provider property and stream_chat method
- StreamChunk dataclass has all required fields (chunk_type, content, thinking_content, tool_call, usage, error)
- LLMProvider enum has ANTHROPIC value
- AnthropicAdapter implements LLMAdapter, yields StreamChunk from Anthropic API
- LLMFactory.create("anthropic") returns AnthropicAdapter instance
- All imports work without errors
</success_criteria>

<output>
After completion, create `.planning/phases/19-backend-abstraction/19-01-SUMMARY.md`
</output>
