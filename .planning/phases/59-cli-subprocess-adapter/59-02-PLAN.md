---
phase: 59-cli-subprocess-adapter
plan: 02
type: execute
wave: 2
depends_on: ["59-01"]
files_modified:
  - backend/tests/unit/llm/test_claude_cli_adapter.py
autonomous: true

must_haves:
  truths:
    - "Unit tests verify subprocess spawning with correct CLI flags"
    - "Unit tests verify JSON event translation for all 4 event types"
    - "Unit tests verify subprocess cleanup in normal and error paths"
    - "Unit tests verify context validation (missing context yields error)"
    - "All existing unit tests still pass (zero regressions)"
  artifacts:
    - path: "backend/tests/unit/llm/test_claude_cli_adapter.py"
      provides: "Comprehensive unit tests for ClaudeCLIAdapter"
      contains: "asyncio.create_subprocess_exec"
  key_links:
    - from: "backend/tests/unit/llm/test_claude_cli_adapter.py"
      to: "backend/app/services/llm/claude_cli_adapter.py"
      via: "import and test of ClaudeCLIAdapter"
      pattern: "from app.services.llm.claude_cli_adapter import ClaudeCLIAdapter"
---

<objective>
Add comprehensive unit tests for ClaudeCLIAdapter covering subprocess lifecycle, JSON event translation, error handling, and integration with AIService routing.

Purpose: Verify the CLI adapter implementation is correct and establish test patterns for ongoing maintenance. Tests use mocks exclusively (no real subprocess or API calls needed).

Output: Updated `test_claude_cli_adapter.py` with 15-20+ unit tests covering all code paths.
</objective>

<execution_context>
@C:/Users/ibcve/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ibcve/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/59-cli-subprocess-adapter/59-01-SUMMARY.md
@.planning/phases/58-agent-sdk-adapter/58-02-SUMMARY.md

# Key test patterns
@backend/tests/unit/llm/test_claude_agent_adapter.py
@backend/tests/unit/test_ai_service_agent.py

# Implementation under test
@backend/app/services/llm/claude_cli_adapter.py
@backend/app/services/llm/base.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement comprehensive unit tests for ClaudeCLIAdapter</name>
  <files>backend/tests/unit/llm/test_claude_cli_adapter.py</files>
  <action>
Replace the existing stub tests in `test_claude_cli_adapter.py` with comprehensive tests. Follow the same patterns established in `test_claude_agent_adapter.py` (Phase 58-02).

**Key mocking strategies:**
- Mock `shutil.which` to return a fake CLI path (all tests need this since adapter checks CLI at init)
- Mock `asyncio.create_subprocess_exec` to return a mock process with controllable stdout/stderr
- Use `AsyncMock` for process.wait(), mock process.returncode
- Create helper function `make_mock_process(stdout_lines, returncode=0)` that returns a mock process
- Mock stdout as an async iterator that yields encoded JSON lines

**Test classes to create:**

### TestClaudeCLIAdapterInit (keep existing + add):
- `test_initializes_with_api_key` (existing, add shutil.which mock)
- `test_uses_default_model` (existing, add shutil.which mock)
- `test_uses_custom_model` (existing, add shutil.which mock)
- `test_provider_returns_claude_code_cli` (existing, add shutil.which mock)
- `test_is_agent_provider_true` - Verify `is_agent_provider == True`
- `test_raises_runtime_error_when_cli_not_found` - Mock `shutil.which` returning None, verify RuntimeError with helpful message
- `test_stores_cli_path` - Verify `self.cli_path` is set from shutil.which result
- `test_has_set_context_method` - Verify method exists and sets db/project_id/thread_id

### TestClaudeCLIAdapterEventTranslation:
- `test_translates_stream_event_text_delta` - Input: `{"type": "stream_event", "event": {"type": "content_block_delta", "delta": {"type": "text_delta", "text": "Hello"}}}` -> `StreamChunk(chunk_type="text", content="Hello")`
- `test_translates_assistant_message_tool_use` - Input: `{"type": "assistant_message", "content": [{"type": "tool_use", "id": "tool_123", "name": "save_artifact", "input": {"title": "BRD"}}]}` -> `StreamChunk(chunk_type="tool_use", tool_call={...})`
- `test_translates_result_event` - Input: `{"type": "result", "usage": {"input_tokens": 100, "output_tokens": 200}}` -> `StreamChunk(chunk_type="complete", usage={...})`
- `test_translates_error_event` - Input: `{"type": "error", "message": "Rate limited"}` -> `StreamChunk(chunk_type="error", error="Rate limited")`
- `test_returns_none_for_unknown_event` - Input: `{"type": "unknown_type"}` -> `None`
- `test_handles_stream_event_without_text_delta` - Input: `{"type": "stream_event", "event": {"type": "other"}}` -> `None`

### TestClaudeCLIAdapterStreamChat:
- `test_stream_chat_yields_text_chunks` - Mock process stdout with text delta events, verify StreamChunk text chunks yielded
- `test_stream_chat_yields_complete_chunk` - Mock process with result event, verify complete chunk with usage
- `test_stream_chat_yields_error_on_process_failure` - Mock process with returncode != 0, verify error chunk yielded
- `test_stream_chat_yields_error_when_context_not_set` - Don't call set_context(), verify error chunk about missing context
- `test_stream_chat_skips_empty_lines` - Include empty lines in stdout, verify they're skipped
- `test_stream_chat_handles_json_decode_error` - Include malformed JSON in stdout, verify graceful handling (no crash, logged warning)
- `test_stream_chat_spawns_subprocess_with_correct_args` - Verify create_subprocess_exec called with correct CLI path and flags
- `test_stream_chat_passes_api_key_in_env` - Verify ANTHROPIC_API_KEY is in subprocess environment
- `test_stream_chat_sets_contextvars` - Verify ContextVars (_db_context, _project_id_context, _thread_id_context) are set before subprocess spawns

### TestClaudeCLIAdapterSubprocessCleanup:
- `test_cleanup_terminates_running_process` - Mock process still running (returncode=None), verify terminate() called
- `test_cleanup_kills_on_timeout` - Mock process that doesn't respond to terminate (wait_for times out), verify kill() called
- `test_cleanup_not_called_when_process_completed` - Mock process that completed normally (returncode=0), verify terminate NOT called

### TestClaudeCLIAdapterMessageConversion:
- `test_converts_string_content` - Messages with string content extracted correctly
- `test_converts_list_content` - Messages with list content (text parts) extracted correctly
- `test_handles_empty_messages` - Empty message list returns empty string

### TestClaudeCLIAdapterFactory (keep existing, update mocks):
- Keep existing factory tests, add `@patch('app.services.llm.claude_cli_adapter.shutil.which', return_value='/usr/bin/claude')` to all tests that instantiate the adapter

**Mock process helper pattern (use for all stream_chat tests):**
```python
import asyncio
from unittest.mock import AsyncMock, MagicMock, patch

async def make_stdout_lines(lines):
    """Create async iterator from list of JSON strings."""
    for line in lines:
        yield (line + "\n").encode("utf-8")

def make_mock_process(stdout_lines, returncode=0, stderr_output=b""):
    """Create mock subprocess process."""
    process = MagicMock()
    process.stdout = make_stdout_lines(stdout_lines)
    process.stderr = AsyncMock()
    process.stderr.read = AsyncMock(return_value=stderr_output)
    process.returncode = returncode
    process.wait = AsyncMock(return_value=returncode)
    process.terminate = MagicMock()
    process.kill = MagicMock()
    return process
```

**Important:** All tests must use `@patch('app.services.llm.claude_cli_adapter.shutil.which', return_value='/usr/bin/claude')` or similar to prevent RuntimeError on machines without claude CLI installed.

For stream_chat tests, also patch `asyncio.create_subprocess_exec` to return the mock process:
```python
@patch('app.services.llm.claude_cli_adapter.asyncio.create_subprocess_exec')
@patch('app.services.llm.claude_cli_adapter.shutil.which', return_value='/usr/bin/claude')
```

**Pattern from Phase 58-02:** Use `async for chunk in adapter.stream_chat(...)` to collect chunks, then assert on the collected list.
  </action>
  <verify>
1. Run `python -m pytest tests/unit/llm/test_claude_cli_adapter.py -v` - all tests pass
2. Run `python -m pytest tests/unit/ -x -q` - full unit suite passes with zero regressions
3. Verify test count: at least 20 tests in test_claude_cli_adapter.py
  </verify>
  <done>
Comprehensive unit tests for ClaudeCLIAdapter covering:
- Initialization (CLI path verification, agent provider attribute, model defaults)
- Event translation (all 4 event types + edge cases)
- Stream chat (subprocess spawning, text streaming, completion, errors)
- Subprocess cleanup (terminate, kill on timeout, no cleanup when completed)
- Message conversion (string, list, empty)
- Factory integration (creation, API key validation, custom model)
- Zero regressions in full unit test suite
  </done>
</task>

<task type="auto">
  <name>Task 2: Run full test suite and verify complete coverage</name>
  <files>backend/tests/unit/llm/test_claude_cli_adapter.py</files>
  <action>
1. Run the full LLM adapter test suite:
   ```bash
   cd backend && python -m pytest tests/unit/llm/ -v --tb=short
   ```
   All LLM adapter tests should pass (Anthropic, Gemini, DeepSeek, Claude Agent, Claude CLI).

2. Run the full unit test suite:
   ```bash
   cd backend && python -m pytest tests/unit/ -x -q
   ```
   All unit tests should pass. Record the total count.

3. Compare with Phase 58 baseline (239 tests). New count should be 239 - (removed stub tests) + (new CLI tests). Expected: ~255+ tests total.

4. If any tests fail:
   - If existing tests fail due to import changes, fix the import (not the implementation)
   - If new CLI tests fail, debug and fix the test (ensure mocks are correct)
   - Re-run until all pass

5. Verify the AIService agent routing tests still pass (these test the is_agent_provider path that CLI adapter uses):
   ```bash
   cd backend && python -m pytest tests/unit/test_ai_service_agent.py -v
   ```
  </action>
  <verify>
1. `python -m pytest tests/unit/llm/ -v` - all LLM adapter tests pass
2. `python -m pytest tests/unit/ -x -q` - all unit tests pass
3. `python -m pytest tests/unit/test_ai_service_agent.py -v` - all agent routing tests pass
4. Total test count >= 255
  </verify>
  <done>
Full test suite passes with zero regressions:
- All LLM adapter tests pass (Anthropic, Gemini, DeepSeek, Claude Agent, Claude CLI)
- All AIService routing tests pass
- Total unit test count meets or exceeds Phase 58 baseline + new CLI tests
  </done>
</task>

</tasks>

<verification>
1. CLI adapter tests: `python -m pytest tests/unit/llm/test_claude_cli_adapter.py -v` - 20+ tests pass
2. LLM suite: `python -m pytest tests/unit/llm/ -v` - all pass
3. Full suite: `python -m pytest tests/unit/ -x -q` - all pass, zero regressions
4. Agent routing: `python -m pytest tests/unit/test_ai_service_agent.py -v` - all pass
</verification>

<success_criteria>
- 20+ unit tests for ClaudeCLIAdapter covering all code paths
- All tests use mocks (no real subprocess or API calls)
- All existing unit tests continue passing (zero regressions)
- Test patterns consistent with Phase 58-02 established patterns
</success_criteria>

<output>
After completion, create `.planning/phases/59-cli-subprocess-adapter/59-02-SUMMARY.md`
</output>
