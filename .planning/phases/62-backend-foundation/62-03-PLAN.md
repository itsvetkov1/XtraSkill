---
phase: 62-backend-foundation
plan: 03
type: execute
wave: 2
depends_on: [62-01]
files_modified:
  - backend/app/services/ai_service.py
  - backend/app/routes/conversations.py
  - backend/app/services/file_validator.py
  - backend/app/services/token_tracking.py
autonomous: true
requirements: [LOGIC-01, LOGIC-02, LOGIC-03]

must_haves:
  truths:
    - "AIService initialized with thread_type='assistant' uses claude-code-cli adapter regardless of provider argument"
    - "AIService initialized with thread_type='assistant' has empty tools list (no search_documents, no save_artifact)"
    - "AIService initialized with thread_type='assistant' sends empty system prompt (no BA instructions)"
    - "Chat endpoint passes thread.thread_type to AIService constructor"
    - "File validator applies expanded limits for Assistant threads (5MB images, 32MB PDFs)"
    - "Token tracking records thread_type for separate usage analytics"
  artifacts:
    - path: "backend/app/services/ai_service.py"
      provides: "AIService with conditional behavior based on thread_type"
      contains: "thread_type"
    - path: "backend/app/routes/conversations.py"
      provides: "Chat endpoint passing thread_type to AIService"
      contains: "thread_type"
    - path: "backend/app/services/file_validator.py"
      provides: "Thread-type-aware file size validation with expanded Assistant limits"
      contains: "get_max_file_size"
    - path: "backend/app/services/token_tracking.py"
      provides: "Token tracking with thread_type parameter for analytics"
      contains: "thread_type"
  key_links:
    - from: "backend/app/routes/conversations.py"
      to: "backend/app/services/ai_service.py"
      via: "AIService(provider=..., thread_type=thread.thread_type)"
      pattern: "AIService.*thread_type"
    - from: "backend/app/services/ai_service.py"
      to: "backend/app/services/llm/factory.py"
      via: "LLMFactory.create('claude-code-cli') when thread_type='assistant'"
      pattern: "claude-code-cli"
---

<objective>
Add thread_type-conditional logic to AIService (system prompt, tools, provider override), update chat endpoint to pass thread_type, expand file validation limits for Assistant threads, and add thread_type to usage tracking.

Purpose: Ensure Assistant threads behave differently from BA threads at the service layer -- no BA system prompt, no BA tools, hardcoded claude-code-cli adapter, expanded file limits, and separate usage tracking.
Output: AIService with conditional routing, updated chat endpoint, thread-type-aware file validator, and usage tracking with thread_type.
</objective>

<execution_context>
@/Users/a1testingmac/.claude/get-shit-done/workflows/execute-plan.md
@/Users/a1testingmac/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/62-backend-foundation/62-RESEARCH.md
@.planning/phases/62-backend-foundation/62-01-SUMMARY.md
@backend/app/services/ai_service.py
@backend/app/routes/conversations.py
@backend/app/services/file_validator.py
@backend/app/services/token_tracking.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add thread_type conditional logic to AIService and chat endpoint</name>
  <files>backend/app/services/ai_service.py, backend/app/routes/conversations.py</files>
  <action>
**AIService changes (backend/app/services/ai_service.py):**

1. Update `__init__` to accept thread_type parameter:
   ```python
   def __init__(self, provider: str = "anthropic", thread_type: str = "ba_assistant"):
       # LOGIC-03: Override provider for Assistant threads (per locked decision: hardcoded to claude-code-cli)
       if thread_type == "assistant":
           provider = "claude-code-cli"

       self.adapter = LLMFactory.create(provider)
       self.thread_type = thread_type

       # LOGIC-02: Conditional tool loading (per locked decision: no BA tools for Assistant)
       if thread_type == "ba_assistant":
           self.tools = [DOCUMENT_SEARCH_TOOL, SAVE_ARTIFACT_TOOL]
       else:
           self.tools = []  # No BA tools for Assistant threads

       self.is_agent_provider = getattr(self.adapter, 'is_agent_provider', False)
   ```

2. Update `_stream_agent_chat` method -- conditional system prompt:
   - Find where SYSTEM_PROMPT is passed to adapter.stream_chat
   - Replace hardcoded SYSTEM_PROMPT with conditional:
     ```python
     # LOGIC-01: No system prompt for Assistant threads (per locked decision)
     system_prompt = SYSTEM_PROMPT if self.thread_type == "ba_assistant" else ""
     ```
   - Apply this in the `self.adapter.stream_chat()` call within _stream_agent_chat

3. Update `stream_chat` method (direct API path) -- conditional system prompt:
   - Same conditional system prompt logic in the `self.adapter.stream_chat()` call
   - The tools are already conditional via self.tools from __init__

**Chat endpoint changes (backend/app/routes/conversations.py):**

4. Update the `stream_chat` endpoint function:
   - Change AIService initialization from:
     ```python
     ai_service = AIService(provider=provider)
     ```
   - To:
     ```python
     ai_service = AIService(provider=provider, thread_type=thread.thread_type or "ba_assistant")
     ```
   - The `or "ba_assistant"` is a safety fallback for any threads that might not have the field yet.

Per locked decisions:
- "No system prompt for Assistant threads -- user messages go directly to the LLM with no instructions"
- "Hardcoded to claude-code-cli adapter -- no override or provider selection possible for Assistant threads"
- "Full conversation history -- all prior messages in the thread sent as context (same as BA threads)"
- "Include thinking/reasoning indicators during streaming -- consistent with BA mode experience"

The last two are already handled by existing code (conversation history built the same way, streaming events already include thinking indicators from the CLI adapter).
  </action>
  <verify>
Run: `cd /Users/a1testingmac/projects/XtraSkill/backend && source venv/bin/activate && python -c "
from app.services.ai_service import AIService
# Test BA mode (default)
ba = AIService(provider='anthropic', thread_type='ba_assistant')
assert len(ba.tools) == 2, f'BA should have 2 tools, got {len(ba.tools)}'
assert ba.thread_type == 'ba_assistant'
# Test Assistant mode
assistant = AIService(provider='anthropic', thread_type='assistant')
assert len(assistant.tools) == 0, f'Assistant should have 0 tools, got {len(assistant.tools)}'
assert assistant.thread_type == 'assistant'
# Verify provider override (can't easily check adapter type without CLI installed, but verify no crash)
print('AIService thread_type routing verified')
"
```
  </verify>
  <done>
- AIService accepts thread_type parameter (default: ba_assistant)
- Assistant threads forced to claude-code-cli adapter
- Assistant threads have empty tools list
- System prompt is empty string for Assistant threads, full BA prompt for BA threads
- Chat endpoint passes thread.thread_type to AIService
- Existing BA behavior unchanged (backward compatible)
  </done>
</task>

<task type="auto">
  <name>Task 2: Add thread-type-aware file validation and usage tracking</name>
  <files>backend/app/services/file_validator.py, backend/app/services/token_tracking.py</files>
  <action>
**File validator changes (backend/app/services/file_validator.py):**

1. Add expanded content types for Assistant mode:
   ```python
   # Assistant mode expanded content types (per locked decision: images + spreadsheets beyond BA mode)
   ASSISTANT_EXTRA_CONTENT_TYPES = [
       "image/png",
       "image/jpeg",
       "image/gif",
   ]
   ```

2. Add `get_max_file_size` function for thread-type-aware limits:
   ```python
   def get_max_file_size(content_type: str, thread_type: str = "ba_assistant") -> int:
       """Get max file size based on content type and thread type.

       Assistant threads have expanded limits per user decision:
       - Images (PNG, JPEG, GIF): 5MB (Anthropic Vision API limit)
       - PDFs: 32MB (Anthropic PDF API limit)
       - Other files: 10MB (same as BA mode)

       BA threads always use 10MB limit.
       """
       if thread_type == "assistant":
           if content_type in ["image/png", "image/jpeg", "image/gif"]:
               return 5 * 1024 * 1024   # 5MB
           elif content_type == "application/pdf":
               return 32 * 1024 * 1024  # 32MB
       # BA mode and Assistant non-image files: 10MB
       return MAX_FILE_SIZE
   ```

3. Update `validate_file_size` to accept optional max_size parameter:
   ```python
   def validate_file_size(file_bytes: bytes, max_size: int = MAX_FILE_SIZE) -> None:
       if len(file_bytes) > max_size:
           raise HTTPException(
               status_code=413,
               detail=f"File too large. Maximum size: {max_size / (1024*1024):.1f}MB"
           )
   ```

4. Update `validate_file_security` to accept optional max_size parameter:
   ```python
   def validate_file_security(file_bytes: bytes, content_type: str, max_size: int = MAX_FILE_SIZE) -> None:
       validate_file_size(file_bytes, max_size)
       validate_magic_number(file_bytes, content_type)
       # ... rest unchanged
   ```

5. Add image MIME types to MAGIC_TYPE_MAP:
   ```python
   "image/png": ["image/png"],
   "image/jpeg": ["image/jpeg"],
   "image/gif": ["image/gif"],
   ```

**Token tracking changes (backend/app/services/token_tracking.py):**

6. Update `track_token_usage` function signature to accept optional thread_type:
   ```python
   async def track_token_usage(
       db: AsyncSession,
       user_id: str,
       model: str,
       input_tokens: int,
       output_tokens: int,
       endpoint: str,
       thread_type: str = "ba_assistant"  # New parameter for analytics
   ) -> None:
   ```
   NOTE: Do NOT modify the TokenUsage model or database schema in this plan -- that would require a migration. Instead, encode thread_type in the endpoint string for now:
   ```python
   # Encode thread_type in endpoint for future analytics separation
   # e.g., "/threads/xxx/chat" -> "/threads/xxx/chat [assistant]"
   if thread_type != "ba_assistant":
       endpoint = f"{endpoint} [{thread_type}]"
   ```
   This is a pragmatic approach that enables analytics queries (`WHERE endpoint LIKE '%[assistant]%'`) without requiring a schema change. A dedicated column can be added in a future phase if needed.

7. Update the chat endpoint in conversations.py to pass thread_type to track_token_usage:
   ```python
   await track_token_usage(
       db,
       current_user["user_id"],
       AGENT_MODEL,
       usage_data.get("input_tokens", 0),
       usage_data.get("output_tokens", 0),
       f"/threads/{thread_id}/chat",
       thread_type=thread.thread_type or "ba_assistant"
   )
   ```

Per locked decision: "Separate usage tracking per thread_type for future analytics"
Per Claude's discretion: "Usage tracking implementation details (counters, storage)" -- encoding in endpoint string avoids schema change while enabling analytics.
  </action>
  <verify>
Run: `cd /Users/a1testingmac/projects/XtraSkill/backend && source venv/bin/activate && python -c "
from app.services.file_validator import get_max_file_size, ASSISTANT_EXTRA_CONTENT_TYPES
# BA mode always 10MB
assert get_max_file_size('image/png', 'ba_assistant') == 10 * 1024 * 1024
assert get_max_file_size('application/pdf', 'ba_assistant') == 10 * 1024 * 1024
# Assistant mode expanded
assert get_max_file_size('image/png', 'assistant') == 5 * 1024 * 1024
assert get_max_file_size('image/jpeg', 'assistant') == 5 * 1024 * 1024
assert get_max_file_size('application/pdf', 'assistant') == 32 * 1024 * 1024
assert get_max_file_size('text/plain', 'assistant') == 10 * 1024 * 1024
print('File validator thread_type limits verified')

from app.services.token_tracking import track_token_usage
import inspect
sig = inspect.signature(track_token_usage)
assert 'thread_type' in sig.parameters, 'track_token_usage missing thread_type param'
print('Token tracking thread_type param verified')
"
```
  </verify>
  <done>
- get_max_file_size returns expanded limits for Assistant threads
- ASSISTANT_EXTRA_CONTENT_TYPES defines image types (PNG, JPEG, GIF)
- validate_file_security and validate_file_size accept optional max_size parameter
- Image MIME types added to MAGIC_TYPE_MAP for magic number validation
- track_token_usage accepts thread_type parameter
- thread_type encoded in endpoint string for analytics separation
- Chat endpoint passes thread_type to track_token_usage
  </done>
</task>

</tasks>

<verification>
1. `AIService(thread_type='assistant')` creates adapter with claude-code-cli, empty tools, empty system prompt
2. `AIService(thread_type='ba_assistant')` keeps full BA tools and system prompt (backward compatible)
3. `get_max_file_size('image/png', 'assistant')` returns 5MB
4. `get_max_file_size('application/pdf', 'assistant')` returns 32MB
5. `get_max_file_size('text/plain', 'assistant')` returns 10MB (same as BA)
6. Chat endpoint passes thread_type to both AIService and track_token_usage
7. No existing BA functionality broken
</verification>

<success_criteria>
- Assistant threads use claude-code-cli adapter with no system prompt and no BA tools
- BA threads behave identically to before (full system prompt, all tools, original provider)
- File validation supports expanded limits for Assistant mode
- Usage tracking separates Assistant and BA usage
- Chat endpoint correctly routes thread_type from database to service layer
</success_criteria>

<output>
After completion, create `.planning/phases/62-backend-foundation/62-03-SUMMARY.md`
</output>
