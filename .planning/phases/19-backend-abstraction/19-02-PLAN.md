---
phase: 19-backend-abstraction
plan: 02
type: execute
wave: 2
depends_on: ["19-01"]
files_modified:
  - backend/app/services/ai_service.py
autonomous: true

must_haves:
  truths:
    - "AIService uses LLMFactory to create adapter instead of direct anthropic.AsyncAnthropic"
    - "stream_chat() yields identical SSE events as before refactor"
    - "Tool execution loop remains in AIService (adapter handles single calls)"
    - "Existing frontend receives same text_delta, tool_executing, artifact_created, message_complete, error events"
  artifacts:
    - path: "backend/app/services/ai_service.py"
      provides: "Refactored AIService using adapter pattern"
      contains: "LLMFactory.create"
  key_links:
    - from: "backend/app/services/ai_service.py"
      to: "backend/app/services/llm/factory.py"
      via: "imports LLMFactory"
      pattern: "from app.services.llm import.*LLMFactory"
    - from: "backend/app/services/ai_service.py"
      to: "backend/app/services/llm/base.py"
      via: "uses StreamChunk"
      pattern: "StreamChunk"
---

<objective>
Wire the LLM adapter pattern into AIService, replacing direct Anthropic SDK calls with LLMFactory + adapter pattern while maintaining identical SSE event output.

Purpose: Complete the adapter pattern integration so existing functionality works through the abstraction layer, enabling future providers to plug in without changing AIService.

Output: Refactored `ai_service.py` that uses adapter pattern internally but produces identical external behavior.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/19-backend-abstraction/19-RESEARCH.md
@.planning/phases/19-backend-abstraction/19-01-SUMMARY.md

# Files to modify:
@backend/app/services/ai_service.py
@backend/app/routes/conversations.py

# Adapter pattern created in Plan 01:
@backend/app/services/llm/__init__.py
@backend/app/services/llm/base.py
@backend/app/services/llm/anthropic_adapter.py
@backend/app/services/llm/factory.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Refactor AIService to use adapter pattern</name>
  <files>backend/app/services/ai_service.py</files>
  <action>
Modify `ai_service.py` to use the adapter pattern while preserving SSE event format:

**1. Update imports (top of file):**
```python
# Remove: import anthropic (no longer needed directly)
# Add:
from app.services.llm import LLMFactory, StreamChunk
```

**2. Remove MODEL constant:**
The model is now managed by the adapter. Remove:
```python
MODEL = "claude-sonnet-4-5-20250929"
```

**3. Modify AIService.__init__:**
```python
def __init__(self, provider: str = "anthropic"):
    """
    Initialize AI service with specified LLM provider.

    Args:
        provider: LLM provider name (default: "anthropic")
    """
    self.adapter = LLMFactory.create(provider)
    self.tools = [DOCUMENT_SEARCH_TOOL, SAVE_ARTIFACT_TOOL]
```

Remove `self.client = anthropic.AsyncAnthropic(...)` line.

**4. Refactor stream_chat() to use adapter:**

The key insight: Adapter handles SINGLE API calls and yields StreamChunk. AIService converts StreamChunk to SSE events AND handles the tool execution loop.

```python
async def stream_chat(
    self,
    messages: List[Dict[str, Any]],
    project_id: str,
    thread_id: str,
    db
) -> AsyncGenerator[Dict[str, Any], None]:
    """
    Stream chat response using LLM adapter with tool use.

    Yields SSE-formatted events:
    - text_delta: Incremental text from LLM
    - tool_executing: Tool is being executed
    - artifact_created: An artifact was generated and saved
    - message_complete: Final message with usage stats
    - error: Error occurred
    """
    import json

    try:
        while True:
            accumulated_text = ""
            tool_calls = []
            usage_data = None

            # Stream from adapter
            async for chunk in self.adapter.stream_chat(
                messages=messages,
                system_prompt=SYSTEM_PROMPT,
                tools=self.tools,
                max_tokens=4096
            ):
                if chunk.chunk_type == "text":
                    accumulated_text += chunk.content
                    yield {
                        "event": "text_delta",
                        "data": json.dumps({"text": chunk.content})
                    }

                elif chunk.chunk_type == "tool_use":
                    tool_calls.append(chunk.tool_call)

                elif chunk.chunk_type == "complete":
                    usage_data = chunk.usage

                elif chunk.chunk_type == "error":
                    yield {
                        "event": "error",
                        "data": json.dumps({"message": chunk.error})
                    }
                    return

            # If no tool calls, we're done
            if not tool_calls:
                yield {
                    "event": "message_complete",
                    "data": json.dumps({
                        "content": accumulated_text,
                        "usage": usage_data or {}
                    })
                }
                return

            # Execute tools and continue conversation
            tool_results = []
            for tool_call in tool_calls:
                tool_name = tool_call["name"]

                # Show tool-specific status
                if tool_name == "save_artifact":
                    yield {
                        "event": "tool_executing",
                        "data": json.dumps({"status": "Generating artifact..."})
                    }
                else:
                    yield {
                        "event": "tool_executing",
                        "data": json.dumps({"status": "Searching project documents..."})
                    }

                result, event_data = await self.execute_tool(
                    tool_name,
                    tool_call["input"],
                    project_id,
                    thread_id,
                    db
                )
                tool_results.append({
                    "type": "tool_result",
                    "tool_use_id": tool_call["id"],
                    "content": result
                })

                # Emit artifact_created event if artifact was saved
                if event_data and tool_name == "save_artifact":
                    yield {
                        "event": "artifact_created",
                        "data": json.dumps(event_data)
                    }

            # Build assistant message content for conversation history
            # Need to reconstruct content blocks for Anthropic format
            assistant_content = []
            if accumulated_text:
                assistant_content.append({"type": "text", "text": accumulated_text})
            for tc in tool_calls:
                assistant_content.append({
                    "type": "tool_use",
                    "id": tc["id"],
                    "name": tc["name"],
                    "input": tc["input"]
                })

            # Continue conversation with tool results
            messages.append({"role": "assistant", "content": assistant_content})
            messages.append({"role": "user", "content": tool_results})

            # Yield accumulated text separator before next iteration
            if accumulated_text:
                yield {
                    "event": "text_delta",
                    "data": json.dumps({"text": "\n\n"})
                }

    except Exception as e:
        yield {
            "event": "error",
            "data": json.dumps({"message": f"Unexpected error: {str(e)}"})
        }
```

**5. Keep execute_tool() unchanged:**
The tool execution logic is provider-agnostic and stays exactly as-is.

**6. Keep SYSTEM_PROMPT, DOCUMENT_SEARCH_TOOL, SAVE_ARTIFACT_TOOL unchanged:**
These are provider-agnostic and stay in ai_service.py.

**Summary of changes:**
- Remove `import anthropic`
- Add `from app.services.llm import LLMFactory, StreamChunk`
- Remove `MODEL` constant
- Change `__init__` to use `LLMFactory.create(provider)` instead of `anthropic.AsyncAnthropic`
- Change `stream_chat` to iterate over `self.adapter.stream_chat()` and convert StreamChunk to SSE events
- Keep tool loop in AIService (while True with tool execution)
- Keep execute_tool, SYSTEM_PROMPT, tools unchanged
  </action>
  <verify>
```bash
cd backend && python -c "
from app.services.ai_service import AIService, SYSTEM_PROMPT, DOCUMENT_SEARCH_TOOL, SAVE_ARTIFACT_TOOL
# Test instantiation (will fail if ANTHROPIC_API_KEY not set)
try:
    service = AIService()
    print(f'AIService created with adapter: {service.adapter.provider.value}')
    print(f'Tools count: {len(service.tools)}')
except ValueError as e:
    if 'API_KEY' in str(e):
        print('AIService init works (API key not set in test env, expected)')
    else:
        raise
print(f'SYSTEM_PROMPT length: {len(SYSTEM_PROMPT)} chars')
print(f'Tools defined: {[t[\"name\"] for t in [DOCUMENT_SEARCH_TOOL, SAVE_ARTIFACT_TOOL]]}')
"
```
  </verify>
  <done>AIService uses LLMFactory to create adapter, stream_chat converts StreamChunk to SSE events, tool loop preserved</done>
</task>

<task type="auto">
  <name>Task 2: Verify SSE contract unchanged</name>
  <files>None (verification only)</files>
  <action>
Verify the SSE event contract is preserved by checking:

1. **Event names unchanged:** text_delta, tool_executing, artifact_created, message_complete, error
2. **Event data format unchanged:**
   - text_delta: `{"text": string}`
   - tool_executing: `{"status": string}`
   - artifact_created: `{"id": string, "artifact_type": string, "title": string}`
   - message_complete: `{"content": string, "usage": {"input_tokens": int, "output_tokens": int}}`
   - error: `{"message": string}`
3. **conversations.py unchanged:** No modifications needed to route handler

Review `backend/app/routes/conversations.py` to confirm it still works with refactored AIService:
- Line ~120: `ai_service = AIService()` - unchanged signature
- Line ~128: `async for event in ai_service.stream_chat(...)` - unchanged method signature
- Event handling in event_generator() - unchanged

The route handler should work without modification because AIService's external interface is unchanged.
  </action>
  <verify>
```bash
cd backend && python -c "
# Verify route can import and use AIService
from app.routes.conversations import stream_chat, ChatRequest
from app.services.ai_service import AIService
print('Route imports work')

# Verify AIService signature
import inspect
sig = inspect.signature(AIService.__init__)
params = list(sig.parameters.keys())
print(f'AIService.__init__ params: {params}')

sig = inspect.signature(AIService.stream_chat)
params = list(sig.parameters.keys())
print(f'AIService.stream_chat params: {params}')
"
```
  </verify>
  <done>SSE event contract verified: event names and data formats unchanged, conversations.py requires no modifications</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Integration test (if API key available):**
Start backend and send test message to verify full flow works:
```bash
cd backend
# If running: curl test to /threads/{id}/chat endpoint
# Verify SSE events stream correctly
```

2. **Code review checklist:**
- [ ] No `import anthropic` in ai_service.py (moved to adapter)
- [ ] No `MODEL` constant in ai_service.py (managed by adapter)
- [ ] `LLMFactory.create` used in AIService.__init__
- [ ] StreamChunk types handled: text, tool_use, complete, error
- [ ] Tool loop (while True) preserved in AIService
- [ ] SSE event names unchanged: text_delta, tool_executing, artifact_created, message_complete, error
- [ ] SYSTEM_PROMPT, DOCUMENT_SEARCH_TOOL, SAVE_ARTIFACT_TOOL unchanged
- [ ] execute_tool() unchanged

3. **Grep verification:**
```bash
cd backend
# Should NOT find direct anthropic import in ai_service.py
grep -n "import anthropic" app/services/ai_service.py || echo "Good: no direct anthropic import"
# Should find LLMFactory import
grep -n "LLMFactory" app/services/ai_service.py
# Should find StreamChunk handling
grep -n "chunk_type" app/services/ai_service.py
```
</verification>

<success_criteria>
- AIService.__init__ uses LLMFactory.create() instead of direct anthropic client
- AIService.stream_chat() converts StreamChunk to SSE events
- Tool execution loop preserved in AIService (not moved to adapter)
- SSE event format identical to pre-refactor (no frontend changes needed)
- conversations.py requires no modifications
- All existing functionality works through adapter pattern
</success_criteria>

<output>
After completion, create `.planning/phases/19-backend-abstraction/19-02-SUMMARY.md`
</output>
