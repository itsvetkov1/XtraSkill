---
phase: 29-backend-service-tests
plan: 04
type: execute
wave: 2
depends_on: [29-01]
files_modified:
  - backend/tests/unit/services/test_ai_service.py
autonomous: true

must_haves:
  truths:
    - "AI service can be tested with MockLLMAdapter (no real API calls)"
    - "Tool execution creates artifacts in database"
    - "Document search tool returns formatted results"
    - "Streaming yields proper SSE event format"
    - "Error handling produces error events"
  artifacts:
    - path: "backend/tests/unit/services/test_ai_service.py"
      provides: "AIService tests with MockLLMAdapter"
      min_lines: 150
  key_links:
    - from: "test_ai_service.py"
      to: "app/services/ai_service.py"
      via: "direct import"
      pattern: "from app.services.ai_service import"
    - from: "test_ai_service.py"
      to: "tests/fixtures/llm_fixtures.py"
      via: "MockLLMAdapter fixture"
      pattern: "mock_llm_adapter|MockLLMAdapter"
---

<objective>
Create unit tests for ai_service module covering AIService class and stream_with_heartbeat function.

Purpose: Fulfill BSVC-06 requirement. Test AI service streaming, tool execution, and error handling using MockLLMAdapter from Phase 28. Verify that no real LLM API calls are made during tests.

Output: Test file with 80%+ coverage on ai_service.py, demonstrating MockLLMAdapter usage patterns.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/29-backend-service-tests/29-RESEARCH.md
@backend/app/services/ai_service.py
@backend/tests/fixtures/llm_fixtures.py
@backend/tests/fixtures/db_fixtures.py
@backend/tests/fixtures/factories.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ai_service execute_tool tests</name>
  <files>backend/tests/unit/services/test_ai_service.py</files>
  <action>
Create test file for ai_service with focus on tool execution:

```python
"""Unit tests for ai_service AIService."""

import pytest
import json
from unittest.mock import AsyncMock, patch
from app.services.ai_service import AIService, stream_with_heartbeat
from app.models import Thread, Project, Document, Artifact, ArtifactType
from tests.fixtures.llm_fixtures import MockLLMAdapter


class TestAIServiceExecuteTool:
    """Tests for AIService.execute_tool method."""

    @pytest.mark.asyncio
    async def test_save_artifact_creates_artifact(self, db_session, user):
        """save_artifact tool creates artifact in database."""
        db_session.add(user)
        await db_session.commit()

        thread = Thread(user_id=user.id, title="Test Thread")
        db_session.add(thread)
        await db_session.commit()

        service = AIService()

        result, event_data = await service.execute_tool(
            tool_name="save_artifact",
            tool_input={
                "artifact_type": "requirements_doc",
                "title": "Test BRD",
                "content_markdown": "# Business Requirements\n\nTest content."
            },
            project_id=None,
            thread_id=thread.id,
            db=db_session
        )

        # Verify artifact created
        assert "saved successfully" in result
        assert event_data is not None
        assert event_data["title"] == "Test BRD"
        assert event_data["artifact_type"] == "requirements_doc"

        # Verify persisted in DB
        from sqlalchemy import select
        stmt = select(Artifact).where(Artifact.thread_id == thread.id)
        db_result = await db_session.execute(stmt)
        artifact = db_result.scalar_one()
        assert artifact.title == "Test BRD"
        assert artifact.artifact_type == ArtifactType.REQUIREMENTS_DOC

    @pytest.mark.asyncio
    async def test_search_documents_returns_results(self, db_session, user):
        """search_documents tool returns formatted document results."""
        db_session.add(user)
        await db_session.commit()

        project = Project(user_id=user.id, name="Test Project")
        db_session.add(project)
        await db_session.commit()

        # Create and index a document
        doc = Document(
            project_id=project.id,
            filename="requirements.md",
            content_encrypted=b"encrypted"
        )
        db_session.add(doc)
        await db_session.commit()

        # Index document content
        from app.services.document_search import index_document
        await index_document(db_session, doc.id, doc.filename, "Login authentication requirements")
        await db_session.commit()

        service = AIService()

        result, event_data = await service.execute_tool(
            tool_name="search_documents",
            tool_input={"query": "authentication"},
            project_id=project.id,
            thread_id="thread-id",
            db=db_session
        )

        assert event_data is None  # No event for search
        assert "requirements.md" in result
        assert "authentication" in result.lower() or "**" in result  # Highlighted

    @pytest.mark.asyncio
    async def test_search_documents_empty_results(self, db_session, user):
        """search_documents returns message when no results."""
        db_session.add(user)
        await db_session.commit()

        project = Project(user_id=user.id, name="Empty Project")
        db_session.add(project)
        await db_session.commit()

        service = AIService()

        result, event_data = await service.execute_tool(
            tool_name="search_documents",
            tool_input={"query": "nonexistent"},
            project_id=project.id,
            thread_id="thread-id",
            db=db_session
        )

        assert "No relevant documents" in result
        assert event_data is None

    @pytest.mark.asyncio
    async def test_unknown_tool_returns_error(self, db_session):
        """Unknown tool name returns error message."""
        service = AIService()

        result, event_data = await service.execute_tool(
            tool_name="unknown_tool",
            tool_input={},
            project_id=None,
            thread_id="thread-id",
            db=db_session
        )

        assert "Unknown tool" in result
        assert event_data is None


class TestAIServiceStreamChat:
    """Tests for AIService.stream_chat method."""

    @pytest.mark.asyncio
    async def test_yields_text_events(self, db_session, user, mock_llm_adapter):
        """Streaming yields text_delta events."""
        db_session.add(user)
        await db_session.commit()

        thread = Thread(user_id=user.id, title="Test")
        db_session.add(thread)
        await db_session.commit()

        # Create service with mock adapter
        adapter = mock_llm_adapter(responses=["Hello", " world", "!"])
        service = AIService.__new__(AIService)
        service.adapter = adapter
        service.tools = []

        events = []
        async for event in service.stream_chat(
            messages=[{"role": "user", "content": "Hi"}],
            project_id=None,
            thread_id=thread.id,
            db=db_session
        ):
            events.append(event)

        # Should have text_delta events
        text_events = [e for e in events if e.get("event") == "text_delta"]
        assert len(text_events) == 3

        # Parse data from events
        texts = [json.loads(e["data"])["text"] for e in text_events]
        assert texts == ["Hello", " world", "!"]

    @pytest.mark.asyncio
    async def test_yields_message_complete(self, db_session, user, mock_llm_adapter):
        """Streaming ends with message_complete event."""
        db_session.add(user)
        await db_session.commit()

        thread = Thread(user_id=user.id, title="Test")
        db_session.add(thread)
        await db_session.commit()

        adapter = mock_llm_adapter(responses=["Response"])
        service = AIService.__new__(AIService)
        service.adapter = adapter
        service.tools = []

        events = []
        async for event in service.stream_chat(
            messages=[{"role": "user", "content": "Test"}],
            project_id=None,
            thread_id=thread.id,
            db=db_session
        ):
            events.append(event)

        # Last event should be message_complete
        assert events[-1]["event"] == "message_complete"
        data = json.loads(events[-1]["data"])
        assert "content" in data
        assert "usage" in data

    @pytest.mark.asyncio
    async def test_records_call_history(self, db_session, user, mock_llm_adapter):
        """MockLLMAdapter records call for verification."""
        db_session.add(user)
        await db_session.commit()

        thread = Thread(user_id=user.id, title="Test")
        db_session.add(thread)
        await db_session.commit()

        adapter = mock_llm_adapter(responses=["OK"])
        service = AIService.__new__(AIService)
        service.adapter = adapter
        service.tools = []

        messages = [{"role": "user", "content": "Test message"}]
        async for _ in service.stream_chat(messages, None, thread.id, db_session):
            pass

        # Verify call was recorded
        assert len(adapter.call_history) == 1
        assert adapter.call_history[0]["messages"] == messages

    @pytest.mark.asyncio
    async def test_handles_error_chunks(self, db_session, user, mock_llm_adapter):
        """Error chunks yield error events."""
        db_session.add(user)
        await db_session.commit()

        thread = Thread(user_id=user.id, title="Test")
        db_session.add(thread)
        await db_session.commit()

        adapter = mock_llm_adapter(raise_error="API rate limit exceeded")
        service = AIService.__new__(AIService)
        service.adapter = adapter
        service.tools = []

        events = []
        async for event in service.stream_chat(
            messages=[{"role": "user", "content": "Hi"}],
            project_id=None,
            thread_id=thread.id,
            db=db_session
        ):
            events.append(event)

        # Should have error event
        error_events = [e for e in events if e.get("event") == "error"]
        assert len(error_events) == 1
        data = json.loads(error_events[0]["data"])
        assert "rate limit" in data["message"]

    @pytest.mark.asyncio
    async def test_tool_execution_in_stream(self, db_session, user, mock_llm_adapter):
        """Tool calls are executed and results sent back."""
        db_session.add(user)
        await db_session.commit()

        thread = Thread(user_id=user.id, title="Test")
        db_session.add(thread)
        await db_session.commit()

        # First adapter call returns tool call, second returns text after tool result
        from app.services.llm.base import StreamChunk

        # Mock adapter that returns tool call then text
        first_call = True

        class ToolThenTextAdapter(MockLLMAdapter):
            async def stream_chat(self, messages, system_prompt, tools=None, max_tokens=4096):
                nonlocal first_call
                self.call_history.append({"messages": messages})

                if first_call:
                    first_call = False
                    yield StreamChunk(chunk_type="tool_use", tool_call={
                        "id": "tool_1",
                        "name": "save_artifact",
                        "input": {
                            "artifact_type": "user_stories",
                            "title": "User Stories",
                            "content_markdown": "# Stories"
                        }
                    })
                    yield StreamChunk(chunk_type="complete", usage={"input_tokens": 10, "output_tokens": 5})
                else:
                    yield StreamChunk(chunk_type="text", content="Artifact saved!")
                    yield StreamChunk(chunk_type="complete", usage={"input_tokens": 15, "output_tokens": 10})

        adapter = ToolThenTextAdapter()
        service = AIService.__new__(AIService)
        service.adapter = adapter
        service.tools = []

        events = []
        async for event in service.stream_chat(
            messages=[{"role": "user", "content": "Create stories"}],
            project_id=None,
            thread_id=thread.id,
            db=db_session
        ):
            events.append(event)

        # Should have tool_executing and artifact_created events
        event_types = [e.get("event") for e in events]
        assert "tool_executing" in event_types
        assert "artifact_created" in event_types


class TestStreamWithHeartbeat:
    """Tests for stream_with_heartbeat function."""

    @pytest.mark.asyncio
    async def test_passes_through_data_events(self):
        """Data events are yielded unchanged."""
        async def data_gen():
            yield {"event": "text_delta", "data": '{"text": "hello"}'}
            yield {"event": "message_complete", "data": "{}"}

        events = []
        async for event in stream_with_heartbeat(data_gen(), initial_delay=10):
            events.append(event)
            if len(events) >= 2:
                break

        assert events[0]["event"] == "text_delta"
        assert events[1]["event"] == "message_complete"

    @pytest.mark.asyncio
    async def test_yields_heartbeat_during_silence(self):
        """Heartbeat comments are yielded during long silences."""
        import asyncio

        async def slow_gen():
            yield {"event": "start", "data": "{}"}
            await asyncio.sleep(0.2)  # Longer than initial_delay
            yield {"event": "end", "data": "{}"}

        events = []
        async for event in stream_with_heartbeat(
            slow_gen(),
            initial_delay=0.05,  # Very short for testing
            heartbeat_interval=0.05
        ):
            events.append(event)

        # Should have heartbeat comments between start and end
        heartbeats = [e for e in events if e.get("comment") == "heartbeat"]
        assert len(heartbeats) >= 1


class TestAIServiceInit:
    """Tests for AIService initialization."""

    def test_creates_with_default_provider(self):
        """Default provider is anthropic."""
        service = AIService()
        assert service.adapter is not None
        assert len(service.tools) == 2  # search_documents, save_artifact

    def test_has_document_search_tool(self):
        """Has document search tool configured."""
        service = AIService()
        tool_names = [t["name"] for t in service.tools]
        assert "search_documents" in tool_names

    def test_has_save_artifact_tool(self):
        """Has save artifact tool configured."""
        service = AIService()
        tool_names = [t["name"] for t in service.tools]
        assert "save_artifact" in tool_names
```
  </action>
  <verify>
```bash
cd backend && python -m pytest tests/unit/services/test_ai_service.py -v
```
All tests pass.
  </verify>
  <done>17 tests pass for ai_service AIService</done>
</task>

<task type="auto">
  <name>Task 2: Verify ai_service coverage and no real API calls</name>
  <files>None (verification only)</files>
  <action>
Run coverage report for ai_service:

```bash
cd backend && python -m pytest tests/unit/services/test_ai_service.py \
  --cov=app.services.ai_service \
  --cov-report=term-missing \
  --cov-fail-under=75
```

Verify no real API calls:
- All tests use MockLLMAdapter
- No ANTHROPIC_API_KEY environment variable needed
- Tests pass without network access

Note: ai_service has more uncovered code paths (stream_with_heartbeat edge cases, exception handling) so 75% threshold is reasonable. The complex heartbeat timeout logic is tested via unit tests for the core paths.
  </action>
  <verify>
Coverage report shows 75%+ line coverage for ai_service.py
Tests run without requiring API keys or network access
  </verify>
  <done>ai_service achieves 75%+ test coverage with no real API calls</done>
</task>

</tasks>

<verification>
Run all ai_service tests:
```bash
cd backend && python -m pytest tests/unit/services/test_ai_service.py -v
```
All 17 tests should pass.

Verify coverage:
```bash
cd backend && python -m pytest tests/unit/services/test_ai_service.py --cov=app.services.ai_service --cov-report=term-missing
```
Should show 75%+ coverage.

Verify MockLLMAdapter used:
All tests should use mock_llm_adapter fixture or MockLLMAdapter directly. No real LLM calls.
</verification>

<success_criteria>
1. test_ai_service.py has 17 passing tests
2. Tool execution tested (save_artifact, search_documents)
3. Streaming text_delta events tested
4. Error handling tested
5. MockLLMAdapter call_history used for assertions
6. ai_service achieves 75%+ line coverage
7. No real API calls made (MockLLMAdapter only)
</success_criteria>

<output>
After completion, create `.planning/phases/29-backend-service-tests/29-04-SUMMARY.md`
</output>
