---
phase: 68-core-conversation-memory-fix
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/services/llm/claude_cli_adapter.py
  - backend/tests/unit/llm/test_claude_cli_adapter.py
  - backend/tests/integration/test_cli_conversation_memory.py
autonomous: true
requirements:
  - CONV-01
  - CONV-02
  - CONV-03
  - CONV-04
  - TEST-01
  - TEST-02
  - TEST-03
  - TEST-05

must_haves:
  truths:
    - "CLI adapter sends full conversation history to subprocess, not just the last message"
    - "Messages are formatted with Human:/Assistant: labels and --- separators"
    - "Multi-part content (tool_use, thinking, tool_result) is handled correctly"
    - "BA flow is completely unchanged (agent_service.py not touched)"
    - "Backend unit tests pass for 1, 3, and 10+ turn conversations"
    - "Integration test verifies fact recall across 3+ turns"
  artifacts:
    - path: "backend/app/services/llm/claude_cli_adapter.py"
      provides: "Multi-turn _convert_messages_to_prompt() and _extract_text_content() helper"
      contains: "_extract_text_content"
    - path: "backend/tests/unit/llm/test_claude_cli_adapter.py"
      provides: "Unit tests for message conversion with multi-turn, multi-part content"
      contains: "test_three_turn_conversation"
    - path: "backend/tests/integration/test_cli_conversation_memory.py"
      provides: "Integration test spawning real CLI subprocess for fact recall"
      contains: "test_assistant_remembers_fact_across_turns"
  key_links:
    - from: "backend/app/services/llm/claude_cli_adapter.py"
      to: "subprocess stdin"
      via: "_convert_messages_to_prompt iterates all messages"
      pattern: "for msg in messages"
    - from: "backend/tests/unit/llm/test_claude_cli_adapter.py"
      to: "backend/app/services/llm/claude_cli_adapter.py"
      via: "direct method call on _convert_messages_to_prompt"
      pattern: "adapter._convert_messages_to_prompt"
---

<objective>
Replace the broken `_convert_messages_to_prompt()` in `claude_cli_adapter.py` that only uses the last message with a multi-turn formatter that sends full conversation history to the CLI subprocess. Add comprehensive backend unit tests and one integration test.

Purpose: Fix the critical bug where Assistant conversations lose context after 2-3 messages.
Output: Working multi-turn CLI adapter with full test coverage.
</objective>

<execution_context>
@/Users/a1testingmac/.claude/get-shit-done/workflows/execute-plan.md
@/Users/a1testingmac/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/68-core-conversation-memory-fix/68-RESEARCH.md
@backend/app/services/llm/claude_cli_adapter.py
@backend/app/services/agent_service.py
@backend/tests/unit/llm/test_claude_cli_adapter.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Replace _convert_messages_to_prompt() with multi-turn formatter</name>
  <files>backend/app/services/llm/claude_cli_adapter.py</files>
  <action>
Replace `_convert_messages_to_prompt()` (lines 106-137) with the multi-turn implementation from the research document. This is a single-method replacement plus adding one new helper method.

1. Replace `_convert_messages_to_prompt()`:
   - Remove the POC implementation that uses `messages[-1]`
   - Iterate ALL messages in the list
   - Map `role == "user"` to `"Human"` label, `role == "assistant"` to `"Assistant"` label
   - Skip system messages and unknown roles (continue)
   - Call `self._extract_text_content(content)` to get text from string or list content
   - Skip messages where extracted text is empty after strip (handles tool_use-only assistant turns)
   - Join formatted parts with `"\n\n---\n\n"` separator
   - Add role alternation validation: log warning (not error) if consecutive same-role messages detected

2. Add `_extract_text_content()` helper method (new private method, placed right after `_convert_messages_to_prompt`):
   - If content is `str`: return directly
   - If content is `list`: iterate blocks:
     - `type == "text"`: append `part.get("text", "")`
     - `type == "thinking"`: skip (exclude internal reasoning per locked decision)
     - `type == "tool_use"`: if name contains "search_documents" → append `"[searched documents]"`, else → append `"[performed an action]"`
     - `type == "tool_result"`: skip (tool results can be very long, not conversation text)
   - Join text_parts with `"\n"`
   - If content is neither str nor list: return `str(content)`

3. Add a comment above the `combined_prompt` line (line 264) explaining that for multi-turn history, `prompt_text` now contains the full conversation with role labels. The outer `[USER]:` wrapper is kept for backward compatibility.

IMPORTANT: Do NOT modify `agent_service.py`. The BA flow is completely separate and unchanged.
IMPORTANT: Do NOT modify the `combined_prompt` assembly at line 264 — only add a clarifying comment.
  </action>
  <verify>
Run existing tests to confirm no regressions (expect 1 pre-existing failure in `test_stream_chat_passes_api_key_in_env` which is NOT a Phase 68 issue):
```bash
cd backend && ./venv/bin/python -m pytest tests/unit/llm/test_claude_cli_adapter.py -q
```
Note: `test_converts_string_content` will need updating in Task 2 since its assertion changes from exact match to labeled format.
  </verify>
  <done>
`_convert_messages_to_prompt()` iterates all messages with Human:/Assistant: labels and --- separators. `_extract_text_content()` handles multi-part content. System messages excluded, empty messages skipped, thinking blocks excluded, tool_use annotated.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add backend unit tests for message conversion</name>
  <files>backend/tests/unit/llm/test_claude_cli_adapter.py</files>
  <action>
Update existing tests and add new tests in `TestClaudeCLIAdapterMessageConversion` class.

1. Update existing tests affected by format change:
   - `test_converts_string_content`: Change assertion from `assert prompt == "Hello, how are you?"` to `assert prompt == "Human: Hello, how are you?"` (single message now has role label)
   - `test_converts_list_content`: Update assertions to check for `"Human:"` label prefix
   - `test_stream_chat_spawns_subprocess_with_correct_args` (line 608): Update `b"[USER]:"` assertion — the stdin now contains `b"Human:"` instead. The `b"[SYSTEM]:"` assertion stays unchanged.

2. Add new TEST-01 tests (multi-turn conversation):
   - `test_three_turn_conversation_has_all_messages`: 3 messages (user, assistant, user). Assert all three appear in output with correct labels. Assert `prompt.count("---") == 2`.
   - `test_ten_turn_conversation_preserves_all`: 10 messages (5 user/assistant pairs). Assert all message texts appear in output.
   - `test_uses_human_assistant_labels`: 2 messages. Assert `"Human:"` and `"Assistant:"` in prompt. Assert `"[USER]:"` NOT in prompt.
   - `test_uses_triple_dash_separator`: 2 messages. Assert `"---"` in prompt.
   - `test_single_message_no_separator`: 1 message. Assert `"---"` NOT in prompt.

3. Add new TEST-02 tests (multi-part content):
   - `test_tool_use_blocks_replaced_with_annotation`: Assistant message with text + tool_use(search_documents). Assert `"[searched documents]"` and text appear.
   - `test_generic_tool_use_annotation`: Assistant message with tool_use (non-search). Assert `"[performed an action]"`.
   - `test_tool_use_only_assistant_messages_skipped`: Assistant message with ONLY tool_use (no text block). Assert no `"Assistant:"` in prompt.
   - `test_thinking_blocks_excluded`: Assistant message with thinking + text. Assert text appears, thinking content does NOT.
   - `test_tool_result_user_messages_skipped`: User message with tool_result content list. Assert tool result content NOT in prompt.
   - `test_system_messages_excluded`: System message + user message. Assert system content NOT in prompt, user content appears.

4. Add TEST-03 test (BA flow regression):
   - `test_ba_flow_uses_agent_service_not_cli_adapter`: Import `AIService` and verify that the `_stream_agent_chat` method exists and the routing logic in `ai_service.py` routes based on `is_agent_provider`. This is a sanity check that the BA flow path hasn't changed. Can verify by checking `ClaudeCLIAdapter.is_agent_provider == True` and that the class doesn't touch `agent_service.py` patterns.

5. Add CONV-03 test (role alternation):
   - `test_warns_on_consecutive_same_role_messages`: Pass two consecutive user messages. Use `unittest.mock.patch` on the logger to verify a warning was logged. Assert both messages still appear in output (warning, not error).

All test methods should follow the existing pattern: `@patch('app.services.llm.claude_cli_adapter.shutil.which', return_value='/usr/bin/claude')` decorator.
  </action>
  <verify>
```bash
cd backend && ./venv/bin/python -m pytest tests/unit/llm/test_claude_cli_adapter.py -q -v 2>&1 | tail -30
```
All new tests pass. Pre-existing `test_stream_chat_passes_api_key_in_env` failure is expected and NOT a regression.
  </verify>
  <done>
13+ new/updated tests covering: multi-turn formatting (1, 3, 10+ messages), role labels, separators, tool_use annotation, thinking exclusion, tool_result skipping, system message exclusion, BA flow regression check, role alternation warning. All pass (1 pre-existing failure documented).
  </done>
</task>

<task type="auto">
  <name>Task 3: Create integration test for conversation memory</name>
  <files>backend/tests/integration/test_cli_conversation_memory.py</files>
  <action>
Create a new integration test file at `backend/tests/integration/test_cli_conversation_memory.py`.

1. Create the `backend/tests/integration/` directory if it doesn't exist (mkdir -p).

2. Create `backend/tests/integration/__init__.py` (empty file).

3. Create the test file with:
   - `@pytest.mark.integration` marker (for exclusion from regular CI)
   - `@pytest.mark.asyncio` marker
   - Import `ClaudeCLIAdapter` from `app.services.llm.claude_cli_adapter`
   - Import `shutil` to check if `claude` CLI is available
   - Add `@pytest.mark.skipif(shutil.which('claude') is None, reason="Claude CLI not installed")` to skip gracefully when CLI is not available

4. Test function `test_assistant_remembers_fact_across_turns`:
   - Purpose: Verify the CLI adapter preserves conversation context across 3 turns
   - Turn 1 message: `{"role": "user", "content": "I'm building a project management tool using FastAPI for the backend and React for the frontend."}` — casually mentions tech stack
   - Turn 1 expected assistant response: collect it
   - Turn 2 message: Add turn 1 exchange to history, add `{"role": "user", "content": "What are some good database options I should consider?"}` — general question
   - Turn 2 expected assistant response: collect it
   - Turn 3 message: Add turns 1-2 to history, add `{"role": "user", "content": "Which backend framework am I using for this project?"}` — asks about turn 1 fact WITHOUT repeating it
   - Assert: Turn 3 response contains "FastAPI" (case-insensitive)

   CRITICAL: Do NOT use "remember this" or "please remember" phrasing. Claude Code intercepts these and writes to memory.md, making the test pass for wrong reasons.

   Implementation approach:
   - Create the adapter: `adapter = ClaudeCLIAdapter(api_key="not-needed")` (CLI uses its own auth)
   - For each turn, build the full messages list and call `adapter._convert_messages_to_prompt(messages)` to verify format, then stream through `adapter.stream_chat()` collecting response text
   - Use `MagicMock()` for `set_context()` db parameter (not doing real DB ops)
   - System prompt: `"You are a helpful assistant."` (minimal, no BA context)
   - Collect streaming chunks, concatenate text deltas into response string
   - After turn 3, assert "fastapi" in response.lower()

5. Add a `conftest.py` in `backend/tests/integration/` with:
   ```python
   import pytest
   def pytest_configure(config):
       config.addinivalue_line("markers", "integration: marks tests as integration tests (deselect with '-m \"not integration\"')")
   ```
  </action>
  <verify>
First verify the test is properly structured and can be collected (without running it to avoid API cost):
```bash
cd backend && ./venv/bin/python -m pytest tests/integration/test_cli_conversation_memory.py --collect-only
```
If Claude CLI is available, optionally run:
```bash
cd backend && ./venv/bin/python -m pytest tests/integration/test_cli_conversation_memory.py -v -m integration
```
  </verify>
  <done>
Integration test file exists with one test that verifies fact recall across 3 turns using casual phrasing. Test is properly marked with `@pytest.mark.integration` for CI exclusion and `@pytest.mark.skipif` for environments without Claude CLI. Pre-existing test failure in unit suite is documented as not related to Phase 68.
  </done>
</task>

</tasks>

<verification>
1. Backend unit tests pass (excluding pre-existing `test_stream_chat_passes_api_key_in_env` failure):
   ```bash
   cd backend && ./venv/bin/python -m pytest tests/unit/llm/test_claude_cli_adapter.py -q
   ```
   Expected: 45+ pass, 1 fail (pre-existing)

2. Integration test collects without errors:
   ```bash
   cd backend && ./venv/bin/python -m pytest tests/integration/test_cli_conversation_memory.py --collect-only
   ```

3. `agent_service.py` is NOT modified:
   ```bash
   git diff backend/app/services/agent_service.py
   ```
   Expected: no output (no changes)

4. `_convert_messages_to_prompt` no longer uses `messages[-1]`:
   ```bash
   grep -n "messages\[-1\]" backend/app/services/llm/claude_cli_adapter.py
   ```
   Expected: no output
</verification>

<success_criteria>
- `_convert_messages_to_prompt()` iterates all messages with Human:/Assistant: labels
- `_extract_text_content()` handles string, list (text/thinking/tool_use/tool_result), and fallback content
- 13+ new/updated unit tests pass covering multi-turn, multi-part, role alternation
- 1 integration test exists for 3-turn fact recall
- BA flow unchanged (agent_service.py untouched)
- No regressions in existing test suite (1 pre-existing failure documented)
</success_criteria>

<output>
After completion, create `.planning/phases/68-core-conversation-memory-fix/68-01-SUMMARY.md`
</output>
