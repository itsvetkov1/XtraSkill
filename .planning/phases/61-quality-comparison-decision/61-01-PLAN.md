---
phase: 61-quality-comparison-decision
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/evaluation/__init__.py
  - backend/evaluation/test_scenarios.json
  - backend/evaluation/quality_rubric.py
  - backend/evaluation/generate_samples.py
autonomous: true

must_haves:
  truths:
    - "5 standardized test scenarios exist with varying complexity levels covering real BA use cases"
    - "Quality rubric defines 4 scoring dimensions (completeness, AC quality, consistency, error coverage) with 1-4 scale and per-level criteria"
    - "Generation script can create BRDs for all 3 providers using existing AIService infrastructure"
  artifacts:
    - path: "backend/evaluation/__init__.py"
      provides: "Package marker for evaluation module"
    - path: "backend/evaluation/test_scenarios.json"
      provides: "5 standardized test prompts with complexity levels and expected sections"
      contains: "test_scenarios"
    - path: "backend/evaluation/quality_rubric.py"
      provides: "Multi-dimensional scoring framework with 4 dimensions and 1-4 scale"
      contains: "class ScoreLevel"
    - path: "backend/evaluation/generate_samples.py"
      provides: "BRD generation orchestrator for 3 providers x 5 scenarios"
      contains: "async def generate_all_samples"
  key_links:
    - from: "backend/evaluation/generate_samples.py"
      to: "backend/app/services/ai_service.py"
      via: "AIService import and stream_chat usage"
      pattern: "from app\\.services\\.ai_service import AIService"
---

<objective>
Build the evaluation framework: standardized test scenarios, quality rubric, and BRD generation orchestrator.

Purpose: Create the infrastructure needed to run a controlled quality comparison across three LLM providers (anthropic, claude-code-sdk, claude-code-cli). This is the foundation for all subsequent evaluation work.

Output: Test scenarios JSON, quality rubric module, generation orchestrator script.
</objective>

<execution_context>
@C:/Users/ibcve/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ibcve/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/61-quality-comparison-decision/61-RESEARCH.md
@backend/app/services/ai_service.py
@backend/app/services/llm/base.py
@backend/app/services/llm/factory.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create test scenarios and quality rubric</name>
  <files>
    backend/evaluation/__init__.py
    backend/evaluation/test_scenarios.json
    backend/evaluation/quality_rubric.py
  </files>
  <action>
    Create the evaluation package and core definitions.

    1. **backend/evaluation/__init__.py** — Empty package marker.

    2. **backend/evaluation/test_scenarios.json** — Create 5 standardized test scenarios that represent real-world BA use cases with varying complexity. Each scenario has:
       - `id`: TC-01 through TC-05
       - `name`: Descriptive scenario name
       - `complexity`: "low", "medium", or "high"
       - `initial_prompt`: The initial user message that starts the conversation
       - `follow_ups`: Array of 1-4 follow-up user messages that simulate multi-turn discovery
       - `final_prompt`: The message that triggers BRD generation (e.g., "Please generate a comprehensive Business Requirements Document based on our discussion.")
       - `expected_sections`: Array of BRD sections expected (e.g., ["executive_summary", "business_objectives", "user_personas", "user_flows", "functional_requirements", "acceptance_criteria", "success_metrics"])
       - `minimum_ac_count`: Minimum number of acceptance criteria expected

       Scenario distribution per research guidance (prevent bias):
       - TC-01: Simple feature (low) — User login with email/password for a mobile app
       - TC-02: Medium workflow (medium) — Employee expense report submission and approval
       - TC-03: Complex multi-stakeholder (high) — E-commerce marketplace with sellers, buyers, and admin
       - TC-04: Integration scenario (medium) — CRM integration with existing email and calendar systems
       - TC-05: Regulatory domain (high) — Healthcare patient portal with HIPAA compliance requirements

    3. **backend/evaluation/quality_rubric.py** — Define the multi-dimensional quality rubric using Python dataclasses:
       - `ScoreLevel` enum: POOR=1, FAIR=2, GOOD=3, EXCELLENT=4
       - `QualityDimension` dataclass: name, description, criteria (dict mapping ScoreLevel to description string)
       - `RUBRIC` dict with 4 dimensions:
         - `completeness`: All required BRD sections present with sufficient detail (per BRD structure in SYSTEM_PROMPT)
         - `ac_quality`: Acceptance criteria are specific, measurable, testable, with explicit actors and thresholds
         - `consistency`: No contradictions, unified terminology, coherent narrative across sections
         - `error_coverage`: Document addresses error handling, edge cases, unhappy paths, and recovery flows
       - Each dimension has 4 scoring level descriptions (POOR through EXCELLENT)
       - Include a `score_summary()` function that takes a dict of dimension->score and returns aggregate stats (mean, min, max)
       - Include a `RUBRIC_DESCRIPTION` string constant that formats the rubric as readable markdown for the human reviewer

    Follow the exact pattern from the research document for ScoreLevel and QualityDimension definitions.
  </action>
  <verify>
    - `python -c "import json; d = json.load(open('backend/evaluation/test_scenarios.json')); assert len(d['test_scenarios']) == 5; print('OK: 5 scenarios')"` passes
    - `python -c "from backend.evaluation.quality_rubric import RUBRIC, ScoreLevel; assert len(RUBRIC) == 4; print('OK: 4 dimensions')"` passes (run from project root with PYTHONPATH=backend)
    - Each scenario has id, name, complexity, initial_prompt, follow_ups, final_prompt, expected_sections, minimum_ac_count fields
    - Each rubric dimension has criteria for all 4 score levels
  </verify>
  <done>
    5 test scenarios exist in JSON with low/medium/high complexity distribution. Quality rubric defines 4 dimensions with 4-level scoring criteria. Both are importable without errors.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create BRD generation orchestrator script</name>
  <files>
    backend/evaluation/generate_samples.py
  </files>
  <action>
    Create the generation orchestrator that produces BRDs across all 3 providers.

    **backend/evaluation/generate_samples.py** — Standalone async script that:

    1. **Imports**: Uses `AIService` from `app.services.ai_service`, `json`, `asyncio`, `time`, `pathlib`, `hashlib`, `random`, `os`, `sys`. Adds backend to sys.path so it can import app modules.

    2. **Database setup**: Creates an async database session using the same pattern as the app (import `get_db` or create engine directly from `app.database`). The script needs a real database connection to support tool execution (search_documents, save_artifact).

    3. **`generate_brd_sample(provider, scenario, db)` async function**:
       - Creates `AIService(provider=provider)`
       - Builds conversation messages from the scenario: starts with `initial_prompt`, adds follow-ups with placeholder assistant responses between them (simulating a multi-turn conversation), ends with `final_prompt`
       - For each message exchange, calls `ai_service.stream_chat()` and accumulates text_delta events into the response
       - Captures the FINAL response (after final_prompt) as the BRD content
       - Records metadata: provider, scenario_id, token_usage (from message_complete event), generation_time (wall clock)
       - Returns dict with `content` (full BRD text), `metadata` (tokens, time, cost estimate)

       IMPORTANT: The conversation simulation approach:
       - Send initial_prompt, collect AI response (this is the first discovery question)
       - For each follow_up: append prior AI response as assistant message, append follow_up as user message, collect new AI response
       - After all follow_ups: append last AI response, append final_prompt, collect the BRD
       - This creates a realistic multi-turn conversation that exercises the full provider pipeline

       NOTE: Each provider handles conversations differently:
       - `anthropic` uses manual tool loop in AIService
       - `claude-code-sdk` routes through agent handler with MCP tools
       - `claude-code-cli` spawns subprocess with MCP tools
       All three use the same `stream_chat()` entry point and yield SSE events.

    4. **`calculate_cost(usage, provider)` function**:
       - Estimates USD cost based on token usage and provider
       - Claude Sonnet pricing: $3/M input, $15/M output tokens (approximate)
       - Agent providers add ~30-50% overhead per research findings
       - Returns float USD estimate

    5. **`generate_all_samples()` async function**:
       - Loads scenarios from `test_scenarios.json`
       - Creates output directories: `backend/evaluation_data/{provider}/` and `backend/evaluation_data/metadata/`
       - For each provider in ["anthropic", "claude-code-sdk", "claude-code-cli"]:
         - For each scenario:
           - Calls `generate_brd_sample(provider, scenario, db)`
           - Saves BRD content to `evaluation_data/{provider}/{scenario_id}.md`
           - Saves metadata to `evaluation_data/metadata/{provider}_{scenario_id}.json`
           - Prints progress: "[provider] TC-XX: Done (input_tokens/output_tokens, $X.XX, Xs)"
       - Prints summary: total BRDs generated, total cost, total time

    6. **`anonymize_for_blind_review()` function**:
       - Reads all generated BRDs from evaluation_data/{provider}/
       - Creates `evaluation_data/blind_review/` directory
       - For each BRD: generates random 8-char hex review_id, copies content to `blind_review/{review_id}.md`
       - Creates `evaluation_data/metadata/review_id_mapping.json` mapping review_id -> {provider, scenario_id}
       - Shuffles the mapping randomly (so file listing order doesn't reveal providers)
       - Creates `evaluation_data/blind_review/scoring_template.csv` with columns: review_id, completeness, ac_quality, consistency, error_coverage, notes (all score columns empty for human to fill)

    7. **`if __name__ == "__main__":`** — Parses simple CLI args:
       - `python generate_samples.py generate` — runs generate_all_samples()
       - `python generate_samples.py anonymize` — runs anonymize_for_blind_review()
       - `python generate_samples.py all` — runs both in sequence
       - Handles KeyboardInterrupt gracefully

    **Critical implementation details:**
    - The script must set up a proper async database session. Use the existing `app.database` module.
    - For each provider, create a FRESH AIService instance (providers may have internal state).
    - Use a real project_id and thread_id — create temporary evaluation entries in DB, or use existing test data. If no project exists, the script should create a temporary evaluation project with the test scenario documents (or skip document search by using a project with no documents).
    - Set PYTHONPATH or sys.path to include `backend/` so app imports resolve.
    - The script should be runnable standalone: `cd backend && python -m evaluation.generate_samples all`
  </action>
  <verify>
    - `cd backend && python -c "from evaluation.generate_samples import generate_brd_sample, anonymize_for_blind_review; print('Import OK')"` succeeds
    - Script has proper CLI argument parsing (generate, anonymize, all)
    - generate_brd_sample function signature matches (provider, scenario, db) pattern
    - anonymize_for_blind_review creates review_id_mapping.json and scoring_template.csv
    - Script imports AIService correctly from app.services.ai_service
  </verify>
  <done>
    Generation orchestrator script importable, has proper CLI interface, and uses existing AIService infrastructure to generate BRDs across all 3 providers. Anonymization function creates blind review materials with shuffled review IDs.
  </done>
</task>

</tasks>

<verification>
1. `backend/evaluation/` directory exists with __init__.py, test_scenarios.json, quality_rubric.py, generate_samples.py
2. Test scenarios JSON is valid and contains 5 scenarios with required fields
3. Quality rubric defines 4 dimensions with 4-level scoring
4. Generation script is importable and uses AIService pattern from codebase
5. All files follow existing codebase conventions (docstrings, type hints, async patterns)
</verification>

<success_criteria>
- 5 standardized test scenarios with low/medium/high complexity distribution
- 4-dimension quality rubric with clear scoring criteria
- Generation orchestrator script that can produce BRDs from all 3 providers
- Blind review anonymization function for unbiased human scoring
- All code importable without runtime errors
</success_criteria>

<output>
After completion, create `.planning/phases/61-quality-comparison-decision/61-01-SUMMARY.md`
</output>
