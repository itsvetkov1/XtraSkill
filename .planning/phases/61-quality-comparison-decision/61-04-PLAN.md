---
phase: 61-quality-comparison-decision
plan: 04
type: execute
wave: 4
depends_on: ["61-03"]
files_modified:
  - backend/evaluation_results/comparison_report.md
  - backend/evaluation_results/statistics.json
  - backend/evaluation_results/scores.csv
autonomous: true

must_haves:
  truths:
    - "Statistical analysis complete with per-dimension means, improvement percentages, and p-values"
    - "Comparison report exists with quality scores, cost analysis, decision matrix, and clear recommendation"
    - "Go/no-go recommendation made: adopt SDK, adopt CLI, stay with direct API, or larger study needed"
  artifacts:
    - path: "backend/evaluation_results/comparison_report.md"
      provides: "Final comparison report with recommendation"
      contains: "Recommendation"
    - path: "backend/evaluation_results/statistics.json"
      provides: "Raw statistical analysis data"
      contains: "sdk_improvement_pct"
    - path: "backend/evaluation_results/scores.csv"
      provides: "Completed scores with provider attribution"
  key_links:
    - from: "backend/evaluation/generate_report.py"
      to: "backend/evaluation_results/comparison_report.md"
      via: "Report generator produces final report"
      pattern: "comparison_report"
---

<objective>
Run statistical analysis on scored BRDs and generate the final comparison report with go/no-go recommendation.

Purpose: Complete the quality evaluation by analyzing human scores, computing statistical significance, performing cost-quality tradeoff analysis, and producing the final recommendation that determines the fate of the Claude Code backend experiment.

Output: Comparison report (QUAL-05), statistics JSON, scored data CSV.
</objective>

<execution_context>
@C:/Users/ibcve/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ibcve/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/61-quality-comparison-decision/61-RESEARCH.md
@.planning/phases/61-quality-comparison-decision/61-01-SUMMARY.md
@.planning/phases/61-quality-comparison-decision/61-02-SUMMARY.md
@.planning/phases/61-quality-comparison-decision/61-03-SUMMARY.md
@backend/evaluation/analyze_results.py
@backend/evaluation/generate_report.py
@backend/evaluation_data/blind_review/scoring_template.csv
</context>

<tasks>

<task type="auto">
  <name>Task 1: Run statistical analysis and generate comparison report</name>
  <files>
    backend/evaluation_results/comparison_report.md
    backend/evaluation_results/statistics.json
    backend/evaluation_results/scores.csv
  </files>
  <action>
    Execute the analysis pipeline and generate the final comparison report.

    **Step 1: Validate scoring data**
    ```bash
    cd backend
    python -c "
    from evaluation.analyze_results import load_scores
    df = load_scores()
    print(f'Loaded {len(df)} scored samples')
    print(f'Providers: {df.provider.unique().tolist()}')
    print(f'Dimensions scored: {[c for c in df.columns if c in [\"completeness\", \"ac_quality\", \"consistency\", \"error_coverage\"]]}')
    print(df.groupby(\"provider\").mean(numeric_only=True))
    "
    ```

    **Step 2: Run full analysis**
    ```bash
    cd backend
    python -m evaluation.analyze_results
    ```
    This produces `evaluation_results/statistics.json` with all computed metrics.

    **Step 3: Generate comparison report**
    ```bash
    cd backend
    python -m evaluation.generate_report
    ```
    This produces `evaluation_results/comparison_report.md` with the full report.

    **Step 4: Save attributed scores**
    Copy the scoring CSV with provider attribution to `evaluation_results/scores.csv` for archival. This is the scored data with provider identity revealed (post-blind-review).

    **Step 5: Verify report contents**
    - Report must contain: Executive Summary, Methodology, Quality Scores table, Cost Analysis, Cost-Quality Tradeoff, Decision Matrix, Limitations, Recommendation, Next Steps
    - Recommendation must be one of: "ADOPT SDK", "ADOPT CLI", "STAY WITH DIRECT API", "LARGER STUDY RECOMMENDED"
    - All tables must have actual numbers (no template placeholders remaining)
    - Limitations section acknowledges small sample size

    **Step 6: Print key findings to stdout**
    Print a summary of the recommendation for the user:
    ```
    === PHASE 61 RESULTS ===
    Recommendation: [recommendation]
    Confidence: [confidence]

    Quality Improvement:
    - SDK: [X.X%] average improvement ([N/4] dimensions significant)
    - CLI: [X.X%] average improvement ([N/4] dimensions significant)

    Cost Impact:
    - SDK: [+X.X%] cost increase
    - CLI: [+X.X%] cost increase

    Full report: backend/evaluation_results/comparison_report.md
    ```
  </action>
  <verify>
    - `backend/evaluation_results/comparison_report.md` exists and is non-empty
    - `backend/evaluation_results/statistics.json` exists and contains "sdk_improvement_pct" key
    - `backend/evaluation_results/scores.csv` exists with 15 rows of scored data
    - Report contains "Recommendation:" section with one of the 4 valid options
    - Statistics JSON contains p-values and significance flags
    - No Jinja2 template placeholders remaining in the report (no {{ or {% blocks)
  </verify>
  <done>
    Comparison report generated with statistical analysis, cost-quality tradeoff, decision matrix, and clear go/no-go recommendation. Phase 61 evaluation complete.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update project state and roadmap with evaluation results</name>
  <files>
    .planning/STATE.md
    .planning/ROADMAP.md
  </files>
  <action>
    Update project documentation to reflect Phase 61 completion and the evaluation outcome.

    **1. Update STATE.md:**
    - Update "Current Position" to reflect Phase 61 complete
    - Update progress bar to 100% for v0.1-claude-code
    - Add evaluation result to "Accumulated Context > Decisions":
      - `Phase 61: Quality evaluation result — [RECOMMENDATION] ([CONFIDENCE] confidence)`
      - Brief rationale from the report
    - Update "Session Continuity" with next action based on recommendation:
      - If ADOPT: "Merge feature/claude-code-backend to master, begin production hardening"
      - If STAY: "Archive experiment branch, focus on direct API prompt improvements"
      - If LARGER STUDY: "Design expanded evaluation with n=30"
    - Update Claude Code milestone plan count in metrics table

    **2. Update ROADMAP.md:**
    - Update Phase 61 entry:
      - Check the checkbox: `[x] **Phase 61: Quality Comparison & Decision**`
      - Update plans: `**Plans:** 4 plans`
      - Add plan list with completion status
    - Update progress table: Phase 61 status = Complete, plans = 4/4
    - Update milestone status based on recommendation
    - Add completion date

    **3. Read the comparison report** to extract the recommendation, confidence level, and key statistics for the state/roadmap updates.
  </action>
  <verify>
    - STATE.md reflects Phase 61 complete
    - ROADMAP.md shows Phase 61 with 4 completed plans
    - Recommendation is recorded in accumulated context decisions
    - Next action is clear based on evaluation outcome
  </verify>
  <done>
    Project state and roadmap updated with Phase 61 completion, evaluation results, and clear next steps based on the go/no-go recommendation.
  </done>
</task>

</tasks>

<verification>
1. Comparison report exists at `backend/evaluation_results/comparison_report.md`
2. Report contains all required sections with actual data (no placeholders)
3. Statistics JSON has complete analysis data
4. Recommendation is clear and actionable
5. Project documentation (STATE.md, ROADMAP.md) updated
6. All QUAL requirements satisfied:
   - QUAL-01: 5+ baseline BRDs (from Plan 03)
   - QUAL-02: 5+ SDK BRDs (from Plan 03)
   - QUAL-03: 5+ CLI BRDs (from Plan 03)
   - QUAL-04: Quality metrics defined and scored (rubric from Plan 01, scoring from Plan 03)
   - QUAL-05: Comparison report with recommendation (this plan)
</verification>

<success_criteria>
- Comparison report written with clear go/no-go recommendation (QUAL-05)
- Statistical significance tested across all dimensions
- Cost-quality tradeoff analyzed
- Project state and roadmap updated with evaluation outcome
- Phase 61 complete — v0.1-claude-code milestone finished
</success_criteria>

<output>
After completion, create `.planning/phases/61-quality-comparison-decision/61-04-SUMMARY.md`
</output>
