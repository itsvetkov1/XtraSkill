---
phase: 54-backend-foundation-document-parsing-security
plan: 02
type: execute
wave: 2
depends_on: ["54-01"]
files_modified:
  - backend/app/models.py
  - backend/app/database.py
  - backend/app/services/encryption.py
autonomous: true

must_haves:
  truths:
    - "Document model has content_type, content_text, and metadata_json columns"
    - "Existing documents get content_type='text/plain' via migration default"
    - "New columns are nullable so existing documents don't break"
    - "Encryption service can encrypt raw bytes (for binary files) in addition to text strings"
    - "Encryption service can decrypt to raw bytes (for binary file download)"
    - "FTS5 table uses unicode61 tokenizer for international character support"
    - "Existing FTS5 entries are re-indexed after tokenizer change"
  artifacts:
    - path: "backend/app/models.py"
      provides: "Document model with content_type, content_text, metadata_json columns"
      contains: "content_type"
    - path: "backend/app/database.py"
      provides: "Migration logic for new Document columns and FTS5 tokenizer upgrade"
      contains: "content_text"
    - path: "backend/app/services/encryption.py"
      provides: "Binary encryption/decryption methods alongside existing text methods"
      contains: "encrypt_binary"
  key_links:
    - from: "backend/app/models.py"
      to: "backend/app/database.py"
      via: "SQLAlchemy model columns matched by migration ALTER TABLE statements"
      pattern: "content_type|content_text|metadata_json"
    - from: "backend/app/services/encryption.py"
      to: "backend/app/routes/documents.py"
      via: "encrypt_binary/decrypt_binary used for rich document storage (consumed by Plan 54-03)"
      pattern: "encrypt_binary|decrypt_binary"
---

<objective>
Extend the database schema and encryption service to support rich document storage.

Purpose: Add the dual-column storage pattern (encrypted binary + extracted text) to the Document model, migrate existing databases, upgrade FTS5 tokenizer for international text support, and add binary encryption methods. These schema changes are prerequisites for the route integration in Plan 54-03.

Output: Updated Document model with 3 new columns, database migration handling new and existing databases, encryption service with binary file support, FTS5 upgraded to unicode61 tokenizer.
</objective>

<execution_context>
@/Users/a1testingmac/.claude/get-shit-done/workflows/execute-plan.md
@/Users/a1testingmac/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/54-backend-foundation-document-parsing-security/54-RESEARCH.md
@.planning/phases/54-backend-foundation-document-parsing-security/54-01-SUMMARY.md

# Existing files being modified
@backend/app/models.py
@backend/app/database.py
@backend/app/services/encryption.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend Document model with new columns and add binary encryption methods</name>
  <files>
    backend/app/models.py
    backend/app/services/encryption.py
  </files>
  <action>
**1. Update Document model in `backend/app/models.py`:**

Add three new columns to the Document class (after the existing `content_encrypted` field):

```python
# Content type for format routing (e.g., "application/pdf", "text/plain")
content_type: Mapped[Optional[str]] = mapped_column(
    String(100), nullable=True, default="text/plain"
)

# Extracted plaintext content (for FTS5 indexing and AI context)
# NULL for legacy documents that haven't been backfilled
content_text: Mapped[Optional[str]] = mapped_column(Text, nullable=True)

# Format-specific metadata as JSON string
# e.g., {"sheet_names": [...], "page_count": 5, "row_count": 100}
metadata_json: Mapped[Optional[str]] = mapped_column(Text, nullable=True)
```

Add `Optional` to the typing imports if not already present. Add `Text` to the SQLAlchemy imports if not already present.

Update the docstring to reflect the new capabilities: replace "Text-only files (.txt, .md) in MVP; max 1MB." with "Supports text files (.txt, .md) and rich documents (.xlsx, .csv, .pdf, .docx); max 10MB."

**2. Update EncryptionService in `backend/app/services/encryption.py`:**

Add two new methods for binary file encryption/decryption (in addition to the existing text methods which must remain unchanged for backward compatibility):

```python
def encrypt_binary(self, data: bytes) -> bytes:
    """
    Encrypt binary file content (for rich documents like PDF, XLSX, DOCX).

    Args:
        data: Raw file bytes

    Returns:
        Encrypted content as bytes
    """
    return self.fernet.encrypt(data)

def decrypt_binary(self, ciphertext: bytes) -> bytes:
    """
    Decrypt binary file content.

    Args:
        ciphertext: Encrypted content as bytes

    Returns:
        Original raw file bytes
    """
    return self.fernet.decrypt(ciphertext)
```

IMPORTANT: Keep the existing `encrypt_document(plaintext: str)` and `decrypt_document(ciphertext: bytes)` methods unchanged — they handle legacy text document encryption and are used by the existing get_document endpoint. The new `encrypt_binary` / `decrypt_binary` methods handle raw bytes without UTF-8 encoding/decoding.
  </action>
  <verify>
```bash
cd backend && source venv/bin/activate
python -c "
from app.models import Document
# Check new columns exist on the model
print('content_type' in Document.__table__.columns)
print('content_text' in Document.__table__.columns)
print('metadata_json' in Document.__table__.columns)

# Check encryption binary methods
from app.services.encryption import get_encryption_service
enc = get_encryption_service()
test_data = b'\x00\x01\x02\xff binary data'
encrypted = enc.encrypt_binary(test_data)
decrypted = enc.decrypt_binary(encrypted)
assert decrypted == test_data, 'Binary encryption roundtrip failed'
print('Binary encryption roundtrip: OK')

# Verify existing text methods still work
text_enc = enc.encrypt_document('hello text')
text_dec = enc.decrypt_document(text_enc)
assert text_dec == 'hello text', 'Text encryption roundtrip failed'
print('Text encryption roundtrip: OK (backward compatible)')
"
```
  </verify>
  <done>
    - Document model has content_type (nullable, default text/plain), content_text (nullable), metadata_json (nullable)
    - Encryption service has encrypt_binary/decrypt_binary for raw bytes alongside existing text methods
    - Existing encrypt_document/decrypt_document methods unchanged (backward compatible)
    - All three new model columns are nullable to avoid breaking existing documents
  </done>
</task>

<task type="auto">
  <name>Task 2: Database migration for new columns and FTS5 unicode61 tokenizer upgrade</name>
  <files>
    backend/app/database.py
  </files>
  <action>
Update `_run_migrations()` in `backend/app/database.py` to handle the new Document columns and FTS5 tokenizer upgrade.

**1. Add Document column migrations** — Add AFTER the existing thread migrations (before the FTS5 section):

```python
# --- Document schema migration for rich document support ---
result = await conn.execute(text("PRAGMA table_info(documents)"))
doc_columns = [row[1] for row in result]

if "content_type" not in doc_columns:
    await conn.execute(text(
        "ALTER TABLE documents ADD COLUMN content_type VARCHAR(100) DEFAULT 'text/plain'"
    ))

if "content_text" not in doc_columns:
    await conn.execute(text(
        "ALTER TABLE documents ADD COLUMN content_text TEXT"
    ))

if "metadata_json" not in doc_columns:
    await conn.execute(text(
        "ALTER TABLE documents ADD COLUMN metadata_json TEXT"
    ))
```

**2. Replace the FTS5 creation section** with tokenizer upgrade logic:

The current code creates FTS5 with `tokenize = 'porter ascii'` only if the table doesn't exist. We need to:
1. Check if document_fts exists
2. If it exists, check if the tokenizer is already unicode61
3. If ascii tokenizer, drop and recreate with unicode61 (re-indexing needed)
4. If not exists, create with unicode61 from the start

```python
# Ensure FTS5 virtual table exists with unicode61 tokenizer for international text
result = await conn.execute(
    text("SELECT sql FROM sqlite_master WHERE type='table' AND name='document_fts'")
)
fts_sql = result.scalar()

if fts_sql is None:
    # FTS5 doesn't exist — create with unicode61 tokenizer
    await conn.execute(text("""
        CREATE VIRTUAL TABLE document_fts USING fts5(
            document_id UNINDEXED,
            filename,
            content,
            tokenize = 'porter unicode61'
        )
    """))
elif 'unicode61' not in (fts_sql or ''):
    # FTS5 exists but uses old ascii tokenizer — upgrade to unicode61
    # Must drop and recreate (SQLite FTS5 doesn't support ALTER)
    await conn.execute(text("DROP TABLE document_fts"))
    await conn.execute(text("""
        CREATE VIRTUAL TABLE document_fts USING fts5(
            document_id UNINDEXED,
            filename,
            content,
            tokenize = 'porter unicode61'
        )
    """))
    # Re-index existing documents
    # For existing text documents, content_text may be NULL (not yet backfilled)
    # Re-index from content_text where available, otherwise we can't re-index
    # (legacy encrypted content can't be decrypted in migration context)
    # This means old documents lose FTS until accessed and backfilled
    await conn.execute(text("""
        INSERT INTO document_fts(document_id, filename, content)
        SELECT d.id, d.filename, d.content_text
        FROM documents d
        WHERE d.content_text IS NOT NULL
    """))
```

NOTE: Old text documents that were indexed in the original FTS5 will lose their search index after the tokenizer upgrade until they are re-indexed. This is acceptable because:
- The migration cannot decrypt content_encrypted (no access to Fernet key at SQL level)
- Plan 54-03 will handle the get_document endpoint which can lazily backfill content_text
- New uploads will have content_text populated from the start
  </action>
  <verify>
```bash
cd backend && source venv/bin/activate
# Test database initialization (creates tables + runs migrations)
python -c "
import asyncio
from app.database import init_db, engine
from sqlalchemy import text

async def test():
    await init_db()

    async with engine.begin() as conn:
        # Check Document columns
        result = await conn.execute(text('PRAGMA table_info(documents)'))
        cols = [row[1] for row in result]
        assert 'content_type' in cols, 'content_type column missing'
        assert 'content_text' in cols, 'content_text column missing'
        assert 'metadata_json' in cols, 'metadata_json column missing'
        print(f'Document columns: {cols}')

        # Check FTS5 tokenizer
        result = await conn.execute(
            text(\"SELECT sql FROM sqlite_master WHERE type='table' AND name='document_fts'\")
        )
        fts_sql = result.scalar()
        assert 'unicode61' in fts_sql, f'FTS5 not using unicode61: {fts_sql}'
        print(f'FTS5 SQL: {fts_sql}')

    print('All migration checks passed')

asyncio.run(test())
"
```
  </verify>
  <done>
    - Database migration adds content_type, content_text, metadata_json to documents table
    - Existing documents get content_type default 'text/plain' via ALTER TABLE DEFAULT
    - FTS5 tokenizer upgraded from 'porter ascii' to 'porter unicode61' for international text
    - Migration is idempotent (safe to run multiple times)
    - Existing FTS entries re-indexed from content_text where available
  </done>
</task>

</tasks>

<verification>
1. Document model has content_type, content_text, metadata_json columns (check with PRAGMA table_info)
2. Migration is idempotent — running init_db() twice doesn't error
3. FTS5 uses unicode61 tokenizer (check sqlite_master SQL)
4. Encryption service binary methods roundtrip correctly
5. Existing text encryption methods still work (backward compatible)
6. New columns are nullable — existing documents don't get constraint violations
</verification>

<success_criteria>
- `PRAGMA table_info(documents)` shows content_type, content_text, metadata_json columns
- `SELECT sql FROM sqlite_master WHERE name='document_fts'` contains 'unicode61'
- EncryptionService.encrypt_binary/decrypt_binary roundtrip raw bytes correctly
- Existing encrypt_document/decrypt_document continue working unchanged
- init_db() completes without errors on both fresh and existing databases
</success_criteria>

<output>
After completion, create `.planning/phases/54-backend-foundation-document-parsing-security/54-02-SUMMARY.md`
</output>
