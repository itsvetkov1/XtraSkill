---
phase: 03-ai-powered-conversations
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/services/ai_service.py
  - backend/app/services/conversation_service.py
  - backend/app/routes/conversations.py
  - backend/main.py
  - backend/requirements.txt
autonomous: true
user_setup:
  - service: anthropic
    why: "AI conversation responses"
    env_vars:
      - name: ANTHROPIC_API_KEY
        source: "Anthropic Console -> API Keys -> Create key"

must_haves:
  truths:
    - "User message sent to backend triggers Claude API call"
    - "AI response streams back to client via SSE"
    - "Document search tool is available to Claude and returns FTS5 results"
    - "Conversation history is loaded from database and sent to Claude"
  artifacts:
    - path: "backend/app/services/ai_service.py"
      provides: "Claude client wrapper with tool definitions"
      exports: ["AIService", "DOCUMENT_SEARCH_TOOL", "SYSTEM_PROMPT"]
    - path: "backend/app/services/conversation_service.py"
      provides: "Message persistence and context building"
      exports: ["save_message", "build_conversation_context"]
    - path: "backend/app/routes/conversations.py"
      provides: "SSE streaming chat endpoint"
      exports: ["router"]
  key_links:
    - from: "backend/app/routes/conversations.py"
      to: "backend/app/services/ai_service.py"
      via: "AIService.stream_chat()"
      pattern: "ai_service\\.stream_chat"
    - from: "backend/app/services/ai_service.py"
      to: "backend/app/services/document_search.py"
      via: "search_documents tool execution"
      pattern: "search_documents"
---

<objective>
Create backend AI service with Claude integration and SSE streaming chat endpoint.

Purpose: Enable AI-powered conversations by integrating Claude API with tool use for document search, streaming responses via SSE, and proper conversation context management.

Output: Working backend endpoint POST /threads/{thread_id}/chat that streams AI responses as SSE events.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-ai-powered-conversations/03-RESEARCH.md

# Existing code to reference
@backend/app/models.py (Thread, Message, TokenUsage models)
@backend/app/routes/threads.py (Thread endpoint patterns)
@backend/app/services/document_search.py (FTS5 search_documents function)
@backend/app/config.py (settings.anthropic_api_key)
@backend/app/utils/jwt.py (get_current_user dependency)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install sse-starlette and create AI service</name>
  <files>
    backend/requirements.txt
    backend/app/services/ai_service.py
  </files>
  <action>
1. Add `sse-starlette>=2.0.0` to backend/requirements.txt (anthropic already installed at 0.76.0+)

2. Create backend/app/services/ai_service.py with:

```python
"""
AI Service for Claude API integration.

Provides streaming chat with tool use for document search.
Uses AsyncAnthropic client for non-blocking operations.
"""
import anthropic
from typing import AsyncGenerator, List, Dict, Any
from app.config import settings
from app.services.document_search import search_documents

# Claude model to use
MODEL = "claude-sonnet-4-5-20250514"

# System prompt for BA assistant behavior
SYSTEM_PROMPT = """You are a Business Analyst AI assistant helping users explore and document software requirements. Your role is to:

1. PROACTIVELY IDENTIFY EDGE CASES: When users describe requirements, think about what could go wrong. Ask questions like "What happens if a user enters invalid data?" or "How should the system behave when the network is unavailable?" without being prompted.

2. ASK CLARIFYING QUESTIONS: Explore requirements deeply. Don't assume - ask about:
   - User personas and their specific needs
   - Error handling and edge cases
   - Performance requirements and constraints
   - Integration points with other systems
   - Security and access control considerations

3. SEARCH DOCUMENTS AUTONOMOUSLY: When users mention anything that might be documented (policies, requirements, specifications, notes), use the search_documents tool to find relevant context. Reference what you find.

4. MAINTAIN CONTEXT: Reference earlier parts of the conversation. Remember decisions made and requirements discussed. Build on previous discussion points.

Be conversational but thorough. Help users think through their requirements completely. Your goal is to ensure no important edge case or requirement is missed."""

# Tool definition for document search
DOCUMENT_SEARCH_TOOL = {
    "name": "search_documents",
    "description": """Search project documents for relevant information.

USE THIS TOOL WHEN:
- User mentions documents, files, or project materials
- User asks about policies, requirements, or specifications
- User references something that might be in uploaded documents
- You need context about the project to answer accurately
- Discussion involves specific features, constraints, or decisions that may be documented

DO NOT USE WHEN:
- User is asking general questions not related to project documents
- You already have sufficient context from conversation history

Returns: Document snippets with filenames and relevance scores.""",
    "input_schema": {
        "type": "object",
        "properties": {
            "query": {
                "type": "string",
                "description": "Search query to find relevant documents. Use specific terms from the conversation."
            }
        },
        "required": ["query"]
    }
}

class AIService:
    """Claude AI service for streaming chat with tool use."""

    def __init__(self):
        self.client = anthropic.AsyncAnthropic(api_key=settings.anthropic_api_key)
        self.tools = [DOCUMENT_SEARCH_TOOL]

    async def execute_tool(
        self,
        tool_name: str,
        tool_input: dict,
        project_id: str,
        db
    ) -> str:
        """Execute a tool and return result as string."""
        if tool_name == "search_documents":
            query = tool_input.get("query", "")
            results = await search_documents(db, project_id, query)

            if not results:
                return "No relevant documents found for this query."

            formatted = []
            for doc_id, filename, snippet, score in results[:5]:
                # Clean up snippet HTML markers for Claude
                clean_snippet = snippet.replace("<mark>", "**").replace("</mark>", "**")
                formatted.append(f"**{filename}**:\n{clean_snippet}")

            return "\n\n---\n\n".join(formatted)

        return f"Unknown tool: {tool_name}"

    async def stream_chat(
        self,
        messages: List[Dict[str, Any]],
        project_id: str,
        db
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Stream chat response from Claude with tool use.

        Yields SSE-formatted events:
        - text_delta: Incremental text from Claude
        - tool_executing: Tool is being executed
        - message_complete: Final message with usage stats
        - error: Error occurred
        """
        import json

        try:
            while True:
                async with self.client.messages.stream(
                    model=MODEL,
                    max_tokens=4096,
                    messages=messages,
                    tools=self.tools,
                    system=SYSTEM_PROMPT
                ) as stream:
                    accumulated_text = ""

                    async for text in stream.text_stream:
                        accumulated_text += text
                        yield {
                            "event": "text_delta",
                            "data": json.dumps({"text": text})
                        }

                    final = await stream.get_final_message()

                    # Check if we need to execute tools
                    if final.stop_reason != "tool_use":
                        # Done - yield completion event
                        yield {
                            "event": "message_complete",
                            "data": json.dumps({
                                "content": accumulated_text,
                                "usage": {
                                    "input_tokens": final.usage.input_tokens,
                                    "output_tokens": final.usage.output_tokens
                                }
                            })
                        }
                        return

                    # Execute tools
                    yield {
                        "event": "tool_executing",
                        "data": json.dumps({"status": "Searching project documents..."})
                    }

                    tool_results = []
                    for block in final.content:
                        if block.type == "tool_use":
                            result = await self.execute_tool(
                                block.name,
                                block.input,
                                project_id,
                                db
                            )
                            tool_results.append({
                                "type": "tool_result",
                                "tool_use_id": block.id,
                                "content": result
                            })

                    # Continue conversation with tool results
                    messages.append({"role": "assistant", "content": final.content})
                    messages.append({"role": "user", "content": tool_results})

                    # Yield accumulated text before tool call
                    if accumulated_text:
                        yield {
                            "event": "text_delta",
                            "data": json.dumps({"text": "\n\n"})
                        }

        except anthropic.APIError as e:
            yield {
                "event": "error",
                "data": json.dumps({"message": f"AI service error: {str(e)}"})
            }
        except Exception as e:
            yield {
                "event": "error",
                "data": json.dumps({"message": f"Unexpected error: {str(e)}"})
            }
```
  </action>
  <verify>
Run `pip install -r backend/requirements.txt` and verify sse-starlette installs.
Import test: `python -c "from app.services.ai_service import AIService, SYSTEM_PROMPT, DOCUMENT_SEARCH_TOOL; print('OK')"`
  </verify>
  <done>
AIService class exists with stream_chat async generator, SYSTEM_PROMPT defines BA assistant behavior, DOCUMENT_SEARCH_TOOL enables autonomous document search.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create conversation service for message persistence and context building</name>
  <files>
    backend/app/services/conversation_service.py
  </files>
  <action>
Create backend/app/services/conversation_service.py:

```python
"""
Conversation service for message persistence and context building.

Handles saving messages to database and building conversation context
for Claude API calls with token-aware truncation.
"""
from typing import List, Dict, Any
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession
from app.models import Message, Thread

# Context window limits
MAX_CONTEXT_TOKENS = 150000  # Leave room for response and system prompt
# Rough estimate: 1 token ~= 4 characters
CHARS_PER_TOKEN = 4


def estimate_tokens(text: str) -> int:
    """Rough token estimation based on character count."""
    return len(text) // CHARS_PER_TOKEN


def estimate_messages_tokens(messages: List[Dict[str, Any]]) -> int:
    """Estimate total tokens in message list."""
    total = 0
    for msg in messages:
        content = msg.get("content", "")
        if isinstance(content, str):
            total += estimate_tokens(content)
        elif isinstance(content, list):
            # Handle tool results or multi-part content
            for part in content:
                if isinstance(part, dict):
                    total += estimate_tokens(str(part.get("content", "")))
    return total


async def save_message(
    db: AsyncSession,
    thread_id: str,
    role: str,
    content: str
) -> Message:
    """
    Save a message to the database.

    Args:
        db: Database session
        thread_id: ID of the thread
        role: 'user' or 'assistant'
        content: Message content

    Returns:
        Created Message object
    """
    message = Message(
        thread_id=thread_id,
        role=role,
        content=content
    )
    db.add(message)
    await db.commit()
    await db.refresh(message)

    # Update thread's updated_at timestamp
    stmt = select(Thread).where(Thread.id == thread_id)
    result = await db.execute(stmt)
    thread = result.scalar_one_or_none()
    if thread:
        from datetime import datetime
        thread.updated_at = datetime.utcnow()
        await db.commit()

    return message


async def build_conversation_context(
    db: AsyncSession,
    thread_id: str
) -> List[Dict[str, Any]]:
    """
    Build conversation context from thread messages.

    Loads all messages from thread and converts to Claude message format.
    Implements token-aware truncation if conversation is too long.

    Args:
        db: Database session
        thread_id: ID of the thread

    Returns:
        List of messages in Claude API format
    """
    # Fetch all messages in chronological order
    stmt = (
        select(Message)
        .where(Message.thread_id == thread_id)
        .order_by(Message.created_at)
    )
    result = await db.execute(stmt)
    messages = result.scalars().all()

    # Convert to Claude message format
    conversation = []
    for msg in messages:
        conversation.append({
            "role": msg.role,
            "content": msg.content
        })

    # Check token count and truncate if needed
    total_tokens = estimate_messages_tokens(conversation)

    if total_tokens > MAX_CONTEXT_TOKENS:
        conversation = truncate_conversation(conversation, MAX_CONTEXT_TOKENS)

    return conversation


def truncate_conversation(
    messages: List[Dict[str, Any]],
    max_tokens: int
) -> List[Dict[str, Any]]:
    """
    Truncate conversation to fit within token budget.

    Strategy: Keep recent messages that fit in 80% of budget.
    Prepend summary note about truncated messages.

    Args:
        messages: Full conversation history
        max_tokens: Maximum tokens allowed

    Returns:
        Truncated message list
    """
    budget = int(max_tokens * 0.8)  # 80% for messages, 20% buffer
    recent_messages = []
    token_count = 0

    # Work backwards from most recent
    for msg in reversed(messages):
        msg_tokens = estimate_messages_tokens([msg])
        if token_count + msg_tokens > budget:
            break
        recent_messages.insert(0, msg)
        token_count += msg_tokens

    # If we truncated, add summary
    truncated_count = len(messages) - len(recent_messages)
    if truncated_count > 0:
        summary = {
            "role": "user",
            "content": f"[System note: {truncated_count} earlier messages in this conversation have been summarized to fit context limits. The conversation began earlier and covered additional topics not shown here.]"
        }
        recent_messages.insert(0, summary)

    return recent_messages


async def get_message_count(db: AsyncSession, thread_id: str) -> int:
    """Get count of messages in a thread."""
    from sqlalchemy import func
    stmt = select(func.count(Message.id)).where(Message.thread_id == thread_id)
    result = await db.execute(stmt)
    return result.scalar() or 0
```
  </action>
  <verify>
Import test: `python -c "from app.services.conversation_service import save_message, build_conversation_context, get_message_count; print('OK')"`
  </verify>
  <done>
Conversation service provides save_message for persistence, build_conversation_context for loading thread history in Claude format, and token-aware truncation for long conversations.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create SSE streaming chat endpoint</name>
  <files>
    backend/app/routes/conversations.py
    backend/main.py
  </files>
  <action>
1. Create backend/app/routes/conversations.py:

```python
"""
Conversation streaming endpoints for AI-powered chat.

Provides SSE streaming endpoint for real-time AI responses with tool use.
"""
import json
from fastapi import APIRouter, Depends, HTTPException, Request, status
from pydantic import BaseModel, Field
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import selectinload
from sse_starlette.sse import EventSourceResponse

from app.database import get_db
from app.models import Thread, Project
from app.utils.jwt import get_current_user
from app.services.ai_service import AIService
from app.services.conversation_service import (
    save_message,
    build_conversation_context,
    get_message_count
)

router = APIRouter()


class ChatRequest(BaseModel):
    """Request model for chat message."""
    content: str = Field(..., min_length=1, max_length=32000)


async def validate_thread_access(
    db: AsyncSession,
    thread_id: str,
    user_id: str
) -> Thread:
    """
    Validate thread exists and belongs to user's project.

    Returns Thread with project loaded, or raises 404.
    """
    stmt = (
        select(Thread)
        .where(Thread.id == thread_id)
        .options(selectinload(Thread.project))
    )
    result = await db.execute(stmt)
    thread = result.scalar_one_or_none()

    if not thread:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Thread not found"
        )

    if thread.project.user_id != user_id:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Thread not found"
        )

    return thread


@router.post("/threads/{thread_id}/chat")
async def stream_chat(
    thread_id: str,
    request: Request,
    body: ChatRequest,
    current_user: dict = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Stream AI response for a chat message.

    Accepts user message, saves to database, streams AI response via SSE.
    AI can use tools (document search) and maintains conversation context.

    SSE Events:
    - text_delta: Incremental text from AI
    - tool_executing: AI is executing a tool
    - message_complete: Response complete with usage stats
    - error: Error occurred

    Args:
        thread_id: ID of the thread
        body: Chat message content
        current_user: Authenticated user
        db: Database session

    Returns:
        EventSourceResponse streaming AI response
    """
    # Validate thread access
    thread = await validate_thread_access(db, thread_id, current_user["user_id"])

    # Save user message to database
    await save_message(db, thread_id, "user", body.content)

    # Build conversation context from thread history
    conversation = await build_conversation_context(db, thread_id)

    # Add the new user message
    conversation.append({"role": "user", "content": body.content})

    # Initialize AI service
    ai_service = AIService()

    async def event_generator():
        """Generate SSE events from AI response."""
        accumulated_text = ""
        usage_data = None

        try:
            async for event in ai_service.stream_chat(
                conversation,
                thread.project_id,
                db
            ):
                # Check for client disconnect
                if await request.is_disconnected():
                    break

                # Track accumulated text for saving
                if event["event"] == "text_delta":
                    data = json.loads(event["data"])
                    accumulated_text += data.get("text", "")

                # Track usage for token tracking
                if event["event"] == "message_complete":
                    data = json.loads(event["data"])
                    usage_data = data.get("usage", {})
                    accumulated_text = data.get("content", accumulated_text)

                yield event

            # Save assistant message after streaming completes
            if accumulated_text:
                await save_message(db, thread_id, "assistant", accumulated_text)

            # TODO: Track token usage (Plan 02)
            # TODO: Update thread summary (Plan 02)

        except Exception as e:
            yield {
                "event": "error",
                "data": json.dumps({"message": str(e)})
            }

    return EventSourceResponse(
        event_generator(),
        headers={
            "X-Accel-Buffering": "no",  # Disable Nginx buffering
            "Cache-Control": "no-cache",
        }
    )
```

2. Update backend/main.py to register the conversations router:

Find the section with router registrations (after `app.include_router(threads_router, prefix="/api")`), and add:

```python
from app.routes.conversations import router as conversations_router

# ... after other router registrations ...
app.include_router(conversations_router, prefix="/api")
```
  </action>
  <verify>
1. Run backend server: `cd backend && uvicorn main:app --reload --port 8002`
2. Test endpoint exists: `curl -X POST http://localhost:8002/api/threads/test-id/chat -H "Content-Type: application/json" -d '{"content":"test"}'`
   - Should return 401 (unauthorized) or 404 (thread not found), not 404 (route not found)
  </verify>
  <done>
POST /threads/{thread_id}/chat endpoint streams AI responses via SSE, validates thread ownership, saves messages to database, and executes document search tool when AI requests it.
  </done>
</task>

</tasks>

<verification>
1. Backend starts without import errors
2. AI service can be instantiated: `AIService()`
3. SSE endpoint returns proper content-type: `text/event-stream`
4. With valid auth and thread, user message triggers Claude API call
5. Document search tool is available to Claude
</verification>

<success_criteria>
- sse-starlette installed and imported
- AIService class with stream_chat method
- SYSTEM_PROMPT defines proactive BA assistant behavior
- DOCUMENT_SEARCH_TOOL enables autonomous document search
- save_message persists user and assistant messages
- build_conversation_context loads thread history for Claude
- POST /threads/{thread_id}/chat streams SSE events
- Thread ownership validation returns 404 for unauthorized access
</success_criteria>

<output>
After completion, create `.planning/phases/03-ai-powered-conversations/03-01-SUMMARY.md`
</output>
