---
phase: 21-provider-adapters
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/config.py
  - backend/app/services/llm/base.py
  - backend/app/services/llm/deepseek_adapter.py
  - backend/app/services/llm/factory.py
  - backend/app/services/llm/__init__.py
autonomous: true
user_setup:
  - service: deepseek
    why: "DeepSeek API access for LLM provider"
    env_vars:
      - name: DEEPSEEK_API_KEY
        source: "DeepSeek Platform -> API Keys (https://platform.deepseek.com/api_keys)"
      - name: DEEPSEEK_MODEL
        source: "Optional: Override default model (default: deepseek-reasoner)"

must_haves:
  truths:
    - "User can create conversation with DeepSeek provider and receive streaming responses"
    - "DeepSeek reasoning_content normalized to StreamChunk format (hidden from user)"
    - "DeepSeek errors show provider name and error code"
  artifacts:
    - path: "backend/app/services/llm/deepseek_adapter.py"
      provides: "DeepSeekAdapter implementation"
      min_lines: 80
      exports: ["DeepSeekAdapter"]
    - path: "backend/app/services/llm/base.py"
      provides: "DEEPSEEK enum value"
      contains: "DEEPSEEK = \"deepseek\""
    - path: "backend/app/services/llm/factory.py"
      provides: "DeepSeek adapter registration"
      contains: "LLMProvider.DEEPSEEK"
  key_links:
    - from: "backend/app/services/llm/factory.py"
      to: "backend/app/services/llm/deepseek_adapter.py"
      via: "import and registration"
      pattern: "from .deepseek_adapter import DeepSeekAdapter"
    - from: "backend/app/services/llm/deepseek_adapter.py"
      to: "OpenAI SDK"
      via: "AsyncOpenAI with base_url override"
      pattern: "base_url.*deepseek"
---

<objective>
Implement DeepSeek adapter using the OpenAI SDK with base_url override for streaming support.

Purpose: Enable users to use DeepSeek's reasoning models for BA conversations with cost savings.
Output: Working DeepSeekAdapter that yields StreamChunk objects from DeepSeek API responses.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/21-provider-adapters/21-CONTEXT.md
@.planning/phases/21-provider-adapters/21-RESEARCH.md
@backend/app/services/llm/base.py
@backend/app/services/llm/anthropic_adapter.py
@backend/app/services/llm/factory.py
@backend/app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add DeepSeek configuration</name>
  <files>
    backend/app/config.py
    backend/app/services/llm/base.py
  </files>
  <action>
1. Add to config.py Settings class (if not already added by 21-01):
   - `deepseek_api_key: str = ""` - DeepSeek API key
   - `deepseek_model: str = "deepseek-reasoner"` - configurable model name

2. Update base.py LLMProvider enum (if not already added by 21-01):
   - Add `DEEPSEEK = "deepseek"` (uncomment from existing comment)

Note: If 21-01 already modified these files, just add the DeepSeek-specific lines.
  </action>
  <verify>
    - `from app.config import settings; settings.deepseek_api_key` exists
    - `from app.services.llm.base import LLMProvider; LLMProvider.DEEPSEEK` works
  </verify>
  <done>DEEPSEEK_API_KEY and DEEPSEEK_MODEL configurable, LLMProvider.DEEPSEEK exists</done>
</task>

<task type="auto">
  <name>Task 2: Create DeepSeekAdapter implementation</name>
  <files>backend/app/services/llm/deepseek_adapter.py</files>
  <action>
Create DeepSeekAdapter using OpenAI SDK with base_url override.

```python
"""
DeepSeek Adapter.

Implements the LLMAdapter interface for DeepSeek models
using the OpenAI SDK with base_url override.

DeepSeek is OpenAI-compatible, so we use the openai package
with a different base URL.
"""
import asyncio
from typing import AsyncGenerator, Dict, Any, List, Optional

from openai import AsyncOpenAI, APIError, RateLimitError, APIStatusError

from .base import LLMAdapter, LLMProvider, StreamChunk

# Retry configuration (per CONTEXT.md: 2 retries, fixed delay)
MAX_RETRIES = 2
RETRY_DELAY = 1.0  # seconds

# Default model (configurable via DEEPSEEK_MODEL env var)
DEFAULT_MODEL = "deepseek-reasoner"

# DeepSeek API base URL
DEEPSEEK_BASE_URL = "https://api.deepseek.com"


class DeepSeekAdapter(LLMAdapter):
    """
    DeepSeek adapter implementation.

    Uses OpenAI SDK with base_url override for API compatibility.
    Reasoning content is hidden per CONTEXT.md decision.

    CRITICAL: reasoning_content must NOT be included in subsequent
    messages - the API returns 400 if you pass it back.
    """

    def __init__(
        self,
        api_key: str,
        model: Optional[str] = None,
    ):
        self._api_key = api_key
        self.model = model or DEFAULT_MODEL
        self.client = AsyncOpenAI(
            api_key=api_key,
            base_url=DEEPSEEK_BASE_URL,
        )

    @property
    def provider(self) -> LLMProvider:
        return LLMProvider.DEEPSEEK

    async def stream_chat(
        self,
        messages: List[Dict[str, Any]],
        system_prompt: str,
        tools: Optional[List[Dict[str, Any]]] = None,
        max_tokens: int = 4096,
    ) -> AsyncGenerator[StreamChunk, None]:
        """Stream chat from DeepSeek, yielding StreamChunk objects."""

        # Convert messages to OpenAI format
        openai_messages = self._convert_messages(messages, system_prompt)

        # Build request kwargs
        kwargs: Dict[str, Any] = {
            "model": self.model,
            "messages": openai_messages,
            "stream": True,
            "max_tokens": max_tokens,
        }

        # Add tools if provided (use deepseek-chat for tools, not deepseek-reasoner)
        if tools:
            kwargs["tools"] = self._convert_tools(tools)
            # Note: deepseek-reasoner may have unstable tool support per RESEARCH.md
            # Consider using deepseek-chat for tool calls if issues arise

        # Streaming with retry
        for attempt in range(MAX_RETRIES + 1):
            try:
                async for chunk in self._stream_impl(**kwargs):
                    yield chunk
                return  # Success
            except RateLimitError as e:
                if attempt < MAX_RETRIES:
                    await asyncio.sleep(RETRY_DELAY)
                    continue
                yield StreamChunk(
                    chunk_type="error",
                    error=f"DeepSeek rate limit exceeded (429): {str(e)}",
                )
                return
            except APIStatusError as e:
                if self._is_retryable(e) and attempt < MAX_RETRIES:
                    await asyncio.sleep(RETRY_DELAY)
                    continue
                yield StreamChunk(
                    chunk_type="error",
                    error=f"DeepSeek error ({e.status_code}): {e.message}",
                )
                return
            except APIError as e:
                yield StreamChunk(
                    chunk_type="error",
                    error=f"DeepSeek API error: {str(e)}",
                )
                return
            except Exception as e:
                yield StreamChunk(
                    chunk_type="error",
                    error=f"DeepSeek unexpected error: {str(e)}",
                )
                return

    async def _stream_impl(self, **kwargs) -> AsyncGenerator[StreamChunk, None]:
        """Internal streaming implementation."""
        usage = {"input_tokens": 0, "output_tokens": 0}

        stream = await self.client.chat.completions.create(**kwargs)

        async for chunk in stream:
            if not chunk.choices:
                continue

            delta = chunk.choices[0].delta

            # Handle reasoning_content (hide per CONTEXT.md)
            if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                # Skip yielding reasoning content to user
                # Could optionally store in database per Claude's discretion
                pass

            # Handle regular content
            if delta.content:
                yield StreamChunk(chunk_type="text", content=delta.content)

            # Handle tool calls
            if hasattr(delta, 'tool_calls') and delta.tool_calls:
                for tool_call in delta.tool_calls:
                    if tool_call.function:
                        yield StreamChunk(
                            chunk_type="tool_use",
                            tool_call={
                                "id": tool_call.id or f"call_{id(tool_call)}",
                                "name": tool_call.function.name or "",
                                "input": self._parse_tool_args(tool_call.function.arguments),
                            },
                        )

            # Track usage from final chunk
            if hasattr(chunk, 'usage') and chunk.usage:
                usage["input_tokens"] = chunk.usage.prompt_tokens or 0
                usage["output_tokens"] = chunk.usage.completion_tokens or 0

        yield StreamChunk(chunk_type="complete", usage=usage)

    def _convert_messages(
        self,
        messages: List[Dict[str, Any]],
        system_prompt: str,
    ) -> List[Dict[str, Any]]:
        """Convert messages to OpenAI/DeepSeek format with system prompt."""
        openai_messages = [{"role": "system", "content": system_prompt}]

        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")

            if isinstance(content, str):
                openai_messages.append({"role": role, "content": content})
            elif isinstance(content, list):
                # Handle multi-part content
                text_parts = []
                for block in content:
                    if isinstance(block, dict):
                        if block.get("type") == "text":
                            text_parts.append(block.get("text", ""))
                        elif block.get("type") == "tool_result":
                            text_parts.append(f"Tool result: {block.get('content', '')}")
                if text_parts:
                    openai_messages.append({"role": role, "content": "\n".join(text_parts)})

        return openai_messages

    def _convert_tools(self, tools: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Convert tools from Anthropic format to OpenAI format."""
        openai_tools = []
        for tool in tools:
            openai_tools.append({
                "type": "function",
                "function": {
                    "name": tool.get("name", ""),
                    "description": tool.get("description", ""),
                    "parameters": tool.get("input_schema", {}),
                },
            })
        return openai_tools

    def _parse_tool_args(self, arguments: Optional[str]) -> Dict[str, Any]:
        """Parse tool arguments from JSON string."""
        if not arguments:
            return {}
        try:
            import json
            return json.loads(arguments)
        except (json.JSONDecodeError, TypeError):
            return {"raw": arguments}

    def _is_retryable(self, error: APIStatusError) -> bool:
        """Check if error is retryable (rate limit or server error)."""
        return error.status_code in (429, 500, 503)
```

Key implementation notes:
- Uses OpenAI SDK with `base_url="https://api.deepseek.com"`
- Handles `reasoning_content` field but does NOT yield it (hidden per CONTEXT.md)
- Does NOT include reasoning_content in subsequent messages (would cause 400)
- Simple retry with fixed delay (2 retries, 1 second delay)
- Provider-specific error messages with error code
- System prompt passed as first message (OpenAI format)
- Note: temperature/top_p parameters ignored by deepseek-reasoner per RESEARCH.md
  </action>
  <verify>
    - File exists with DeepSeekAdapter class
    - Class inherits from LLMAdapter
    - stream_chat method is async generator yielding StreamChunk
    - `python -c "from app.services.llm.deepseek_adapter import DeepSeekAdapter"` succeeds
  </verify>
  <done>DeepSeekAdapter implementation with streaming, reasoning hidden, retry logic, error handling</done>
</task>

<task type="auto">
  <name>Task 3: Register DeepSeek adapter in factory</name>
  <files>
    backend/app/services/llm/factory.py
    backend/app/services/llm/__init__.py
  </files>
  <action>
1. Update factory.py:
   - Import DeepSeekAdapter: `from .deepseek_adapter import DeepSeekAdapter`
   - Add to _adapters registry: `LLMProvider.DEEPSEEK: DeepSeekAdapter,`
   - Add to _get_api_key method:
     ```python
     if provider == LLMProvider.DEEPSEEK:
         key = settings.deepseek_api_key
         if not key:
             raise ValueError(
                 "DEEPSEEK_API_KEY not configured. "
                 "Get API key from https://platform.deepseek.com/api_keys"
             )
         return key
     ```
   - For model, check settings.deepseek_model in create() if provider is DEEPSEEK and model not provided

2. Update __init__.py:
   - Import DeepSeekAdapter
   - Add to __all__ list
  </action>
  <verify>
    - `from app.services.llm import DeepSeekAdapter, LLMFactory`
    - `LLMFactory.list_providers()` includes "deepseek"
    - Factory code handles DEEPSEEK API key retrieval
  </verify>
  <done>DeepSeekAdapter registered in factory, importable from package, list_providers includes "deepseek"</done>
</task>

</tasks>

<verification>
1. Adapter importable: `from app.services.llm import DeepSeekAdapter`
2. Factory creates adapter: `LLMFactory.create("deepseek")` returns DeepSeekAdapter (with API key configured)
3. Provider enum: `LLMProvider.DEEPSEEK.value == "deepseek"`
4. Uses OpenAI SDK with base_url override
</verification>

<success_criteria>
- DEEPSEEK_API_KEY and DEEPSEEK_MODEL settings exist in config
- LLMProvider.DEEPSEEK enum value exists
- DeepSeekAdapter class implements LLMAdapter with stream_chat async generator
- DeepSeekAdapter uses OpenAI SDK with base_url="https://api.deepseek.com"
- DeepSeekAdapter registered in LLMFactory._adapters
- LLMFactory.create("deepseek") returns DeepSeekAdapter instance
- All imports work without errors
</success_criteria>

<output>
After completion, create `.planning/phases/21-provider-adapters/21-02-SUMMARY.md`
</output>
