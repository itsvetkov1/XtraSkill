---
phase: 59-cli-subprocess-adapter
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/services/llm/claude_cli_adapter.py
autonomous: true

must_haves:
  truths:
    - "ClaudeCLIAdapter spawns CLI subprocess with correct flags and parses JSON output"
    - "Subprocess cleanup prevents zombie processes even when exceptions occur"
    - "JSON stream events translate to StreamChunk format (text, tool_use, complete, error)"
    - "CLI path verified at init time, clear error if not found"
    - "Adapter signals is_agent_provider=True for AIService routing"
  artifacts:
    - path: "backend/app/services/llm/claude_cli_adapter.py"
      provides: "Full ClaudeCLIAdapter implementation with stream_chat()"
      contains: "asyncio.create_subprocess_exec"
  key_links:
    - from: "backend/app/services/llm/claude_cli_adapter.py"
      to: "backend/app/services/ai_service.py"
      via: "is_agent_provider attribute and set_context() method"
      pattern: "is_agent_provider.*=.*True"
    - from: "backend/app/services/llm/claude_cli_adapter.py"
      to: "backend/app/services/llm/base.py"
      via: "StreamChunk and LLMAdapter ABC"
      pattern: "class ClaudeCLIAdapter\\(LLMAdapter\\)"
---

<objective>
Implement ClaudeCLIAdapter.stream_chat() that spawns Claude Code CLI as an async subprocess, parses line-delimited JSON from stdout, and translates events to StreamChunk format with robust process lifecycle management.

Purpose: Enable the CLI subprocess integration path for quality comparison against the SDK adapter (Phase 58). This is the core implementation that makes `claude-code-cli` provider functional.

Output: Fully implemented `claude_cli_adapter.py` replacing the current stub.
</objective>

<execution_context>
@C:/Users/ibcve/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ibcve/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/59-cli-subprocess-adapter/59-RESEARCH.md
@.planning/phases/58-agent-sdk-adapter/58-01-SUMMARY.md

# Key source files
@backend/app/services/llm/base.py
@backend/app/services/llm/claude_agent_adapter.py
@backend/app/services/llm/factory.py
@backend/app/services/ai_service.py
@backend/app/services/mcp_tools.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement ClaudeCLIAdapter with subprocess lifecycle and JSON event translation</name>
  <files>backend/app/services/llm/claude_cli_adapter.py</files>
  <action>
Replace the entire stub in `claude_cli_adapter.py` with the full implementation. Follow the Phase 58 ClaudeAgentAdapter pattern closely for consistency.

**Imports needed:**
- `asyncio`, `json`, `logging`, `os`, `shutil` from stdlib
- `LLMAdapter`, `LLMProvider`, `StreamChunk` from `.base`
- Context vars from `app.services.mcp_tools` (`_db_context`, `_project_id_context`, `_thread_id_context`, `_documents_used_context`)

**Class attributes and __init__:**
- `is_agent_provider = True` class attribute (same as ClaudeAgentAdapter)
- `__init__(self, api_key, model=None)`:
  - Store api_key and model (default: `claude-sonnet-4-5-20250929`)
  - Call `shutil.which("claude")` to verify CLI is available
  - If not found, raise `RuntimeError` with clear message about installing claude-agent-sdk v0.1.35+
  - Store cli_path, initialize db/project_id/thread_id to None
  - Log CLI path at info level

**provider property:** Return `LLMProvider.CLAUDE_CODE_CLI`

**set_context(self, db, project_id, thread_id):** Same pattern as ClaudeAgentAdapter - store db, project_id, thread_id for request scope.

**_convert_messages_to_prompt(self, messages):**
- For POC: Extract last user message content as prompt string
- Handle str content directly, list content by extracting text parts
- Return empty string for empty messages

**_translate_event(self, event):** Map CLI JSON event types to StreamChunk:
- `type: "stream_event"` with nested `event.type: "content_block_delta"` and `delta.type: "text_delta"` -> `StreamChunk(chunk_type="text", content=delta.text)`
- `type: "assistant_message"` with content blocks containing `type: "tool_use"` -> `StreamChunk(chunk_type="tool_use", tool_call={id, name, input})`. Also emit tool status metadata.
- `type: "result"` -> `StreamChunk(chunk_type="complete", usage={input_tokens, output_tokens})`
- `type: "error"` -> `StreamChunk(chunk_type="error", error=message)`
- Unhandled types: log at debug level, return None

**stream_chat() async generator:**
1. Validate context is set (db, project_id, thread_id), yield error chunk if not
2. Set ContextVars for MCP tools (`_db_context`, `_project_id_context`, `_thread_id_context`, `_documents_used_context`)
3. Build prompt from messages using `_convert_messages_to_prompt()`
4. Build CLI command: `[self.cli_path, "-p", "--output-format", "stream-json", "--verbose", "--model", self.model, prompt_text]`
   - NOTE: Do NOT pass `--system-prompt` flag - CLI may not support it. Instead, prepend system prompt to the prompt text with clear delimiter: `[SYSTEM]: {system_prompt}\n\n[USER]: {prompt_text}`
   - Do NOT pass `--include-partial-messages` unless verified it works (remove if it causes CLI errors)
5. Environment: `{**os.environ, "ANTHROPIC_API_KEY": self._api_key}`
6. Spawn with `asyncio.create_subprocess_exec(*cmd, stdout=PIPE, stderr=PIPE, env=env)`
7. Read stdout line-by-line: `async for line in process.stdout:`
   - Decode UTF-8, strip whitespace, skip empty lines
   - Parse JSON, call `_translate_event()`, yield non-None chunks
   - Catch `json.JSONDecodeError`, log warning, continue
8. After stdout exhausted: `await process.wait()`
9. If returncode != 0: read stderr, yield error chunk
10. After all events, check `_documents_used_context` for source attribution
11. Yield final `StreamChunk(chunk_type="complete", ...)` with usage and documents_used in metadata
12. **CRITICAL finally block** for zombie prevention:
    - Check `process.returncode is None` (still running)
    - Call `process.terminate()`
    - Wait with `asyncio.wait_for(process.wait(), timeout=5.0)`
    - On TimeoutError: call `process.kill()` then `await process.wait()`
    - Log all cleanup actions at warning level

**Important implementation notes:**
- Do NOT duplicate the completion chunk. The `_translate_event` method handles `type: "result"` events from CLI. Only yield a fallback completion chunk if NO result event was received from CLI (use a `received_result` flag).
- Wrap outer try/except around entire generator body, yield error chunk on any Exception
- Follow the exact same docstring and typing conventions as ClaudeAgentAdapter

**What NOT to implement (deferred):**
- MCP tool configuration via temporary .mcp.json file (CLI will use its own tool handling or we rely on system prompt instructions)
- Hard timeout on subprocess (per locked decision: no request timeout for POC)
- HTTP MCP server connection (deferred per Phase 58 decision)
  </action>
  <verify>
1. Run `python -c "from app.services.llm.claude_cli_adapter import ClaudeCLIAdapter; print('Import OK')"` from backend directory
2. Verify class has `is_agent_provider = True`
3. Verify `__init__` calls `shutil.which("claude")`
4. Verify `stream_chat` uses `asyncio.create_subprocess_exec`
5. Verify finally block has `process.terminate()` and `process.kill()` calls
6. Verify `_translate_event` handles all 4 event types (stream_event, assistant_message, result, error)
  </verify>
  <done>
ClaudeCLIAdapter fully implements LLMAdapter.stream_chat() with:
- CLI subprocess spawned via asyncio.create_subprocess_exec with correct flags
- Line-delimited JSON parsed from stdout and translated to StreamChunk
- Subprocess cleanup in finally block prevents zombie processes
- is_agent_provider=True for AIService routing compatibility
- set_context() for request-scoped dependency injection
- CLI path verified at init time with clear error message
  </done>
</task>

<task type="auto">
  <name>Task 2: Verify adapter integrates with existing factory and AIService routing</name>
  <files>backend/app/services/llm/claude_cli_adapter.py</files>
  <action>
Verify the updated adapter works with existing infrastructure without modifying other files.

1. **Import verification:** Run the following from backend directory:
   ```python
   from app.services.llm.claude_cli_adapter import ClaudeCLIAdapter
   from app.services.llm.factory import LLMFactory
   from app.services.llm.base import LLMAdapter
   ```
   All must import without errors.

2. **Factory integration check:** The factory already imports ClaudeCLIAdapter (from Phase 57). Verify:
   - `LLMFactory._adapters[LLMProvider.CLAUDE_CODE_CLI]` points to ClaudeCLIAdapter
   - The adapter class signature matches what factory expects: `__init__(api_key, model=None)`

3. **AIService routing check:** Verify that:
   - `ClaudeCLIAdapter.is_agent_provider` is `True`
   - `ClaudeCLIAdapter` has `set_context()` method
   - AIService._stream_agent_chat() will work with this adapter (same interface as ClaudeAgentAdapter)

4. **Run existing test suite to verify zero regressions:**
   ```bash
   cd backend && python -m pytest tests/unit/ -x -q
   ```
   All existing tests must pass. The existing CLI adapter stub tests will likely need updates since they test for NotImplementedError which is now removed. If these tests fail, update them minimally:
   - Remove the `test_stream_chat_raises_not_implemented` test (no longer valid)
   - Keep init and factory tests (they should still pass since constructor signature is unchanged, though init now requires `claude` in PATH - mock `shutil.which` to return a path)
   - Add `@patch('shutil.which', return_value='/usr/bin/claude')` to init tests that instantiate the adapter

5. **Verify subprocess pattern:** Confirm the adapter's stream_chat is an async generator (has yield statements) and follows the LLMAdapter ABC contract.
  </action>
  <verify>
1. `python -m pytest tests/unit/ -x -q` passes with zero failures
2. `python -c "from app.services.llm.claude_cli_adapter import ClaudeCLIAdapter; assert ClaudeCLIAdapter.is_agent_provider == True; print('Agent provider OK')"` passes
3. `python -c "from app.services.llm.claude_cli_adapter import ClaudeCLIAdapter; assert hasattr(ClaudeCLIAdapter, 'set_context'); print('set_context OK')"` passes
  </verify>
  <done>
ClaudeCLIAdapter integrates cleanly with LLMFactory and AIService:
- Factory creates adapter correctly
- AIService routes to _stream_agent_chat() based on is_agent_provider=True
- set_context() provides request-scoped context same as ClaudeAgentAdapter
- All existing unit tests pass (zero regressions)
  </done>
</task>

</tasks>

<verification>
1. Import check: `from app.services.llm.claude_cli_adapter import ClaudeCLIAdapter` succeeds
2. Class attributes: `is_agent_provider = True`, `provider == LLMProvider.CLAUDE_CODE_CLI`
3. Subprocess management: `asyncio.create_subprocess_exec` in stream_chat, terminate/kill in finally
4. JSON translation: `_translate_event` handles stream_event, assistant_message, result, error
5. Zero regressions: `python -m pytest tests/unit/ -x -q` all pass
</verification>

<success_criteria>
- ClaudeCLIAdapter.stream_chat() fully implemented (no more NotImplementedError)
- Subprocess lifecycle managed with try/finally cleanup
- JSON events translated to StreamChunk format
- Adapter compatible with LLMFactory and AIService routing
- All existing unit tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/59-cli-subprocess-adapter/59-01-SUMMARY.md`
</output>
