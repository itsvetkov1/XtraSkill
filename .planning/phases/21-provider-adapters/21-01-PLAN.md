---
phase: 21-provider-adapters
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/requirements.txt
  - backend/app/config.py
  - backend/app/services/llm/base.py
  - backend/app/services/llm/gemini_adapter.py
  - backend/app/services/llm/factory.py
  - backend/app/services/llm/__init__.py
autonomous: true
user_setup:
  - service: google-gemini
    why: "Gemini API access for LLM provider"
    env_vars:
      - name: GOOGLE_API_KEY
        source: "Google AI Studio -> Get API key (https://aistudio.google.com/app/apikey)"
      - name: GEMINI_MODEL
        source: "Optional: Override default model (default: gemini-3-flash-preview)"

must_haves:
  truths:
    - "User can create conversation with Gemini provider and receive streaming responses"
    - "Gemini thinking output normalized to StreamChunk format (hidden from user)"
    - "Gemini errors show provider name and error code"
  artifacts:
    - path: "backend/app/services/llm/gemini_adapter.py"
      provides: "GeminiAdapter implementation"
      min_lines: 80
      exports: ["GeminiAdapter"]
    - path: "backend/app/services/llm/base.py"
      provides: "GOOGLE enum value"
      contains: "GOOGLE = \"google\""
    - path: "backend/app/services/llm/factory.py"
      provides: "Gemini adapter registration"
      contains: "LLMProvider.GOOGLE"
  key_links:
    - from: "backend/app/services/llm/factory.py"
      to: "backend/app/services/llm/gemini_adapter.py"
      via: "import and registration"
      pattern: "from .gemini_adapter import GeminiAdapter"
    - from: "backend/app/services/llm/gemini_adapter.py"
      to: "google-genai SDK"
      via: "client.aio.models.generate_content_stream"
      pattern: "generate_content_stream"
---

<objective>
Implement Gemini adapter using the google-genai SDK with async streaming support.

Purpose: Enable users to use Google's Gemini models for BA conversations with thinking capabilities.
Output: Working GeminiAdapter that yields StreamChunk objects from Gemini API responses.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/21-provider-adapters/21-CONTEXT.md
@.planning/phases/21-provider-adapters/21-RESEARCH.md
@backend/app/services/llm/base.py
@backend/app/services/llm/anthropic_adapter.py
@backend/app/services/llm/factory.py
@backend/app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add Gemini dependencies and configuration</name>
  <files>
    backend/requirements.txt
    backend/app/config.py
    backend/app/services/llm/base.py
  </files>
  <action>
1. Add `google-genai>=1.0.0` to requirements.txt (after anthropic line)

2. Add to config.py Settings class:
   - `google_api_key: str = ""` - Gemini API key
   - `gemini_model: str = "gemini-3-flash-preview"` - configurable model name

3. Update base.py LLMProvider enum:
   - Add `GOOGLE = "google"` (uncomment from existing comment)
  </action>
  <verify>
    - `pip install google-genai` succeeds
    - `from app.config import settings; settings.google_api_key` exists
    - `from app.services.llm.base import LLMProvider; LLMProvider.GOOGLE` works
  </verify>
  <done>google-genai in requirements, GOOGLE_API_KEY and GEMINI_MODEL configurable, LLMProvider.GOOGLE exists</done>
</task>

<task type="auto">
  <name>Task 2: Create GeminiAdapter implementation</name>
  <files>backend/app/services/llm/gemini_adapter.py</files>
  <action>
Create GeminiAdapter following AnthropicAdapter pattern exactly.

```python
"""
Google Gemini Adapter.

Implements the LLMAdapter interface for Google's Gemini models
using the google-genai SDK with async streaming support.
"""
import asyncio
from typing import AsyncGenerator, Dict, Any, List, Optional

from google import genai
from google.genai import types, errors

from .base import LLMAdapter, LLMProvider, StreamChunk

# Retry configuration (per CONTEXT.md: 2 retries, fixed delay)
MAX_RETRIES = 2
RETRY_DELAY = 1.0  # seconds

# Default model (configurable via GEMINI_MODEL env var)
DEFAULT_MODEL = "gemini-3-flash-preview"


class GeminiAdapter(LLMAdapter):
    """
    Google Gemini adapter implementation.

    Provides streaming chat using google-genai SDK.
    Thinking content is hidden per CONTEXT.md decision.
    """

    def __init__(
        self,
        api_key: str,
        model: Optional[str] = None,
    ):
        self._api_key = api_key
        self.model = model or DEFAULT_MODEL
        self.client = genai.Client(api_key=api_key)

    @property
    def provider(self) -> LLMProvider:
        return LLMProvider.GOOGLE

    async def stream_chat(
        self,
        messages: List[Dict[str, Any]],
        system_prompt: str,
        tools: Optional[List[Dict[str, Any]]] = None,
        max_tokens: int = 4096,
    ) -> AsyncGenerator[StreamChunk, None]:
        """Stream chat from Gemini, yielding StreamChunk objects."""

        # Convert messages to Gemini format
        contents = self._convert_messages(messages)

        # Build config
        config = types.GenerateContentConfig(
            system_instruction=system_prompt,
            max_output_tokens=max_tokens,
            thinking_config=types.ThinkingConfig(thinking_level="high"),
            # Note: NOT including include_thoughts=True per CONTEXT.md
        )

        # Handle tools if provided (non-streaming required for Gemini tools)
        if tools:
            # Gemini doesn't support streaming + tools, use non-streaming
            async for chunk in self._non_streaming_with_tools(
                contents, config, tools
            ):
                yield chunk
            return

        # Streaming without tools
        for attempt in range(MAX_RETRIES + 1):
            try:
                async for chunk in self._stream_impl(contents, config):
                    yield chunk
                return  # Success
            except errors.APIError as e:
                if self._is_retryable(e) and attempt < MAX_RETRIES:
                    await asyncio.sleep(RETRY_DELAY)
                    continue
                yield StreamChunk(
                    chunk_type="error",
                    error=f"Gemini error ({e.code}): {e.message}",
                )
                return
            except Exception as e:
                yield StreamChunk(
                    chunk_type="error",
                    error=f"Gemini unexpected error: {str(e)}",
                )
                return

    async def _stream_impl(
        self,
        contents: list,
        config: types.GenerateContentConfig,
    ) -> AsyncGenerator[StreamChunk, None]:
        """Internal streaming implementation."""
        usage = {"input_tokens": 0, "output_tokens": 0}

        async for chunk in await self.client.aio.models.generate_content_stream(
            model=self.model,
            contents=contents,
            config=config,
        ):
            if chunk.text:
                yield StreamChunk(chunk_type="text", content=chunk.text)

            # Track usage from final chunk
            if hasattr(chunk, 'usage_metadata') and chunk.usage_metadata:
                usage["input_tokens"] = chunk.usage_metadata.prompt_token_count or 0
                usage["output_tokens"] = chunk.usage_metadata.candidates_token_count or 0

        yield StreamChunk(chunk_type="complete", usage=usage)

    async def _non_streaming_with_tools(
        self,
        contents: list,
        config: types.GenerateContentConfig,
        tools: List[Dict[str, Any]],
    ) -> AsyncGenerator[StreamChunk, None]:
        """Handle tool use (requires non-streaming for Gemini)."""
        try:
            # Convert tools to Gemini format
            gemini_tools = self._convert_tools(tools)
            config.tools = gemini_tools

            response = await self.client.aio.models.generate_content(
                model=self.model,
                contents=contents,
                config=config,
            )

            # Process response
            for part in response.candidates[0].content.parts:
                if hasattr(part, 'text') and part.text:
                    yield StreamChunk(chunk_type="text", content=part.text)
                if hasattr(part, 'function_call') and part.function_call:
                    yield StreamChunk(
                        chunk_type="tool_use",
                        tool_call={
                            "id": getattr(part.function_call, 'id', f"call_{id(part)}"),
                            "name": part.function_call.name,
                            "input": dict(part.function_call.args) if part.function_call.args else {},
                        },
                    )

            # Usage
            usage = {"input_tokens": 0, "output_tokens": 0}
            if response.usage_metadata:
                usage["input_tokens"] = response.usage_metadata.prompt_token_count or 0
                usage["output_tokens"] = response.usage_metadata.candidates_token_count or 0

            yield StreamChunk(chunk_type="complete", usage=usage)

        except errors.APIError as e:
            yield StreamChunk(
                chunk_type="error",
                error=f"Gemini error ({e.code}): {e.message}",
            )
        except Exception as e:
            yield StreamChunk(
                chunk_type="error",
                error=f"Gemini unexpected error: {str(e)}",
            )

    def _convert_messages(self, messages: List[Dict[str, Any]]) -> list:
        """Convert messages from Anthropic format to Gemini format."""
        contents = []
        for msg in messages:
            role = "user" if msg["role"] == "user" else "model"
            content = msg.get("content", "")
            if isinstance(content, str):
                contents.append(types.Content(
                    role=role,
                    parts=[types.Part(text=content)]
                ))
            elif isinstance(content, list):
                # Handle multi-part content (text blocks)
                parts = []
                for block in content:
                    if isinstance(block, dict) and block.get("type") == "text":
                        parts.append(types.Part(text=block.get("text", "")))
                    elif isinstance(block, dict) and block.get("type") == "tool_result":
                        parts.append(types.Part(text=f"Tool result: {block.get('content', '')}"))
                if parts:
                    contents.append(types.Content(role=role, parts=parts))
        return contents

    def _convert_tools(self, tools: List[Dict[str, Any]]) -> list:
        """Convert tools from Anthropic format to Gemini format."""
        gemini_tools = []
        for tool in tools:
            gemini_tools.append(types.Tool(
                function_declarations=[
                    types.FunctionDeclaration(
                        name=tool.get("name", ""),
                        description=tool.get("description", ""),
                        parameters=tool.get("input_schema", {}),
                    )
                ]
            ))
        return gemini_tools

    def _is_retryable(self, error: errors.APIError) -> bool:
        """Check if error is retryable (rate limit or server error)."""
        return error.code in (429, 500, 503)
```

Key implementation notes:
- Uses `client.aio.models.generate_content_stream()` for async streaming
- ThinkingConfig with `thinking_level="high"` per CONTEXT.md (heavy thinking)
- Does NOT include `include_thoughts=True` to hide thinking from user
- Non-streaming fallback for tool use (Gemini limitation per RESEARCH.md)
- Simple retry with fixed delay (2 retries, 1 second delay)
- Provider-specific error messages with error code
  </action>
  <verify>
    - File exists with GeminiAdapter class
    - Class inherits from LLMAdapter
    - stream_chat method is async generator yielding StreamChunk
    - `python -c "from app.services.llm.gemini_adapter import GeminiAdapter"` succeeds
  </verify>
  <done>GeminiAdapter implementation with streaming, thinking hidden, retry logic, error handling</done>
</task>

<task type="auto">
  <name>Task 3: Register Gemini adapter in factory</name>
  <files>
    backend/app/services/llm/factory.py
    backend/app/services/llm/__init__.py
  </files>
  <action>
1. Update factory.py:
   - Import GeminiAdapter: `from .gemini_adapter import GeminiAdapter`
   - Add to _adapters registry: `LLMProvider.GOOGLE: GeminiAdapter,`
   - Add to _get_api_key method:
     ```python
     if provider == LLMProvider.GOOGLE:
         key = settings.google_api_key
         if not key:
             raise ValueError(
                 "GOOGLE_API_KEY not configured. "
                 "Get API key from https://aistudio.google.com/app/apikey"
             )
         return key
     ```
   - For model, check settings.gemini_model in create() if provider is GOOGLE and model not provided

2. Update __init__.py:
   - Import GeminiAdapter
   - Add to __all__ list
  </action>
  <verify>
    - `from app.services.llm import GeminiAdapter, LLMFactory`
    - `LLMFactory.list_providers()` includes "google"
    - Factory code handles GOOGLE API key retrieval
  </verify>
  <done>GeminiAdapter registered in factory, importable from package, list_providers includes "google"</done>
</task>

</tasks>

<verification>
1. Dependencies installed: `pip install -r requirements.txt` includes google-genai
2. Adapter importable: `from app.services.llm import GeminiAdapter`
3. Factory creates adapter: `LLMFactory.create("google")` returns GeminiAdapter (with API key configured)
4. Provider enum: `LLMProvider.GOOGLE.value == "google"`
</verification>

<success_criteria>
- google-genai dependency added to requirements.txt
- GOOGLE_API_KEY and GEMINI_MODEL settings exist in config
- LLMProvider.GOOGLE enum value exists
- GeminiAdapter class implements LLMAdapter with stream_chat async generator
- GeminiAdapter registered in LLMFactory._adapters
- LLMFactory.create("google") returns GeminiAdapter instance
- All imports work without errors
</success_criteria>

<output>
After completion, create `.planning/phases/21-provider-adapters/21-01-SUMMARY.md`
</output>
