---
phase: 30-backend-llm-api-tests
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/tests/unit/llm/__init__.py
  - backend/tests/unit/llm/conftest.py
  - backend/tests/unit/llm/test_anthropic_adapter.py
  - backend/tests/unit/llm/test_gemini_adapter.py
  - backend/tests/unit/llm/test_deepseek_adapter.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "AnthropicAdapter unit tests pass without real API calls"
    - "GeminiAdapter unit tests pass without real API calls"
    - "DeepSeekAdapter unit tests pass without real API calls"
    - "Each adapter test verifies text streaming, tool calls, and error handling"
  artifacts:
    - path: "backend/tests/unit/llm/test_anthropic_adapter.py"
      provides: "AnthropicAdapter test coverage"
      min_lines: 80
    - path: "backend/tests/unit/llm/test_gemini_adapter.py"
      provides: "GeminiAdapter test coverage"
      min_lines: 80
    - path: "backend/tests/unit/llm/test_deepseek_adapter.py"
      provides: "DeepSeekAdapter test coverage"
      min_lines: 80
  key_links:
    - from: "backend/tests/unit/llm/test_anthropic_adapter.py"
      to: "anthropic.AsyncAnthropic"
      via: "unittest.mock.patch"
      pattern: "patch.*anthropic"
    - from: "backend/tests/unit/llm/test_gemini_adapter.py"
      to: "google.genai.Client"
      via: "unittest.mock.patch"
      pattern: "patch.*genai"
    - from: "backend/tests/unit/llm/test_deepseek_adapter.py"
      to: "openai.AsyncOpenAI"
      via: "unittest.mock.patch"
      pattern: "patch.*openai"
---

<objective>
Create unit tests for all three LLM adapters (Anthropic, Gemini, DeepSeek)

Purpose: Verify adapter implementations correctly convert API responses to StreamChunk format without making real API calls (BLLM-01, BLLM-02, BLLM-03)

Output: Test files in backend/tests/unit/llm/ with mocked HTTP client tests for each adapter
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/30-backend-llm-api-tests/30-RESEARCH.md
@backend/app/services/llm/anthropic_adapter.py
@backend/app/services/llm/gemini_adapter.py
@backend/app/services/llm/deepseek_adapter.py
@backend/app/services/llm/base.py
@backend/tests/fixtures/llm_fixtures.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LLM test module structure</name>
  <files>
    backend/tests/unit/llm/__init__.py
    backend/tests/unit/llm/conftest.py
  </files>
  <action>
Create the LLM test module directory structure:

1. Create `backend/tests/unit/llm/__init__.py` (empty file)

2. Create `backend/tests/unit/llm/conftest.py` with shared fixtures:
   - `mock_text_stream()`: Async generator helper that yields text chunks
   - `mock_final_message()`: Factory for creating mock final message objects with usage stats
   - `mock_api_error()`: Factory for creating mock API errors

Example helper functions for conftest.py:
```python
import pytest
from unittest.mock import MagicMock, AsyncMock

async def async_iter(items):
    """Helper to create async iterator from list."""
    for item in items:
        yield item

@pytest.fixture
def mock_anthropic_stream():
    """Factory for creating mock Anthropic stream context manager."""
    def _create(text_chunks, tool_blocks=None, usage=None):
        mock_stream = AsyncMock()
        mock_stream.__aenter__.return_value = mock_stream
        mock_stream.__aexit__.return_value = None

        # text_stream is an async iterable
        mock_stream.text_stream = async_iter(text_chunks)

        # Final message mock
        mock_final = MagicMock()
        mock_final.content = tool_blocks or []
        mock_final.usage.input_tokens = (usage or {}).get("input_tokens", 10)
        mock_final.usage.output_tokens = (usage or {}).get("output_tokens", 5)
        mock_stream.get_final_message = AsyncMock(return_value=mock_final)

        return mock_stream
    return _create
```
  </action>
  <verify>
```bash
cd G:\git_repos\BA_assistant\backend
python -c "from tests.unit.llm import conftest; print('LLM test module created')"
```
  </verify>
  <done>LLM test module exists with conftest.py containing shared fixtures</done>
</task>

<task type="auto">
  <name>Task 2: Create AnthropicAdapter tests</name>
  <files>backend/tests/unit/llm/test_anthropic_adapter.py</files>
  <action>
Create comprehensive tests for AnthropicAdapter in `backend/tests/unit/llm/test_anthropic_adapter.py`.

Use class-based organization (established pattern from Phase 29):

**Class: TestAnthropicAdapterInit**
- `test_initializes_with_api_key`: Verify client created with API key
- `test_uses_default_model`: Verify DEFAULT_MODEL used when none specified
- `test_uses_custom_model`: Verify custom model parameter respected
- `test_provider_returns_anthropic`: Verify provider property returns LLMProvider.ANTHROPIC

**Class: TestAnthropicAdapterStreamChat**
- `test_yields_text_chunks`: Mock stream returning ["Hello", " world"], verify 2 text chunks yielded
- `test_yields_tool_use_chunks`: Mock final message with tool_use blocks, verify tool_use chunks yielded
- `test_yields_complete_with_usage`: Verify complete chunk has correct input_tokens/output_tokens
- `test_passes_messages_to_api`: Verify messages passed to stream() call
- `test_passes_system_prompt`: Verify system parameter passed
- `test_passes_tools_when_provided`: Verify tools parameter passed
- `test_handles_api_error`: Mock anthropic.APIError, verify error chunk yielded
- `test_handles_unexpected_error`: Mock generic Exception, verify error chunk yielded

Mocking pattern for Anthropic:
```python
from unittest.mock import patch, MagicMock, AsyncMock
import pytest
from app.services.llm.anthropic_adapter import AnthropicAdapter
from app.services.llm.base import LLMProvider, StreamChunk

class TestAnthropicAdapterStreamChat:
    @pytest.mark.asyncio
    async def test_yields_text_chunks(self, mock_anthropic_stream):
        """Text content is yielded as text chunks."""
        mock_stream = mock_anthropic_stream(["Hello", " world"])

        with patch('anthropic.AsyncAnthropic') as MockClient:
            MockClient.return_value.messages.stream.return_value = mock_stream

            adapter = AnthropicAdapter(api_key="test-key")

            chunks = []
            async for chunk in adapter.stream_chat(
                messages=[{"role": "user", "content": "Hi"}],
                system_prompt="You are helpful."
            ):
                chunks.append(chunk)

            text_chunks = [c for c in chunks if c.chunk_type == "text"]
            assert len(text_chunks) == 2
            assert text_chunks[0].content == "Hello"
            assert text_chunks[1].content == " world"
```
  </action>
  <verify>
```bash
cd G:\git_repos\BA_assistant\backend
pytest tests/unit/llm/test_anthropic_adapter.py -v
```
All tests pass.
  </verify>
  <done>AnthropicAdapter has 8+ unit tests covering init, streaming, tool use, and error handling</done>
</task>

<task type="auto">
  <name>Task 3: Create GeminiAdapter and DeepSeekAdapter tests</name>
  <files>
    backend/tests/unit/llm/test_gemini_adapter.py
    backend/tests/unit/llm/test_deepseek_adapter.py
  </files>
  <action>
Create tests for GeminiAdapter and DeepSeekAdapter following same class-based pattern.

**GeminiAdapter tests** (`test_gemini_adapter.py`):

Mock pattern for Gemini:
```python
with patch('google.genai.Client') as MockClient:
    # Mock streaming response
    async def mock_stream():
        for text in ["Hello", " world"]:
            chunk = MagicMock()
            chunk.text = text
            chunk.usage_metadata = None
            yield chunk
        # Final chunk with usage
        final = MagicMock()
        final.text = None
        final.usage_metadata = MagicMock()
        final.usage_metadata.prompt_token_count = 10
        final.usage_metadata.candidates_token_count = 5
        yield final

    MockClient.return_value.aio.models.generate_content_stream = AsyncMock(return_value=mock_stream())
```

Test classes for GeminiAdapter:
- **TestGeminiAdapterInit**: api_key, model, provider property
- **TestGeminiAdapterStreamChat**: text chunks, usage, message conversion
- **TestGeminiAdapterNonStreaming**: tool calls (Gemini uses non-streaming for tools)
- **TestGeminiAdapterRetry**: retry on 429/500/503 errors
- **TestGeminiAdapterErrors**: API errors, unexpected errors

**DeepSeekAdapter tests** (`test_deepseek_adapter.py`):

Mock pattern for DeepSeek (OpenAI-compatible):
```python
with patch('openai.AsyncOpenAI') as MockClient:
    async def mock_stream():
        for text in ["Hello", " world"]:
            chunk = MagicMock()
            chunk.choices = [MagicMock()]
            chunk.choices[0].delta.content = text
            chunk.choices[0].delta.reasoning_content = None
            chunk.choices[0].delta.tool_calls = None
            chunk.usage = None
            yield chunk
        # Final chunk with usage
        final = MagicMock()
        final.choices = []
        final.usage = MagicMock()
        final.usage.prompt_tokens = 10
        final.usage.completion_tokens = 5
        yield final

    MockClient.return_value.chat.completions.create = AsyncMock(return_value=mock_stream())
```

Test classes for DeepSeekAdapter:
- **TestDeepSeekAdapterInit**: api_key, model, base_url, provider property
- **TestDeepSeekAdapterStreamChat**: text chunks, usage, message conversion
- **TestDeepSeekAdapterReasoningHidden**: reasoning_content is not yielded
- **TestDeepSeekAdapterToolCalls**: tool_calls chunks yielded correctly
- **TestDeepSeekAdapterRetry**: retry on RateLimitError, APIStatusError 500/503
- **TestDeepSeekAdapterErrors**: various error types
  </action>
  <verify>
```bash
cd G:\git_repos\BA_assistant\backend
pytest tests/unit/llm/test_gemini_adapter.py tests/unit/llm/test_deepseek_adapter.py -v
```
All tests pass.
  </verify>
  <done>GeminiAdapter and DeepSeekAdapter each have 8+ unit tests covering streaming, tools, retry, and errors</done>
</task>

</tasks>

<verification>
Run all LLM adapter tests:
```bash
cd G:\git_repos\BA_assistant\backend
pytest tests/unit/llm/ -v --tb=short
```

Expected: All tests pass (24+ tests across 3 adapter files)
</verification>

<success_criteria>
- backend/tests/unit/llm/ directory exists with __init__.py and conftest.py
- test_anthropic_adapter.py has tests for init, streaming, tool_use, error handling
- test_gemini_adapter.py has tests for streaming, non-streaming tools, retry logic
- test_deepseek_adapter.py has tests for streaming, reasoning hidden, tool calls, retry
- All tests pass with `pytest tests/unit/llm/ -v`
- No real API calls made (all HTTP mocked)
</success_criteria>

<output>
After completion, create `.planning/phases/30-backend-llm-api-tests/30-01-SUMMARY.md`
</output>
