---
phase: 58-agent-sdk-adapter
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/services/llm/claude_agent_adapter.py
  - backend/app/services/llm/base.py
  - backend/app/services/mcp_tools.py
autonomous: true

must_haves:
  truths:
    - "ClaudeAgentAdapter.stream_chat() yields StreamChunk events from SDK agent loop"
    - "SDK events (text, tool_use, thinking, complete, error) translate to StreamChunk without data loss"
    - "MCP tools receive database/project/thread context via HTTP-based transport"
    - "Agent decides when to search docs or save artifacts (proactive tool use)"
    - "Source attribution tracking works via DOCUMENTS_USED markers in tool results"
  artifacts:
    - path: "backend/app/services/llm/claude_agent_adapter.py"
      provides: "Full stream_chat implementation with SDK event translation"
      contains: "async for message in query"
    - path: "backend/app/services/mcp_tools.py"
      provides: "HTTP MCP transport with context propagation via headers"
      contains: "start_mcp_http_server|register_db_session"
  key_links:
    - from: "backend/app/services/llm/claude_agent_adapter.py"
      to: "claude_agent_sdk.query"
      via: "async generator iteration"
      pattern: "async for message in query"
    - from: "backend/app/services/llm/claude_agent_adapter.py"
      to: "backend/app/services/mcp_tools.py"
      via: "HTTP MCP transport with session registry"
      pattern: "get_mcp_http_server_url|register_db_session|unregister_db_session"
---

<objective>
Implement ClaudeAgentAdapter.stream_chat() with full SDK event translation and HTTP-based MCP tool integration.

Purpose: This is the core adapter that translates Claude Agent SDK streaming events into the existing StreamChunk format, enabling the SDK's multi-turn agent loop to work through the LLMAdapter abstraction. The adapter must set up MCP tools with proper context propagation so the agent can proactively search documents and save artifacts.

Output: Working ClaudeAgentAdapter that yields StreamChunk events from SDK query(), with MCP tools accessible via HTTP transport for production-ready context isolation.
</objective>

<execution_context>
@/Users/a1testingmac/.claude/get-shit-done/workflows/execute-plan.md
@/Users/a1testingmac/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Phase 57 summaries (provides SDK setup and MCP tools)
@.planning/phases/57-foundation/57-01-SUMMARY.md
@.planning/phases/57-foundation/57-02-SUMMARY.md

# Research with SDK patterns and pitfalls
@.planning/phases/58-agent-sdk-adapter/58-RESEARCH.md

# Key source files
@backend/app/services/llm/claude_agent_adapter.py
@backend/app/services/llm/base.py
@backend/app/services/mcp_tools.py
@backend/app/services/agent_service.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement ClaudeAgentAdapter.stream_chat() with SDK event translation and MCP integration</name>
  <files>
    backend/app/services/llm/claude_agent_adapter.py
    backend/app/services/llm/base.py
    backend/app/services/mcp_tools.py
  </files>
  <action>
    Replace the stub implementation of ClaudeAgentAdapter.stream_chat() with a working implementation.
    Use the existing AgentService (agent_service.py) as a proven reference — it already works with
    the same SDK, same tools, same event types. The adapter must produce StreamChunk objects instead
    of SSE event dicts.

    **ClaudeAgentAdapter changes (claude_agent_adapter.py):**

    1. Add imports for SDK types: `query`, `ClaudeAgentOptions`, `AssistantMessage`, `TextBlock`,
       `ToolUseBlock`, `ToolResultBlock`, `ResultMessage`, `StreamEvent` from `claude_agent_sdk`.

    2. Add imports from mcp_tools: `get_mcp_http_server_url`, `register_db_session`,
       `unregister_db_session`. Also import `uuid4` from `uuid`.

    3. Add class attribute `is_agent_provider = True` — this signals to AIService that this adapter
       handles tools internally and the manual tool loop must be skipped.

    4. Expand `__init__` to accept optional `db`, `project_id`, `thread_id` parameters that will be
       set before stream_chat is called. Store them as instance attributes. Get the HTTP MCP server
       URL via `get_mcp_http_server_url()` — store as `self.mcp_server_url`.

    5. Add method `set_context(self, db, project_id, thread_id)` that stores these values. AIService
       will call this before calling stream_chat().

    6. Implement `stream_chat()`:
       a. Register db session for HTTP MCP context: generate `session_id = str(uuid4())`, call
          `register_db_session(session_id, db)`. Initialize `documents_used = []` locally to
          track source attribution from tool results.
       b. Build prompt from messages using `_convert_messages_to_prompt(messages)` — extract conversation
          history. Use the same format as AgentService: `[ROLE]: content` separated by double newlines.
       c. Configure `ClaudeAgentOptions` with:
          - `system_prompt=system_prompt` (the BA system prompt passed in by AIService — per locked
            decision to use current BA system prompt identical to direct API adapter)
          - `mcp_servers={"ba": {"type": "url", "url": self.mcp_server_url, "headers": {"X-Project-ID": project_id, "X-Thread-ID": thread_id, "X-DB-Session-ID": session_id, "X-Max-Results": "5"}}}`
            (per locked decision: HTTP-based MCP transport with context via headers)
          - `allowed_tools=["mcp__ba__search_documents", "mcp__ba__save_artifact"]`
          - `permission_mode="acceptEdits"`
          - `include_partial_messages=True` (enables StreamEvent for incremental text)
          - `model=self.model`
          - No `max_turns` parameter (per locked decision: no turn limit for POC)
       d. Iterate `async for message in query(prompt=prompt, options=options)`:

          **StreamEvent handling:**
          - Check for `delta` attribute with `text` attribute -> yield `StreamChunk(chunk_type="text", content=delta.text)`
          - This provides incremental streaming (locked decision: continuous stream across agent rounds)

          **AssistantMessage handling:**
          - Iterate `message.content` blocks
          - `TextBlock`: Do NOT yield as text (already streamed via StreamEvent when `include_partial_messages=True`)
          - `ToolUseBlock`: Yield `StreamChunk(chunk_type="tool_use", tool_call={"id": block.id, "name": block.name, "input": block.input})`
            This provides tool activity visibility (locked decision: show tool activity indicators)
          - `ToolResultBlock`: Check for ARTIFACT_CREATED marker in content (same pattern as AgentService).
            Parse artifact event data and yield a special StreamChunk. Also check for DOCUMENTS_USED
            marker — parse and append to local `documents_used` list for source attribution.
            Add a new optional field `metadata` to StreamChunk to carry artifact_created event data.

          **ResultMessage handling:**
          - Extract usage from `message.usage` (input_tokens, output_tokens)
          - Get documents_used from the locally accumulated `documents_used` list (populated from
            DOCUMENTS_USED markers in ToolResultBlock content during iteration)
          - Yield `StreamChunk(chunk_type="complete", usage={...})` with documents_used in metadata

       e. Wrap entire iteration in try/except/finally. On exception:
          - Yield `StreamChunk(chunk_type="error", error=f"Agent SDK error (turn N): {str(e)}")`
          - Per locked decision: error messages include diagnostic info (which tool failed, which turn)
          - Per locked decision: discard partial output on failure
          In finally block:
          - `unregister_db_session(session_id)` to clean up the session registry

    7. Implement `_convert_messages_to_prompt(messages)`:
       - Same logic as AgentService: iterate messages, format as `[ROLE]: content`
       - Handle list content (tool_results, multi-part)
       - Return joined string

    **StreamChunk extension (base.py):**

    8. Add `metadata: Optional[Dict[str, Any]] = None` field to StreamChunk dataclass.
       This carries agent-specific data: artifact_created events, documents_used for source attribution,
       tool status indicators. Keep it optional to avoid breaking existing adapters.

    **MCP tools — HTTP-based transport (mcp_tools.py):**

    9. Per user's locked decision: "Use proper HTTP-based MCP transport for context propagation —
       production-ready architecture, not shortcuts." Implement HTTP transport instead of ContextVars.

       **Architecture overview:**
       - mcp_tools.py will expose a function `start_mcp_http_server()` that launches a Streamable
         HTTP MCP server on a dynamically assigned port (use port 0 for OS-assigned free port).
       - The server wraps the existing BA tools (search_documents_tool, save_artifact_tool).
       - Context (db session ID, project_id, thread_id) is passed via custom HTTP headers from the
         adapter, NOT via ContextVars.
       - A session registry (`_session_registry: Dict[str, AsyncSession]`) maps session IDs to
         live database sessions, enabling the HTTP MCP server to look up the correct db session
         from the header value.

       **Changes to mcp_tools.py:**

       a. Add `_session_registry: Dict[str, Any] = {}` at module level. This maps string session
          IDs to active SQLAlchemy AsyncSession objects.

       b. Add `register_db_session(session_id: str, db: AsyncSession)` and
          `unregister_db_session(session_id: str)` functions for lifecycle management.

       c. Modify `search_documents_tool` and `save_artifact_tool` to accept context from HTTP
          headers. The MCP HTTP server will extract headers from the incoming request and pass
          them as part of the tool invocation context. Specifically:
          - Read `X-Project-ID` header for project_id
          - Read `X-Thread-ID` header for thread_id
          - Read `X-DB-Session-ID` header, look up db session from `_session_registry`
          - Fall back to ContextVar if headers not present (backward compatibility with AgentService)

       d. Add `start_mcp_http_server()` async function:
          - Creates an MCP HTTP server using `mcp.server.streamable_http` (or the transport
            mechanism available in the mcp library bundled with claude-agent-sdk)
          - Registers the BA tools
          - Starts listening on port 0 (OS-assigned) and returns `(url, server_handle)`
          - The server_handle supports `await server_handle.stop()` for cleanup

       e. Add `get_mcp_http_server_url()` function that starts the server lazily (singleton
          pattern — one HTTP MCP server per process, not per request). Returns the URL string
          like "http://localhost:{port}/mcp".

       f. Increase search result count from 3 to 5 for agent mode (locked decision: larger context
          windows for search results). Add a `max_results` parameter to search_documents_tool that
          defaults to 3. When called via HTTP transport, the adapter passes `X-Max-Results: 5`
          header to request more results.

       g. Update `_documents_used_context` tracking to work with both HTTP headers and ContextVar
          fallback. For HTTP mode, the documents_used list is returned in the tool result payload
          and the adapter extracts it.

       **Changes to claude_agent_adapter.py for HTTP MCP transport:**

       h. In `__init__`, do NOT create MCP server via `create_ba_mcp_server()`. Instead, call
          `get_mcp_http_server_url()` to get the HTTP URL of the singleton MCP server.

       i. In `stream_chat()`, instead of setting ContextVars:
          - Generate a unique session_id (uuid4)
          - Register the db session: `register_db_session(session_id, db)`
          - Configure `ClaudeAgentOptions` with HTTP MCP server:
            ```python
            mcp_servers={
                "ba": {
                    "type": "url",
                    "url": self.mcp_server_url,
                    "headers": {
                        "X-Project-ID": project_id,
                        "X-Thread-ID": thread_id,
                        "X-DB-Session-ID": session_id,
                        "X-Max-Results": "5",
                    }
                }
            }
            ```
          - After streaming completes (in finally block): `unregister_db_session(session_id)`

       j. For `_documents_used_context` (source attribution), since HTTP transport means the tool
          runs in a separate request context, the adapter cannot read ContextVar after streaming.
          Instead, track documents_used by inspecting ToolResultBlock content for a
          `DOCUMENTS_USED:{json}|` marker (same pattern as ARTIFACT_CREATED marker). Modify
          search_documents_tool to append this marker to its result when called via HTTP.

       **Backward compatibility:**
       - The existing AgentService (agent_service.py) continues using ContextVar + in-process MCP.
         It is NOT modified in this phase.
       - Tools fall back to ContextVar when HTTP headers are not present.
       - `create_ba_mcp_server()` factory remains available for in-process usage.

    **Do NOT:**
    - Add turn limits or request timeouts (locked decision: no limits for POC)
    - Implement Claude Code skills integration (deferred)
    - Add production hardening like rate limiting (deferred)
    - Duplicate tool execution logic — SDK handles tools internally via MCP
  </action>
  <verify>
    Run: `cd /Users/a1testingmac/projects/XtraSkill/backend && python -c "from app.services.llm.claude_agent_adapter import ClaudeAgentAdapter; a = ClaudeAgentAdapter(api_key='test'); print(f'Provider: {a.provider.value}, Model: {a.model}, Has MCP URL: {a.mcp_server_url is not None}, Is Agent: {a.is_agent_provider}')"` — should print provider, model, MCP HTTP URL presence, and is_agent_provider=True.

    Run: `cd /Users/a1testingmac/projects/XtraSkill/backend && python -c "from app.services.llm.base import StreamChunk; c = StreamChunk(chunk_type='text', content='hello', metadata={'test': True}); print(f'Type: {c.chunk_type}, Meta: {c.metadata}')"` — should print metadata field.

    Run: `cd /Users/a1testingmac/projects/XtraSkill/backend && python -c "from app.services.mcp_tools import register_db_session, unregister_db_session, get_mcp_http_server_url; print('HTTP MCP transport functions imported OK')"` — should import without error.
  </verify>
  <done>
    ClaudeAgentAdapter has a working stream_chat() that:
    - Iterates SDK query() with proper options (system_prompt, HTTP MCP server URL, model)
    - Passes context via HTTP headers (X-Project-ID, X-Thread-ID, X-DB-Session-ID, X-Max-Results)
    - Translates StreamEvent -> StreamChunk(text)
    - Translates ToolUseBlock -> StreamChunk(tool_use) for visibility
    - Translates ToolResultBlock -> StreamChunk with artifact metadata + DOCUMENTS_USED tracking
    - Translates ResultMessage -> StreamChunk(complete) with usage and documents_used
    - Registers/unregisters db sessions via session registry (cleanup in finally block)
    - Handles errors with diagnostic info
    - Has is_agent_provider=True class attribute
    - StreamChunk has metadata field
    - MCP tools serve via HTTP transport with header-based context and larger search results
  </done>
</task>

<task type="auto">
  <name>Task 2: Modify AIService to bypass manual tool loop for agent providers</name>
  <files>
    backend/app/services/ai_service.py
  </files>
  <action>
    Modify AIService.stream_chat() to detect agent providers (ClaudeAgentAdapter) and skip the
    manual tool execution loop. The SDK handles tool execution internally via MCP, so AIService
    must only forward StreamChunk events without trying to execute tools itself.

    **Changes to AIService.__init__:**

    1. After creating the adapter via `LLMFactory.create(provider)`, check if the adapter has
       `is_agent_provider = True`. Store this as `self.is_agent_provider`.

    **Changes to AIService.stream_chat():**

    2. Add an early branch at the top of stream_chat:
       ```python
       if self.is_agent_provider:
           async for event in self._stream_agent_chat(messages, project_id, thread_id, db):
               yield event
           return
       ```

    3. Implement `_stream_agent_chat()` method:
       a. Call `self.adapter.set_context(db, project_id, thread_id)` to pass request context
       b. Stream from adapter: `async for chunk in self.adapter.stream_chat(messages, SYSTEM_PROMPT, max_tokens=4096)`
          Note: Do NOT pass `tools` parameter — agent providers use MCP tools, not Anthropic tool format
       c. Translate StreamChunk events to SSE event dicts (same format as existing stream_chat):

          - `chunk.chunk_type == "text"` -> yield `{"event": "text_delta", "data": json.dumps({"text": chunk.content})}`
          - `chunk.chunk_type == "tool_use"` -> yield `{"event": "tool_executing", "data": json.dumps({"status": _tool_status_message(chunk.tool_call["name"])})}`
            where _tool_status_message maps tool names:
            - "mcp__ba__save_artifact" or contains "save_artifact" -> "Generating artifact..."
            - "mcp__ba__search_documents" or contains "search_documents" -> "Searching project documents..."
            - default -> f"Using tool: {name}..."
            This satisfies locked decision: show tool activity indicators
          - `chunk.chunk_type == "complete"` -> yield message_complete event with content, usage, and
            documents_used from chunk.metadata (for source attribution chips — locked decision)
          - `chunk.chunk_type == "error"` -> yield error event
          - If chunk.metadata contains "artifact_created" -> yield artifact_created event BEFORE message_complete

       d. Log AI stream start and completion with timing (same logging pattern as existing stream_chat)

    4. Add helper `_tool_status_message(tool_name: str) -> str` for mapping tool names to status messages.

    **IMPORTANT: Do NOT modify the existing tool loop path.** The `while True` loop with tool
    execution must remain intact for direct API providers (anthropic, google, deepseek). Only the
    new `_stream_agent_chat` method handles agent providers.

    **Do NOT:**
    - Execute tools manually for agent providers (SDK does this via MCP)
    - Modify the conversations.py route (it already passes thread.model_provider to AIService)
    - Buffer entire response (stream incrementally)
  </action>
  <verify>
    Run: `cd /Users/a1testingmac/projects/XtraSkill/backend && python -c "
from app.services.ai_service import AIService
from unittest.mock import patch, MagicMock

# Test with direct API provider
with patch('app.services.ai_service.LLMFactory') as mock_factory:
    mock_adapter = MagicMock()
    mock_adapter.is_agent_provider = False
    mock_factory.create.return_value = mock_adapter
    svc = AIService(provider='anthropic')
    print(f'Direct API is_agent_provider: {svc.is_agent_provider}')  # Should be False

# Test with agent provider
with patch('app.services.ai_service.LLMFactory') as mock_factory:
    mock_adapter = MagicMock()
    mock_adapter.is_agent_provider = True
    mock_factory.create.return_value = mock_adapter
    svc = AIService(provider='claude-code-sdk')
    print(f'Agent is_agent_provider: {svc.is_agent_provider}')  # Should be True
"` — should print False then True.

    Verify the existing stream_chat path is untouched by checking the `while True` loop still exists:
    `grep -n "while True" /Users/a1testingmac/projects/XtraSkill/backend/app/services/ai_service.py` — should find the existing tool loop.
  </verify>
  <done>
    AIService correctly detects agent providers and routes to _stream_agent_chat() which:
    - Sets context on adapter before streaming
    - Forwards StreamChunk events as SSE event dicts
    - Shows tool activity indicators for tool_use chunks
    - Includes documents_used in message_complete for source attribution
    - Emits artifact_created events when artifacts are saved
    - Logs stream start/completion with timing
    - Does NOT execute tools manually (SDK handles via MCP)
    - Existing direct API tool loop is untouched
  </done>
</task>

</tasks>

<verification>
1. Import verification: `python -c "from app.services.llm.claude_agent_adapter import ClaudeAgentAdapter"` succeeds
2. Adapter has is_agent_provider=True: `python -c "from app.services.llm.claude_agent_adapter import ClaudeAgentAdapter; assert ClaudeAgentAdapter(api_key='test').is_agent_provider"`
3. StreamChunk has metadata field: `python -c "from app.services.llm.base import StreamChunk; StreamChunk(chunk_type='test', metadata={})"`
3b. HTTP MCP server functions exist: `python -c "from app.services.mcp_tools import get_mcp_http_server_url, register_db_session, unregister_db_session"`
4. AIService detects agent provider: AIService('claude-code-sdk').is_agent_provider is True
5. AIService preserves direct API path: AIService('anthropic').is_agent_provider is False
6. No circular imports: All imports resolve without error
7. Existing tests still pass: `cd backend && python -m pytest tests/unit/llm/ -v`
</verification>

<success_criteria>
1. ClaudeAgentAdapter.stream_chat() is implemented (not a stub) and yields StreamChunk events
2. SDK event types (StreamEvent, AssistantMessage, ToolUseBlock, ToolResultBlock, ResultMessage) are translated to StreamChunk
3. MCP tools (search_documents, save_artifact) are connected via HTTP-based MCP transport with context propagated via HTTP headers
4. AIService routes agent providers to _stream_agent_chat() which forwards chunks without manual tool execution
5. Tool activity indicators yield from tool_use chunks
6. Source attribution data (documents_used) included in complete chunk metadata
7. Error handling includes diagnostic info (per locked decision)
8. No turn limits or timeouts (per locked decision)
9. Existing direct API adapter path unchanged
</success_criteria>

<output>
After completion, create `.planning/phases/58-agent-sdk-adapter/58-01-SUMMARY.md`
</output>
