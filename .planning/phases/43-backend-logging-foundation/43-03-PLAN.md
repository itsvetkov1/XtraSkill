---
phase: 43-backend-logging-foundation
plan: 03
type: execute
wave: 3
depends_on: ["43-02"]
files_modified:
  - backend/app/services/logging_service.py
  - backend/app/services/ai_service.py
  - backend/app/database.py
autonomous: true

must_haves:
  truths:
    - "AI service calls are logged with provider, model, input/output tokens, and timing"
    - "Database operations are logged with table, operation type, and duration"
    - "SSE streaming summaries are logged with event count and total duration"
    - "Sensitive data is sanitized before logging (tokens, API keys, PII fields)"
  artifacts:
    - path: "backend/app/services/logging_service.py"
      provides: "LogSanitizer class for sensitive data redaction"
      contains: "LogSanitizer"
    - path: "backend/app/services/ai_service.py"
      provides: "AI call logging integration"
      contains: "logging_service"
  key_links:
    - from: "backend/app/services/ai_service.py"
      to: "backend/app/services/logging_service.py"
      via: "get_logging_service import"
      pattern: "from app.services.logging_service import"
    - from: "backend/app/services/ai_service.py"
      to: "backend/app/middleware/logging_middleware.py"
      via: "get_correlation_id import"
      pattern: "from app.middleware.logging_middleware import get_correlation_id"
---

<objective>
Add AI service logging, database operation logging, and sensitive data sanitization.

Purpose: Complete the logging integration by instrumenting AI calls (with token counts and timing), database operations, and implementing sanitization to prevent sensitive data exposure (P-04).

Output: Enhanced logging_service.py with sanitizer, instrumented ai_service.py, database logging
</objective>

<execution_context>
@C:\Users\ibcve\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\ibcve\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/43-backend-logging-foundation/43-01-SUMMARY.md
@.planning/phases/43-backend-logging-foundation/43-02-SUMMARY.md
@.planning/research/SUMMARY_v1.9.5.md
@backend/app/services/logging_service.py
@backend/app/services/ai_service.py
@backend/app/database.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add LogSanitizer for sensitive data redaction</name>
  <files>backend/app/services/logging_service.py</files>
  <action>
Add LogSanitizer class to logging_service.py for sanitizing sensitive data before logging.

Add after the imports, before the LoggingService class:

```python
import re
from typing import Any, Dict, List, Set


class LogSanitizer:
    """
    Sanitizes sensitive data before logging.

    Redacts API keys, tokens, passwords, and other sensitive fields
    to prevent accidental exposure in log files.
    """

    # Fields that should always be redacted
    SENSITIVE_FIELDS: Set[str] = {
        "password",
        "token",
        "api_key",
        "apikey",
        "secret",
        "authorization",
        "bearer",
        "credential",
        "private_key",
        "access_token",
        "refresh_token",
        "anthropic_api_key",
        "google_api_key",
        "deepseek_api_key",
        "fernet_key",
        "secret_key",
    }

    # Regex patterns for sensitive values
    SENSITIVE_PATTERNS: List[re.Pattern] = [
        re.compile(r"sk-[a-zA-Z0-9]{20,}"),  # Anthropic API key
        re.compile(r"AIza[a-zA-Z0-9_-]{35}"),  # Google API key
        re.compile(r"Bearer\s+[a-zA-Z0-9\-_.]+", re.IGNORECASE),  # OAuth tokens
        re.compile(r"eyJ[a-zA-Z0-9_-]+\.eyJ[a-zA-Z0-9_-]+\.[a-zA-Z0-9_-]+"),  # JWT tokens
    ]

    REDACTED = "[REDACTED]"

    @classmethod
    def sanitize(cls, data: Any) -> Any:
        """
        Recursively sanitize sensitive data.

        Args:
            data: Data to sanitize (dict, list, or primitive)

        Returns:
            Sanitized copy of data with sensitive values redacted
        """
        if isinstance(data, dict):
            return cls._sanitize_dict(data)
        elif isinstance(data, list):
            return [cls.sanitize(item) for item in data]
        elif isinstance(data, str):
            return cls._sanitize_string(data)
        return data

    @classmethod
    def _sanitize_dict(cls, data: Dict[str, Any]) -> Dict[str, Any]:
        """Sanitize dictionary, redacting sensitive keys."""
        result = {}
        for key, value in data.items():
            key_lower = key.lower()
            # Check if key matches sensitive field
            if any(sensitive in key_lower for sensitive in cls.SENSITIVE_FIELDS):
                result[key] = cls.REDACTED
            else:
                result[key] = cls.sanitize(value)
        return result

    @classmethod
    def _sanitize_string(cls, value: str) -> str:
        """Sanitize string, redacting sensitive patterns."""
        result = value
        for pattern in cls.SENSITIVE_PATTERNS:
            result = pattern.sub(cls.REDACTED, result)
        return result
```

Then update the LoggingService.log() method to use sanitization:

```python
def log(
    self,
    level: str,
    message: str,
    category: str,
    correlation_id: Optional[str] = None,
    **data: Any
) -> None:
    """
    Log a structured message with automatic sanitization.

    Args:
        level: Log level (DEBUG, INFO, WARNING, ERROR)
        message: Human-readable message
        category: Log category (api, ai, db, etc.)
        correlation_id: Request correlation ID
        **data: Additional structured data (will be sanitized)
    """
    logger = self.get_logger()

    # Sanitize additional data
    sanitized_data = LogSanitizer.sanitize(data)

    log_data: Dict[str, Any] = {
        "category": category,
        "message": message,
    }
    if correlation_id:
        log_data["correlationId"] = correlation_id
    log_data.update(sanitized_data)

    log_method = getattr(logger, level.lower(), logger.info)
    log_method(**log_data)
```
  </action>
  <verify>
Run:
```bash
cd backend && python -c "
from app.services.logging_service import LogSanitizer

# Test field redaction
data = {'password': 'secret123', 'username': 'john', 'api_key': 'sk-abc123'}
result = LogSanitizer.sanitize(data)
assert result['password'] == '[REDACTED]'
assert result['username'] == 'john'
assert result['api_key'] == '[REDACTED]'
print('Field redaction: PASS')

# Test pattern redaction
text = 'Using key sk-abcdefghijklmnop123456789'
result = LogSanitizer.sanitize(text)
assert 'sk-' not in result
print('Pattern redaction: PASS')

# Test nested data
nested = {'user': {'password': 'secret', 'name': 'john'}}
result = LogSanitizer.sanitize(nested)
assert result['user']['password'] == '[REDACTED]'
assert result['user']['name'] == 'john'
print('Nested redaction: PASS')
"
```
  </verify>
  <done>LogSanitizer redacts sensitive fields and patterns before logging</done>
</task>

<task type="auto">
  <name>Task 2: Add AI service call logging to ai_service.py</name>
  <files>backend/app/services/ai_service.py</files>
  <action>
Instrument ai_service.py to log AI service calls with provider, model, token counts, and timing.

1. Add imports at top of file:
```python
from app.services.logging_service import get_logging_service
from app.middleware.logging_middleware import get_correlation_id
```

2. In the AIService.stream_chat() method, add logging around the LLM calls:

Add before the `while True:` loop:
```python
logging_service = get_logging_service()
correlation_id = get_correlation_id()
stream_start_time = time.time()
total_input_tokens = 0
total_output_tokens = 0
sse_event_count = 0
```

Add import at top: `import time` (if not already present)

After each `yield` statement that sends an event, increment the counter:
```python
sse_event_count += 1
```

Update the message_complete handling to track tokens:
```python
elif chunk.chunk_type == "complete":
    usage_data = chunk.usage
    if usage_data:
        total_input_tokens = usage_data.get("input_tokens", 0)
        total_output_tokens = usage_data.get("output_tokens", 0)
```

At the end of the method (after all yields, before returning), add SSE summary logging:
```python
# Log SSE streaming summary (BLOG-06)
stream_duration_ms = (time.time() - stream_start_time) * 1000
logging_service.log(
    level="INFO",
    message="SSE stream completed",
    category="ai",
    correlation_id=correlation_id,
    provider=self.adapter.__class__.__name__.replace("Adapter", "").lower(),
    event_count=sse_event_count,
    duration_ms=round(stream_duration_ms, 2),
    input_tokens=total_input_tokens,
    output_tokens=total_output_tokens,
)
```

Also add AI call logging when starting the stream:
After the line `async for chunk in self.adapter.stream_chat(...):`, add at the start of the loop (only log once):
```python
# Log on first chunk (stream started)
if not hasattr(self, '_stream_logged'):
    self._stream_logged = True
    logging_service.log(
        level="INFO",
        message="AI stream started",
        category="ai",
        correlation_id=correlation_id,
        provider=self.adapter.__class__.__name__.replace("Adapter", "").lower(),
        model=getattr(self.adapter, 'model', 'unknown'),
    )
```

Better approach: Log at the start of the method instead:
```python
# Log AI call start (BLOG-03)
logging_service.log(
    level="INFO",
    message="AI stream starting",
    category="ai",
    correlation_id=correlation_id,
    provider=self.adapter.__class__.__name__.replace("Adapter", "").lower(),
    model=getattr(self.adapter, 'model', 'unknown'),
    message_count=len(messages),
)
```

Handle errors with logging:
In the except block, before re-raising:
```python
except Exception as e:
    stream_duration_ms = (time.time() - stream_start_time) * 1000
    logging_service.log(
        level="ERROR",
        message="AI stream failed",
        category="ai",
        correlation_id=correlation_id,
        provider=self.adapter.__class__.__name__.replace("Adapter", "").lower(),
        duration_ms=round(stream_duration_ms, 2),
        error=str(e),
        error_type=type(e).__name__,
    )
    yield {
        "event": "error",
        "data": json.dumps({"message": f"Unexpected error: {str(e)}"})
    }
```
  </action>
  <verify>
Verify the code syntax is valid:
```bash
cd backend && python -c "from app.services.ai_service import AIService; print('AI service imports successfully')"
```
  </verify>
  <done>AI service logs stream start, completion with tokens/timing, and errors</done>
</task>

<task type="auto">
  <name>Task 3: Add database operation logging to database.py</name>
  <files>backend/app/database.py</files>
  <action>
Add database operation logging using SQLAlchemy event listeners.

1. Add imports at top:
```python
import time
from app.services.logging_service import get_logging_service
from app.middleware.logging_middleware import get_correlation_id
```

2. Add a context variable to track query timing:
```python
from contextvars import ContextVar

# Track query start time for duration calculation
_query_start_time: ContextVar[float] = ContextVar("query_start_time", default=0.0)
```

3. Add event listeners for query logging. Add these after the engine creation but before AsyncSessionLocal:

```python
# Database query logging (BLOG-05)
@event.listens_for(Engine, "before_cursor_execute")
def before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    """Track query start time for duration calculation."""
    _query_start_time.set(time.perf_counter())


@event.listens_for(Engine, "after_cursor_execute")
def after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    """Log database query with duration."""
    try:
        duration_ms = (time.perf_counter() - _query_start_time.get()) * 1000

        # Extract table name from SQL (simplified extraction)
        table = "unknown"
        statement_upper = statement.upper().strip()
        if "FROM " in statement_upper:
            # SELECT ... FROM table_name
            parts = statement_upper.split("FROM ")
            if len(parts) > 1:
                table = parts[1].split()[0].strip().lower()
        elif "INTO " in statement_upper:
            # INSERT INTO table_name
            parts = statement_upper.split("INTO ")
            if len(parts) > 1:
                table = parts[1].split()[0].strip("( ").lower()
        elif "UPDATE " in statement_upper:
            # UPDATE table_name
            parts = statement_upper.split("UPDATE ")
            if len(parts) > 1:
                table = parts[1].split()[0].strip().lower()
        elif "DELETE FROM " in statement_upper:
            # DELETE FROM table_name
            parts = statement_upper.split("DELETE FROM ")
            if len(parts) > 1:
                table = parts[1].split()[0].strip().lower()

        # Determine operation type
        if statement_upper.startswith("SELECT"):
            operation = "SELECT"
        elif statement_upper.startswith("INSERT"):
            operation = "INSERT"
        elif statement_upper.startswith("UPDATE"):
            operation = "UPDATE"
        elif statement_upper.startswith("DELETE"):
            operation = "DELETE"
        elif "PRAGMA" in statement_upper:
            operation = "PRAGMA"
            table = "sqlite_internal"
        else:
            operation = "OTHER"

        # Only log actual data operations (skip PRAGMA for noise reduction)
        if operation != "PRAGMA":
            logging_service = get_logging_service()
            correlation_id = get_correlation_id()

            logging_service.log(
                level="DEBUG",
                message="Database query executed",
                category="db",
                correlation_id=correlation_id,
                table=table,
                operation=operation,
                duration_ms=round(duration_ms, 2),
            )
    except Exception:
        # Don't let logging errors break database operations
        pass
```

Note: Log at DEBUG level to avoid excessive log volume. Can be elevated to INFO via config if needed.
  </action>
  <verify>
Run:
```bash
cd backend && python -c "
import asyncio
from app.database import AsyncSessionLocal, init_db
from app.services.logging_service import get_logging_service
from sqlalchemy import text

async def test():
    await init_db()
    async with AsyncSessionLocal() as session:
        await session.execute(text('SELECT 1'))
        await session.commit()
    get_logging_service().shutdown()
    print('DB logging integration: PASS')

asyncio.run(test())
"
```
  </verify>
  <done>Database operations log table, operation type, and duration at DEBUG level</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Verify sensitive data sanitization:
   ```bash
   cd backend && python -c "
   from app.services.logging_service import get_logging_service, LogSanitizer

   ls = get_logging_service()
   ls.log('INFO', 'Test with sensitive data', 'test',
          api_key='sk-supersecret123456789012345',
          username='john',
          password='hunter2')
   ls.shutdown()

   # Check log doesn't contain sensitive data
   with open('logs/app.log') as f:
       content = f.read()
       assert 'sk-supersecret' not in content
       assert 'hunter2' not in content
       assert '[REDACTED]' in content
       print('Sanitization: PASS')
   "
   ```

2. Verify AI service can be imported with logging:
   ```bash
   cd backend && python -c "
   from app.services.ai_service import AIService
   print('AIService with logging: PASS')
   "
   ```

3. Verify database logging:
   ```bash
   cd backend && python -c "
   import asyncio
   from app.database import AsyncSessionLocal, init_db
   from app.config import settings
   from app.services.logging_service import get_logging_service
   from sqlalchemy import text

   # Set log level to DEBUG to see DB logs
   import os
   os.environ['LOG_LEVEL'] = 'DEBUG'

   async def test():
       await init_db()
       async with AsyncSessionLocal() as session:
           await session.execute(text('SELECT * FROM users LIMIT 1'))
           await session.commit()
       get_logging_service().shutdown()

   asyncio.run(test())

   # Check for db category in logs
   with open('logs/app.log') as f:
       content = f.read()
       if '\"category\": \"db\"' in content or '\"category\":\"db\"' in content:
           print('DB logging: PASS')
       else:
           print('DB logging: Check log level is DEBUG')
   "
   ```

4. Full integration test:
   ```bash
   cd backend && python -c "
   import json
   with open('logs/app.log') as f:
       entries = [json.loads(line) for line in f if line.strip()]

   categories = set(e.get('category') for e in entries)
   print(f'Log categories found: {categories}')

   # Should have at least 'api' from middleware, 'test' from our tests
   assert 'api' in categories or 'test' in categories, 'Missing expected categories'
   print('Integration: PASS')
   "
   ```
</verification>

<success_criteria>
- LogSanitizer redacts sensitive fields (password, api_key, token) and patterns (sk-*, Bearer)
- AI service logs stream start/complete with provider, model, token counts, timing
- Database operations log table, operation type, and duration at DEBUG level
- SSE streaming summaries include event count and total duration
- All logging uses correlation ID from middleware
</success_criteria>

<output>
After completion, create `.planning/phases/43-backend-logging-foundation/43-03-SUMMARY.md`
</output>
